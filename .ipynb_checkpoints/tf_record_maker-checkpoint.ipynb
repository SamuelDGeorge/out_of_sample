{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Neccesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All neccesary classes for project\n",
    "\n",
    "#general\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "#image manipulation\n",
    "from PIL import Image as PI\n",
    "from PIL import ImageFilter\n",
    "from resizeimage import resizeimage\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import Custom Functions\n",
    "from Utilities.model_builder import get_image\n",
    "from Utilities.model_builder import get_file_lists\n",
    "from Utilities.model_builder import parse_record\n",
    "from Utilities.model_builder import get_batch\n",
    "from Utilities.model_builder import build_iterator\n",
    "from Utilities.models import log_dir_build\n",
    "from Utilities.utilities import generate_image\n",
    "from Utilities.utilities import generate_image_array\n",
    "from Utilities.blur_tool import blur_images_in_directory\n",
    "from Utilities.resize_tool import resize_images_in_directory\n",
    "from Utilities.blur_tool import blur_and_print_image\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "from Utilities.utilities import display_scores\n",
    "from Utilities.bounded_file_label_extractor import get_files_and_labels\n",
    "from Utilities.bounded_box_record_maker import process_bounded_image_files\n",
    "from Utilities.build_image_data_notebook import process_dataset\n",
    "\n",
    "#for preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TF Records\n",
    "\n",
    "Links for labels, train, and validation are used to build tf_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build tf_records\n",
    "#Set Variables\n",
    "validation_directory = \"D:/Machine_Learning/Datasets/Cifar_80/test_data\"\n",
    "train_directory = \"D:/Machine_Learning/Datasets/Cifar_80/train_data\"\n",
    "output_directory = \"D:/Machine_Learning/Datasets/Cifar_80/tf_records\"\n",
    "labels_file = \"D:/Machine_Learning/Datasets/Cifar_80/labels.txt\"\n",
    "\n",
    "num_threads = 2\n",
    "num_shards = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining list of input files and labels from D:/Machine_Learning/Datasets/Cifar_80/test_data.\n",
      "Found 8000 JPEG files across 80 labels inside D:/Machine_Learning/Datasets/Cifar_80/test_data.\n",
      "Launching 1 threads for spacings: [[0, 8000]]\n",
      "2019-01-09 12:57:33.785990 [thread 0]: Processed 1000 of 8000 images in thread batch.\n",
      "2019-01-09 12:57:35.159991 [thread 0]: Processed 2000 of 8000 images in thread batch.\n",
      "2019-01-09 12:57:36.506990 [thread 0]: Processed 3000 of 8000 images in thread batch.\n",
      "2019-01-09 12:57:37.855991 [thread 0]: Processed 4000 of 8000 images in thread batch.\n",
      "2019-01-09 12:57:39.205991 [thread 0]: Processed 5000 of 8000 images in thread batch.\n",
      "2019-01-09 12:57:40.544991 [thread 0]: Processed 6000 of 8000 images in thread batch.\n",
      "2019-01-09 12:57:41.880991 [thread 0]: Processed 7000 of 8000 images in thread batch.\n",
      "2019-01-09 12:57:43.218991 [thread 0]: Processed 8000 of 8000 images in thread batch.\n",
      "2019-01-09 12:57:43.240991 [thread 0]: Wrote 8000 images to D:/Machine_Learning/Datasets/Cifar_80/tf_records\\validation-00000-of-00001\n",
      "2019-01-09 12:57:43.241991 [thread 0]: Wrote 8000 images to 8000 shards.\n",
      "2019-01-09 12:57:43.410991: Finished writing all 8000 images in data set.\n",
      "Determining list of input files and labels from D:/Machine_Learning/Datasets/Cifar_80/train_data.\n",
      "Found 40000 JPEG files across 80 labels inside D:/Machine_Learning/Datasets/Cifar_80/train_data.\n",
      "Launching 1 threads for spacings: [[0, 40000]]\n",
      "2019-01-09 12:57:45.687991 [thread 0]: Processed 1000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:47.046991 [thread 0]: Processed 2000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:48.395992 [thread 0]: Processed 3000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:49.802990 [thread 0]: Processed 4000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:51.150991 [thread 0]: Processed 5000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:52.503990 [thread 0]: Processed 6000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:53.856990 [thread 0]: Processed 7000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:55.220992 [thread 0]: Processed 8000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:56.571990 [thread 0]: Processed 9000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:57.914991 [thread 0]: Processed 10000 of 40000 images in thread batch.\n",
      "2019-01-09 12:57:59.281992 [thread 0]: Processed 11000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:00.638991 [thread 0]: Processed 12000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:01.997990 [thread 0]: Processed 13000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:03.368991 [thread 0]: Processed 14000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:04.726991 [thread 0]: Processed 15000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:06.090991 [thread 0]: Processed 16000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:07.442991 [thread 0]: Processed 17000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:08.803991 [thread 0]: Processed 18000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:10.165991 [thread 0]: Processed 19000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:11.627992 [thread 0]: Processed 20000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:12.992990 [thread 0]: Processed 21000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:14.436991 [thread 0]: Processed 22000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:15.799993 [thread 0]: Processed 23000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:17.154994 [thread 0]: Processed 24000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:18.515991 [thread 0]: Processed 25000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:19.891990 [thread 0]: Processed 26000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:21.273990 [thread 0]: Processed 27000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:22.634992 [thread 0]: Processed 28000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:23.995991 [thread 0]: Processed 29000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:24.748990 [thread 0]: Processed 30000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:25.445991 [thread 0]: Processed 31000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:26.144991 [thread 0]: Processed 32000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:26.847991 [thread 0]: Processed 33000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:27.548991 [thread 0]: Processed 34000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:28.245991 [thread 0]: Processed 35000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:28.943991 [thread 0]: Processed 36000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:29.655990 [thread 0]: Processed 37000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:30.371990 [thread 0]: Processed 38000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:31.096992 [thread 0]: Processed 39000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:32.383990 [thread 0]: Processed 40000 of 40000 images in thread batch.\n",
      "2019-01-09 12:58:32.424990 [thread 0]: Wrote 40000 images to D:/Machine_Learning/Datasets/Cifar_80/tf_records\\train-00000-of-00001\n",
      "2019-01-09 12:58:32.425991 [thread 0]: Wrote 40000 images to 40000 shards.\n",
      "2019-01-09 12:58:33.358990: Finished writing all 40000 images in data set.\n"
     ]
    }
   ],
   "source": [
    "#make validation records\n",
    "process_dataset('validation', validation_directory, num_shards, labels_file, num_threads, output_directory)\n",
    "\n",
    "#make validation records\n",
    "process_dataset('train', train_directory, num_shards, labels_file, num_threads, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Area\n",
    "\n",
    "Use this area to run tests on above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
