{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#All neccesary classes for project\n",
    "\n",
    "#general\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#for preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "\n",
    "#for machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "#for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "from Utilities.utilities import display_scores\n",
    "from Utilities.utilities import pipeline_transform\n",
    "from Utilities.utilities import reset_graph\n",
    "from Utilities.models import DNN_Model\n",
    "from Utilities.models import cross_val_score_dnn\n",
    "from functools import partial\n",
    "\n",
    "#image manipulation\n",
    "from PIL import Image as PI\n",
    "from resizeimage import resizeimage\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.data_utils import get_file\n",
    "import imagenet_helper_files.vgg_preprocessing\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "import imagenet_helper_files.pnasnet as nas\n",
    "\n",
    "#Import Custom Functions\n",
    "from Utilities.model_builder import get_image\n",
    "from Utilities.model_builder import get_file_lists\n",
    "from Utilities.model_builder import parse_record\n",
    "from Utilities.model_builder import get_batch\n",
    "from Utilities.model_builder import build_iterator\n",
    "from Utilities.model_builder import build_dataset\n",
    "from Utilities.bounded_model_builder import build_bounded_iterator\n",
    "from Utilities.bounded_model_builder import build_bounded_iterator_points\n",
    "from Utilities.model_builder import get_values_imagenet\n",
    "from Utilities.model_builder import get_values_bounded\n",
    "from Utilities.model_builder import get_values_bounded_points\n",
    "from Utilities.models import log_dir_build\n",
    "from Utilities.utilities import generate_image\n",
    "from Utilities.utilities import generate_image_array\n",
    "from Utilities.cell_net_predictor import Binary_Categorical_Predictor\n",
    "from Utilities.build_image_data_notebook import process_dataset\n",
    "\n",
    "from Utilities.utilities import get_ground_truth_string\n",
    "from Utilities.utilities import find_new_imagenet_ground_truth_int\n",
    "from Utilities.cell_net_predictor import Binary_Categorical_Predictor\n",
    "from Utilities.build_image_data_notebook import process_dataset\n",
    "\n",
    "from tan.tan_util import get_tan_nll as tan\n",
    "from tan.tan_util import get_tan_nll_cond as tan_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Here we will load the training and validation data in order to do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build tf_records\n",
    "#Set Variables\n",
    "validation_directory = \"D:/Machine_Learning/Datasets/airplane_vs_horse/validation\"\n",
    "train_directory = \"D:/Machine_Learning/Datasets/airplane_vs_horse/train\"\n",
    "output_directory = \"D:/Machine_Learning/Datasets/airplane_vs_horse/tf_records\"\n",
    "labels_file = \"D:/Machine_Learning/Datasets/airplane_vs_horse/labels.txt\"\n",
    "\n",
    "num_threads = 2\n",
    "num_shards = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make validation records\n",
    "process_dataset('validation', validation_directory, num_shards, labels_file, num_threads, output_directory)\n",
    "\n",
    "#make validation records\n",
    "process_dataset('train', train_directory, num_shards, labels_file, num_threads, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane', 'horse']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import TFRecords for both Training and Testing of the Dat\n",
    "#Use the build_image_data.py to create these sets from your data\n",
    "class_file = open(labels_file,'r')\n",
    "class_list = class_file.read().split('\\n')\n",
    "\n",
    "train_list, val_list = get_file_lists(output_directory)\n",
    "labels = class_list\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'horse',\n",
       " 'book',\n",
       " 'cake',\n",
       " 'car',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'laptop',\n",
       " 'pizza',\n",
       " 'zebra']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_directory_all = \"D:/Machine_Learning/Datasets/ten_class_classifier/validation\"\n",
    "train_directory_all = \"D:/Machine_Learning/Datasets/ten_class_classifier/train\"\n",
    "output_directory_all = \"D:/Machine_Learning/Datasets/ten_class_classifier/tf_records\"\n",
    "labels_file_all = \"D:/Machine_Learning/Datasets/ten_class_classifier/labels.txt\"\n",
    "\n",
    "\n",
    "class_file_all = open(labels_file_all,'r')\n",
    "class_list_all = class_file_all.read().split('\\n')\n",
    "train_list_all, val_list_all = get_file_lists(output_directory_all)\n",
    "labels_all = class_list_all\n",
    "labels_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make validation records\n",
    "process_dataset('validation', validation_directory_all, num_shards, labels_file_all, num_threads, output_directory_all)\n",
    "\n",
    "#make validation records\n",
    "process_dataset('train', train_directory_all, num_shards, labels_file_all, num_threads, output_directory_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import imagenet files\n",
    "\n",
    "with open('D:/AI/models/out_of_set_net_v2/image_net_dict.txt','r') as inf:\n",
    "    image_net_dict_file = eval(inf.read())\n",
    "\n",
    "old_image_net = open('D:/AI/models/out_of_set_net_v2/image_net_dict_2.txt').read().split('\\n')\n",
    "    \n",
    "validation_array = open('D:/Machine_Learning/Devkits/Image_Net/ILSVRC2013_devkit/data/ILSVRC2013_clsloc_validation_ground_truth.txt').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'PACCombiner'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ca1a6940cd3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPACCombiner\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'PACCombiner'"
     ]
    }
   ],
   "source": [
    "#Build a workflow to extract the data \n",
    "reset_graph()\n",
    "\n",
    "filename = tf.placeholder(tf.string, shape=[None], name=\"tf_records\")\n",
    "batch_size = tf.placeholder(tf.int64, shape=[], name= \"Batch_Size\")\n",
    "num_epochs = tf.placeholder(tf.int64, shape=[], name= \"Num_epochs\")\n",
    "training = tf.placeholder_with_default(True, shape=(), name = 'training')\n",
    "handle = tf.placeholder(tf.string, shape=[], name=\"Dataset\")\n",
    "\n",
    "training_set = build_dataset(True, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "val_set = build_dataset(False, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "\n",
    "train_iterator = training_set.make_initializable_iterator()\n",
    "val_iterator = val_set.make_initializable_iterator()\n",
    "\n",
    "\n",
    "\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, training_set.output_types, training_set.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "X, y, file = next_element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = tf.where(tf.equal(y,0))\n",
    "class_1 = tf.where(tf.equal(y,1))\n",
    "\n",
    "c0_X=tf.gather_nd(X,class_0)\n",
    "c1_X=tf.gather_nd(X,class_1)\n",
    "\n",
    "c0_y=tf.gather_nd(y,class_0)\n",
    "c1_y=tf.gather_nd(y,class_1)\n",
    "\n",
    "X_cond_0 = tf.cond(training, lambda: c0_X, lambda: X)\n",
    "X_cond_1 = tf.cond(training, lambda: c1_X, lambda: X)\n",
    "\n",
    "output = tf.cond(tf.equal(0,0), lambda: tf.gather_nd(X, tf.where(tf.equal(y,0))),lambda: tf.gather_nd(X, tf.where(tf.equal(y,1)))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test pulling a piece of data out of the set to ensure that records were created properly\n",
    "with tf.Session() as sess:\n",
    "    #saver2.restore(sess, landmark_ml_model)\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: 10, num_epochs:1})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    \n",
    "    \n",
    "    X_cond_out_0, X_cond_out_1 = sess.run([X_cond_0, X_cond_1], feed_dict={handle: train_handle,training: True})\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = single_file[0][0]\n",
    "Y_val = single_file[0][1]\n",
    "filename = single_file[0][2]\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_val.reshape(331,331,3)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all[Y_val[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network on top of Large Nas-Net\n",
    "\n",
    "Here we will build the Nas-Net and then stack our own network on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import pnas_net model\n",
    "\n",
    "#Nasnet Model Location\n",
    "nas_net_model = 'D:/AI/models/pnas_net/model.ckpt'\n",
    "\n",
    "out_of_set_net_logs = 'D:/AI/models/out_of_set_net_v2/logs'\n",
    "model_path = log_dir_build(out_of_set_net_logs, \"out_of_set_v2_tan\")\n",
    "out_of_set_net_model_class = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_class'\n",
    "out_of_set_net_model_ae = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_ae'\n",
    "out_of_set_net_model_tan = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_tan'\n",
    "out_of_set_net_best = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_best'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.\n",
      "Building tan Graph,\n",
      "\tconditioning Tensor(\"Out_Of_Set_Classifier/Tan/Sort_to_correct_Tan/cond/Merge:0\", shape=(?, 1), dtype=float32)\n",
      "[<function log_rescale.<locals>.invmap at 0x000002112FF780D0>, <function log_rescale.<locals>.invmap at 0x000002111F87CE18>, <function log_rescale.<locals>.invmap at 0x000002111F87CBF8>, <function log_rescale.<locals>.invmap at 0x000002111F87C9D8>, <function log_rescale.<locals>.invmap at 0x000002111F87C7B8>, <function log_rescale.<locals>.invmap at 0x000002111F87C598>, <function log_rescale.<locals>.invmap at 0x000002111F87C378>, <function log_rescale.<locals>.invmap at 0x000002111F87C048>, <function get_LU_map.<locals>.invmap at 0x000002111C334F28>]\n"
     ]
    }
   ],
   "source": [
    "#Reset the graph \n",
    "reset_graph()\n",
    "\n",
    "#Set constants for Neural Network\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 1\n",
    "n_hidden1 = 2000\n",
    "n_hidden2 = 1000\n",
    "n_hidden3 = 500\n",
    "n_hidden4 = 100\n",
    "n_final_layer = 2\n",
    "\n",
    "#Size of final Layer after CNN\n",
    "cnn_code_layer_size = 4320\n",
    "\n",
    "#For Decay\n",
    "likely_batch_size = 60\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#Placeholder for input data\n",
    "X = tf.placeholder(tf.float32, shape=[None, 331, 331, 3], name=\"input\")\n",
    "y = tf.placeholder(tf.float32, shape=[None,8], name=\"bounding_box\")\n",
    "\"\"\"\n",
    "\n",
    "filename = tf.placeholder(tf.string, shape=[None], name=\"tf_records\")\n",
    "batch_size = tf.placeholder(tf.int64, shape=[], name= \"Batch_Size\")\n",
    "num_epochs = tf.placeholder(tf.int64, shape=[], name= \"Num_epochs\")\n",
    "training = tf.placeholder_with_default(True, shape=(), name = 'training')\n",
    "handle = tf.placeholder(tf.string, shape=[], name=\"Dataset\")\n",
    "\n",
    "training_set = build_dataset(True, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "val_set = build_dataset(False, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "\n",
    "train_iterator = training_set.make_initializable_iterator()\n",
    "val_iterator = val_set.make_initializable_iterator()\n",
    "\n",
    "\n",
    "\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, training_set.output_types, training_set.output_shapes)\n",
    "next_data = iterator.get_next()\n",
    "X, y, file = next_data\n",
    "\n",
    "\n",
    "#Define initalizer and batch normalization layers\n",
    "bn_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "#Import the Nas_Net and build it\n",
    "with slim.arg_scope(nas.pnasnet_large_arg_scope()):\n",
    "    net, end_points = nas.build_pnasnet_large(X, num_classes=1001, is_training=False)\n",
    "    \n",
    "    #A saver to load the pretrained Data\n",
    "    saver = tf.train.Saver(name=\"Original_Saver\")\n",
    "    \n",
    "    #For getting predictions from Original Network\n",
    "    pnas_net_predictions = tf.get_default_graph().get_tensor_by_name(\"final_layer/predictions:0\")\n",
    "    \n",
    "    #get indicies for doing reduction\n",
    "    indices = tf.get_default_graph().get_tensor_by_name(\"final_layer/Mean/reduction_indices:0\")\n",
    "    \n",
    "    #Load in the noder where we are going to connect our own network\n",
    "    last_feature_node = tf.get_default_graph().get_tensor_by_name(\"cell_11/cell_output/concat:0\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Out_Of_Set_Classifier\"):\n",
    "    #Use a stop layer to freeze all the layers beneath in Nas-Net\n",
    "    \n",
    "    with tf.name_scope(\"Isolate_Image_Features\"):\n",
    "        #get the output of the last cell layer\n",
    "\n",
    "        starting_relu = tf.nn.relu(last_feature_node, name=\"first_relu\")\n",
    "        mean_pool = tf.reduce_mean(starting_relu, indices, name=\"condensing_mean\")\n",
    "        cnn_code_layer = tf.stop_gradient(mean_pool)\n",
    "    \n",
    "    with tf.name_scope(\"Autoencoder\"):\n",
    "        with tf.name_scope(\"Original_Coding\"):\n",
    "            ae_code_layer = tf.nn.sigmoid(cnn_code_layer)\n",
    "        \n",
    "        with tf.name_scope(\"Hidden_Layer_forward_1\"):\n",
    "            hidden1 = tf.layers.dense(ae_code_layer, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "            bn1 = bn_batch_norm_layer(hidden1)\n",
    "            bn1_act = tf.nn.sigmoid(bn1)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"Hidden_Layer_forward_2\"):\n",
    "            hidden3 = tf.layers.dense(bn1_act, n_hidden3, name=\"hidden3\", kernel_initializer=he_init)\n",
    "            bn3 = bn_batch_norm_layer(hidden3)\n",
    "            bn3_act = tf.nn.sigmoid(bn3) \n",
    "            \n",
    "        with tf.name_scope(\"Hidden_Layer_forward_3\"):\n",
    "            hidden4 = tf.layers.dense(bn3_act, n_hidden4, name=\"hidden4\", kernel_initializer=he_init)\n",
    "            bn4 = bn_batch_norm_layer(hidden4)\n",
    "            AutoCode = tf.nn.sigmoid(bn4, name=\"Autoencoder_Value\")\n",
    "            AutoOutput = tf.stop_gradient(AutoCode, name=\"Autoencoder_Output\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"Hidden_Layer_Reverse_1\"):\n",
    "            hidden5 = tf.layers.dense(AutoCode, n_hidden3, name=\"hidden5\", kernel_initializer=he_init)\n",
    "            bn5 = bn_batch_norm_layer(hidden5)\n",
    "            bn5_act = tf.nn.sigmoid(bn5) \n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"Hidden_Layer_Reverse_2\"):\n",
    "            hidden7 = tf.layers.dense(bn5_act, n_hidden1, name=\"hidden7\", kernel_initializer=he_init)\n",
    "            bn7 = bn_batch_norm_layer(hidden7)\n",
    "            bn7_act = tf.nn.sigmoid(bn7)\n",
    "        \n",
    "        with tf.name_scope(\"Reconstruction_Layer_Final\"):\n",
    "            final_layer = tf.layers.dense(bn7_act, cnn_code_layer_size, name=\"Reconstruction_Layer\", \n",
    "                                                       kernel_initializer=he_init)\n",
    "            final_layer_bn = bn_batch_norm_layer(final_layer)\n",
    "            final_reconstruction_layer = tf.nn.sigmoid(final_layer_bn)\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"AutoEncoder_Loss\"):\n",
    "            ae_loss = tf.losses.mean_squared_error(final_reconstruction_layer,ae_code_layer)\n",
    "            ae_loss_summary = tf.summary.scalar('ae_loss_summary', ae_loss)\n",
    "            \n",
    "           \n",
    "        with tf.name_scope(\"AutoEncoder_Train\"):\n",
    "            \n",
    "            ae_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            ae_extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            \n",
    "            ae_global_step = tf.Variable(0, trainable=False, name='ae_global_step')\n",
    "            \n",
    "            with tf.control_dependencies(ae_extra_update_ops):\n",
    "                training_op_ae = ae_optimizer.minimize(ae_loss, global_step=ae_global_step)\n",
    " \n",
    "    saver_ae = tf.train.Saver(name=\"AutoEncoder_Saver\")\n",
    "    \n",
    "    with tf.name_scope(\"Classifier\"):\n",
    "        \n",
    "        with tf.name_scope(\"Class_Hidden_Layer_1\"):\n",
    "            hidden1_cat = tf.layers.dense(cnn_code_layer, n_hidden1, name=\"hidden1_cat\", kernel_initializer=he_init)\n",
    "            hidden1_drop = tf.layers.dropout(hidden1_cat, dropout_rate, training=training)\n",
    "            bn1_cat = bn_batch_norm_layer(hidden1_drop)\n",
    "            bn1_act_cat = tf.nn.relu(bn1_cat)\n",
    "\n",
    "        with tf.name_scope(\"Class_Hidden_Layer_2\"):\n",
    "            hidden2_cat = tf.layers.dense(bn1_act_cat, n_hidden2, name=\"hidden2_cat\", kernel_initializer=he_init)\n",
    "            hidden2_drop = tf.layers.dropout(hidden2_cat, dropout_rate, training=training)\n",
    "            bn2_cat = bn_batch_norm_layer(hidden2_drop)\n",
    "            bn2_act_cat = tf.nn.relu(bn2_cat)\n",
    "\n",
    "        with tf.name_scope(\"Class_Hidden_Layer_3\"):\n",
    "            hidden3_cat = tf.layers.dense(bn2_act_cat, n_hidden3, name=\"hidden3_cat\", kernel_initializer=he_init)\n",
    "            hidden3_drop = tf.layers.dropout(hidden3_cat, dropout_rate, training=training)\n",
    "            bn3_cat = bn_batch_norm_layer(hidden3_drop)\n",
    "            bn3_act_cat = tf.nn.relu(bn3_cat) \n",
    "\n",
    "        with tf.name_scope(\"Class_Hidden_Layer_4\"):\n",
    "            hidden4_cat = tf.layers.dense(bn3_act_cat, n_hidden4, name=\"hidden4_cat\", kernel_initializer=he_init)\n",
    "            hidden4_drop = tf.layers.dropout(hidden4_cat, dropout_rate, training=training)\n",
    "            bn4_cat = bn_batch_norm_layer(hidden4_drop)\n",
    "            bn4_act_cat = tf.nn.relu(bn4_cat)     \n",
    "        \n",
    "        with tf.name_scope(\"Final_Layer\"): \n",
    "            logits_before_bn = tf.layers.dense(bn4_act_cat, n_final_layer, name=\"outputs\")\n",
    "            logits = bn_batch_norm_layer(logits_before_bn, name=\"logits\")\n",
    "            softmax = tf.nn.softmax(logits, name=\"final_soft_max\")\n",
    "            max_softmax_val = tf.argmax(softmax,axis=1,name=\"softmax_Category_int\",output_type=tf.int32)\n",
    "            stop_max = tf.stop_gradient(max_softmax_val, name=\"Stop_Max\")\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "                xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "                loss_cat = tf.reduce_mean(xentropy, name=\"loss_cat\")\n",
    "                loss_summary_cat = tf.summary.scalar('loss_summary_cat', loss_cat)\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            with tf.control_dependencies(extra_update_ops):\n",
    "                training_op = optimizer.minimize(loss_cat, global_step=global_step)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            accuracy_summary = tf.summary.scalar('accuracy_summary', accuracy)\n",
    "    \n",
    "    saver_class = tf.train.Saver(name=\"Class_Saver\")\n",
    "            \n",
    "    with tf.name_scope(\"Tan\"):\n",
    "        \n",
    "        with tf.name_scope(\"Sort_to_correct_Tan\"):\n",
    "            y_true = tf.to_float(tf.expand_dims(y,1))\n",
    "            y_guess = tf.to_float(tf.expand_dims(stop_max,1))\n",
    "            conditional = tf.cond(training, lambda: y_true, lambda: y_guess)\n",
    "            \n",
    "        \n",
    "        with tf.name_scope(\"Tan_Network\"):\n",
    "            tan_loss, tan_likelihoods, samp = tan_cond(AutoOutput, conditional)\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"Tan_Trainer\"):\n",
    "            likelihood_loss_summary = tf.summary.scalar('likelihood_loss_summary_class', tan_loss)\n",
    "            \n",
    "            global_step_tan = tf.Variable(0, trainable=False, name=\"global_step_tan\")\n",
    "            \n",
    "            learning_rate = tf.train.exponential_decay( \n",
    "                learning_rate=0.001, \n",
    "                global_step=global_step_tan,\n",
    "                decay_steps=int( ( 50000 / ( 2 * likely_batch_size ) ) ), \n",
    "                decay_rate=0.99, \n",
    "                staircase=True\n",
    "            )\n",
    "            tan_train_op = tf.train.RMSPropOptimizer( learning_rate ).minimize( tan_loss )\n",
    "                 \n",
    "            \n",
    "#Variables for global initialization\n",
    "saver_tan = tf.train.Saver(name=\"Final_Saver\")\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Graph to log directory along with checkpoints\n",
    "filewriter = tf.summary.FileWriter(model_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/pnas_net/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Initialize all variables and restore the lower layer\n",
    "with tf.Session() as sess:\n",
    "    #Initalizer all variables\n",
    "    init.run()\n",
    "    \n",
    "    #Restore the pretrained variables from Nas-Net\n",
    "    saver.restore(sess, nas_net_model)\n",
    "    \n",
    "    \n",
    "    #Save all of these variables to the new Cell_Net Model\n",
    "    saver_class.save(sess, out_of_set_net_model_class)\n",
    "    saver_ae.save(sess, out_of_set_net_model_ae)\n",
    "    saver_tan.save(sess, out_of_set_net_model_tan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "Here we are going to train the network. Accuracy/Loss is recorded\n",
    "Note for this version, a certain amount of the data is seen every training step. \n",
    "set train_size for how many images are trained on in each epoch\n",
    "set batch_size for how many images are trained at once.\n",
    "num_epochs is how many times the network sees that ammount of training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AutoEncoder\n",
    "\n",
    "We will Begin by training the autoencoder on all the data in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_v2/final_model/out_of_set_net_ae\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/out_of_set_net_v2/logs/out_of_set_v2_tan-run-20181130212408/\n",
      "Epoch: 1 Loss: 0.00187834\n",
      "Epoch: 2 Loss: 0.00181757\n",
      "Epoch: 3 Loss: 0.00210133\n",
      "Epoch: 4 Loss: 0.002175\n",
      "Epoch: 5 Loss: 0.0018528\n",
      "Epoch: 6 Loss: 0.00187567\n",
      "Epoch: 7 Loss: 0.00182192\n",
      "Epoch: 8 Loss: 0.00189778\n",
      "Epoch: 9 Loss: 0.00231938\n",
      "Epoch: 10 Loss: 0.00161182\n",
      "Epoch: 11 Loss: 0.00209961\n",
      "Epoch: 12 Loss: 0.00223041\n",
      "Epoch: 13 Loss: 0.00212466\n",
      "Epoch: 14 Loss: 0.00238058\n",
      "Epoch: 15 Loss: 0.00226947\n",
      "Epoch: 16 Loss: 0.00220414\n",
      "Epoch: 17 Loss: 0.00180945\n",
      "Epoch: 18 Loss: 0.0024066\n",
      "Epoch: 19 Loss: 0.00189465\n",
      "Epoch: 20 Loss: 0.0024645\n",
      "Epoch: 21 Loss: 0.00214683\n",
      "Epoch: 22 Loss: 0.00201887\n",
      "Epoch: 23 Loss: 0.00188657\n",
      "Epoch: 24 Loss: 0.00200601\n",
      "Epoch: 25 Loss: 0.00233785\n",
      "Epoch: 26 Loss: 0.00208514\n",
      "Epoch: 27 Loss: 0.0020174\n",
      "Epoch: 28 Loss: 0.00232504\n",
      "Epoch: 29 Loss: 0.00189756\n",
      "Epoch: 30 Loss: 0.00205864\n",
      "Epoch: 31 Loss: 0.00209645\n",
      "Epoch: 32 Loss: 0.00233073\n",
      "Epoch: 33 Loss: 0.00210453\n",
      "Epoch: 34 Loss: 0.0020082\n",
      "Epoch: 35 Loss: 0.00228552\n",
      "Epoch: 36 Loss: 0.00225471\n",
      "Epoch: 37 Loss: 0.00223004\n",
      "Epoch: 38 Loss: 0.00236788\n",
      "Epoch: 39 Loss: 0.00192721\n",
      "Epoch: 40 Loss: 0.00188782\n",
      "Epoch: 41 Loss: 0.00197852\n",
      "Epoch: 42 Loss: 0.002152\n",
      "Epoch: 43 Loss: 0.00181153\n",
      "Epoch: 44 Loss: 0.00201381\n",
      "Epoch: 45 Loss: 0.00175557\n",
      "Epoch: 46 Loss: 0.00213903\n",
      "Epoch: 47 Loss: 0.00192115\n",
      "Epoch: 48 Loss: 0.00219232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-155e00d926d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m#run Training Op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_op_ae\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_handle\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#see if we are improving on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "epochs = 50\n",
    "steps_between_test_save = 1\n",
    "batch = 30\n",
    "train_size = 4444\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "lowest_loss = 10000\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_ae.restore(sess, out_of_set_net_model_ae)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([training_op_ae], feed_dict={handle: train_handle})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0 and step != 0) :\n",
    "            loss_sum, loss_val = sess.run([ae_loss_summary, ae_loss], feed_dict = {handle: val_handle, training: False})\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            saver_ae.save(sess, out_of_set_net_model_ae)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_ae.save(sess, out_of_set_net_model_ae)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, ae_global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the classifier\n",
    "\n",
    "Here we are going to train it to recognize the two classes of images that we have for that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_v2/final_model/out_of_set_net_ae\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/out_of_set_net_v2/logs/out_of_set_v2_tan-run-20181130212408/\n",
      "Epoch: 1 Loss: 0.0171216\n",
      "Epoch: 2 Loss: 0.11351\n",
      "Epoch: 3 Loss: 0.00601353\n",
      "Epoch: 4 Loss: 0.0964301\n",
      "Epoch: 5 Loss: 0.00410368\n",
      "Epoch: 6 Loss: 0.00860036\n",
      "Epoch: 7 Loss: 0.00167922\n",
      "Epoch: 8 Loss: 0.0213538\n",
      "Epoch: 9 Loss: 0.043526\n",
      "Epoch: 10 Loss: 0.161292\n",
      "Epoch: 11 Loss: 0.415333\n",
      "Epoch: 12 Loss: 0.355385\n",
      "Epoch: 13 Loss: 0.238138\n",
      "Epoch: 14 Loss: 0.0486593\n",
      "Epoch: 15 Loss: 0.290909\n",
      "Epoch: 16 Loss: 0.015599\n",
      "Epoch: 17 Loss: 0.00151145\n",
      "Epoch: 18 Loss: 0.240642\n",
      "Epoch: 19 Loss: 0.000277208\n",
      "Epoch: 20 Loss: 0.002077\n",
      "Epoch: 21 Loss: 0.165315\n",
      "Epoch: 22 Loss: 0.0656035\n",
      "Epoch: 23 Loss: 0.00300592\n",
      "Epoch: 24 Loss: 0.253954\n",
      "Epoch: 25 Loss: 0.730388\n",
      "Epoch: 26 Loss: 0.120617\n",
      "Epoch: 27 Loss: 0.285156\n",
      "Epoch: 28 Loss: 0.000189971\n",
      "Epoch: 29 Loss: 0.062535\n",
      "Epoch: 30 Loss: 0.208028\n",
      "Epoch: 31 Loss: 0.751379\n",
      "Epoch: 32 Loss: 0.0012476\n",
      "Epoch: 33 Loss: 0.638927\n",
      "Epoch: 34 Loss: 0.150137\n",
      "Epoch: 35 Loss: 0.171545\n",
      "Epoch: 36 Loss: 0.126497\n",
      "Epoch: 37 Loss: 0.0324961\n",
      "Epoch: 38 Loss: 0.135377\n",
      "Epoch: 39 Loss: 0.287819\n",
      "Epoch: 40 Loss: 0.109478\n",
      "Epoch: 41 Loss: 0.0444381\n",
      "Epoch: 42 Loss: 0.348417\n",
      "Epoch: 43 Loss: 0.270918\n",
      "Epoch: 44 Loss: 0.00218371\n",
      "Epoch: 45 Loss: 2.27751e-05\n",
      "Epoch: 46 Loss: 0.027089\n",
      "Epoch: 47 Loss: 1.33527e-05\n",
      "Epoch: 48 Loss: 0.0279924\n",
      "Epoch: 49 Loss: 0.00130314\n",
      "Epoch: 50 Loss: 0.216307\n",
      "Epoch: 51 Loss: 0.00676354\n",
      "Epoch: 52 Loss: 0.453295\n",
      "Epoch: 53 Loss: 0.000607868\n",
      "Epoch: 54 Loss: 0.000245289\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-90b400612106>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m#run Training Op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_handle\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m#see if we are improving on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "steps_between_test_save = 1\n",
    "batch = 30\n",
    "train_size = 4444\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "lowest_loss = 10000\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    init.run()\n",
    "    saver_ae.restore(sess, out_of_set_net_model_ae)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([training_op], feed_dict={handle: train_handle})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0 and step != 0) :\n",
    "            loss_sum, loss_val, acc_sum = sess.run([loss_summary_cat, loss_cat, accuracy_summary], \n",
    "                                                   feed_dict = {handle: val_handle ,training: False})\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            saver_class.save(sess, out_of_set_net_model_class)\n",
    "\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_class.save(sess, out_of_set_net_model_class)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train theTAN\n",
    "\n",
    "Next we train the TAN to get the proper likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_v2/final_model/out_of_set_net_class\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/out_of_set_net_v2/logs/out_of_set_v2_tan-run-20181130212408/\n",
      "Epoch: 0 Loss: 116.556\n",
      "Epoch: 1 Loss: -159.895\n",
      "Epoch: 2 Loss: -117.652\n",
      "Epoch: 3 Loss: -202.973\n",
      "Epoch: 4 Loss: -194.213\n",
      "Epoch: 5 Loss: -250.859\n",
      "Epoch: 6 Loss: -274.975\n",
      "Epoch: 7 Loss: -251.355\n",
      "Epoch: 8 Loss: -270.779\n",
      "Epoch: 9 Loss: -170.471\n",
      "Epoch: 10 Loss: -254.994\n",
      "Epoch: 11 Loss: -307.448\n",
      "Epoch: 12 Loss: -264.353\n",
      "Epoch: 13 Loss: -290.299\n",
      "Epoch: 14 Loss: -317.054\n",
      "Epoch: 15 Loss: -300.761\n",
      "Epoch: 16 Loss: -306.715\n",
      "Epoch: 17 Loss: -246.373\n",
      "Epoch: 18 Loss: -291.561\n",
      "Epoch: 19 Loss: -267.133\n",
      "Epoch: 20 Loss: -286.58\n",
      "Epoch: 21 Loss: -272.392\n",
      "Epoch: 22 Loss: -260.336\n",
      "Epoch: 23 Loss: -274.561\n",
      "Epoch: 24 Loss: -285.252\n",
      "Epoch: 25 Loss: -267.082\n",
      "Epoch: 26 Loss: -314.165\n",
      "Epoch: 27 Loss: -292.911\n",
      "Epoch: 28 Loss: -280.741\n",
      "Epoch: 29 Loss: -303.202\n",
      "Epoch: 30 Loss: -293.738\n",
      "Epoch: 31 Loss: -312.145\n",
      "Epoch: 32 Loss: -306.929\n",
      "Epoch: 33 Loss: -274.799\n",
      "Epoch: 34 Loss: -277.79\n",
      "Epoch: 35 Loss: -302.501\n",
      "Epoch: 36 Loss: -289.909\n",
      "Epoch: 37 Loss: -282.74\n",
      "Epoch: 38 Loss: -280.844\n",
      "Epoch: 39 Loss: -277.826\n",
      "Epoch: 40 Loss: -292.447\n",
      "Epoch: 41 Loss: -309.002\n",
      "Epoch: 42 Loss: -324.364\n",
      "Epoch: 43 Loss: -283.651\n",
      "Epoch: 44 Loss: -286.26\n",
      "Epoch: 45 Loss: -295.066\n",
      "Epoch: 46 Loss: -306.069\n",
      "Epoch: 47 Loss: -307.557\n",
      "Epoch: 48 Loss: -313.888\n",
      "Epoch: 49 Loss: -292.806\n",
      "Epoch: 50 Loss: -306.847\n",
      "Epoch: 51 Loss: -306.642\n",
      "Epoch: 52 Loss: -311.736\n",
      "Epoch: 53 Loss: -307.482\n",
      "Epoch: 54 Loss: -299.314\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-93ada6b603f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m#run Training Op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtan_train_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_handle\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m#see if we are improving on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "epochs = 200\n",
    "steps_between_test_save = 1\n",
    "batch = 30\n",
    "train_size = 4444\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "lowest_like = 10000\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    init.run()\n",
    "    saver_class.restore(sess, out_of_set_net_model_class)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    like_sum, like_val = sess.run([likelihood_loss_summary, tan_loss], feed_dict = {handle: val_handle, training: False})\n",
    "    filewriter.add_summary(like_sum, step)\n",
    "    print(\"Epoch: \" + str(step) + \" Loss: \" + str(like_val))\n",
    "\n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([tan_train_op], feed_dict={handle: train_handle})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0 and step != 0) :\n",
    "            like_sum, like_val = sess.run([likelihood_loss_summary, tan_loss], feed_dict = {handle: val_handle, training: False})\n",
    "            filewriter.add_summary(like_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(like_val))\n",
    "            saver_tan.save(sess, out_of_set_net_model_tan)\n",
    "            if lowest_like > like_val:\n",
    "                lowest_like = like_val\n",
    "                saver_tan.save(sess, out_of_set_net_best)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_tan.save(sess, out_of_set_net_model_tan)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step_tan)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do metric testing\n",
    "\n",
    "Here we will run through all of the training data and relate accuracy with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_set_net_model = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net'\n",
    "\n",
    "batch = 1\n",
    "epochs = 1\n",
    "\n",
    "#Set up frame\n",
    "column_list = ['Item_Number', 'file', 'Correct_Category', 'Correct_Softmax_Value', 'Correct_Softmax_Index','Estimated_Category','Estimated_Category_Index',\n",
    "                                       'Estimated_Category_Strength','Max_Confidence_Index_Value', 'Calculated_Confidence',\n",
    "                                       'Correct_Confidence','Confidence_Score']\n",
    "full_data_frame = pd.DataFrame(columns=column_list)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(iterator.initializer, feed_dict={filenames: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    \n",
    "    #Get Saver Data\n",
    "    new_saver = tf.train.import_meta_graph(out_of_set_net_model + '.meta')\n",
    "    new_saver.restore(sess, out_of_set_net_model)\n",
    "    \n",
    "    #Set up environment for test\n",
    "    training = tf.get_default_graph().get_tensor_by_name(\"training:0\")\n",
    "    soft_max_pna = tf.get_default_graph().get_tensor_by_name(\"final_layer/predictions:0\")\n",
    "    soft_max_confidence = tf.get_default_graph().get_tensor_by_name(\"Out_Of_Set_Classifier/Final_Layer/final_soft_max:0\")\n",
    "    \n",
    "    \n",
    "    #Loop Through Test Data\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        X_val, y_val, filename, y_category, y_confidence, logs = sess.run([X, y, file, soft_max_pna, soft_max_confidence, logits], \n",
    "                                                                   feed_dict={training:False})\n",
    "        file_string = filename[0].decode(\"utf-8\")\n",
    "        \n",
    "        #correct_class = get_ground_truth_string(file_string, validation_array, class_list=old_image_net)\n",
    "        #correct_imagenet_class_num = find_new_imagenet_ground_truth_int(correct_class, image_net_dict_file)\n",
    "\n",
    "        correct_class = 'Dog'\n",
    "        correct_imagenet_class_num = 5\n",
    "\n",
    "        y_confidence_correct = y_val[[0]]\n",
    "        \n",
    "        #Append Data Frame with Requried Information\n",
    "        cat_max = np.argmax(y_category)\n",
    "        con_max = np.argmax(y_confidence)\n",
    "\n",
    "        case_number = i\n",
    "        item_name = image_net_dict_file.get(cat_max - 1)\n",
    "        y_category_strength = y_category[[0],[cat_max]][0]\n",
    "        y_confidence_strength = y_confidence[[0],[con_max]][0]\n",
    "        y_confidence_correct = y_val[[0]][0]\n",
    "        confidence_score = y_confidence[[0],[0]][0]\n",
    "        correct_softmax_value = y_category[[0],correct_imagenet_class_num + 1][0]\n",
    "\n",
    "        to_add = pd.DataFrame([[case_number,file_string,correct_class,correct_softmax_value,correct_imagenet_class_num,item_name,cat_max - 1,y_category_strength,con_max,y_confidence_strength,\n",
    "                            y_confidence_correct,confidence_score]], columns = column_list)\n",
    "        full_data_frame = full_data_frame.append(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the dataframe\n",
    "\n",
    "full_data_frame.to_csv('Out_Of_Set_Test_Run_1.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Testing\n",
    "\n",
    "Here are some helpful scripts for doing additional testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location and models to use for testing\n",
    "\n",
    "out_of_set_net_model_class = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_class'\n",
    "out_of_set_net_model_ae = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_ae'\n",
    "out_of_set_net_model_tan = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_tan'\n",
    "out_of_set_net_best = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_best'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Autoencoder\n",
    "\n",
    "Here we need to test to see that the autoencoder does a decent job of capturing the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_v2/final_model/out_of_set_net_tan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0024000425"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_model_tan)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: 10, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    code, decode, encode, losses, cat = sess.run([ae_code_layer,final_reconstruction_layer,AutoCode,ae_loss,y], feed_dict={handle:val_handle})\n",
    "                    \n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2217.5286411 2242.82018712\n"
     ]
    }
   ],
   "source": [
    "a = sum(code[0])\n",
    "b = sum(decode[0])\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Look at the code as a graph of tensor values coming out\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,cnn_code_layer_size),code[7])\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compare to the decode as a graph of tensor values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,cnn_code_layer_size),decode[7])\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare to the decode as a graph of tensor values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,100),encode[7])\n",
    "plt.axis('on')\n",
    "plt.show()\n",
    "cat[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare to the decode as a graph of tensor values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,100),encode[0])\n",
    "plt.axis('on')\n",
    "plt.show()\n",
    "cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the classifier\n",
    "\n",
    "Here we need to test to see that the classifier does a decent job of capturing the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_v2/final_model/out_of_set_net_tan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument 1.0 has invalid type <class 'numpy.float32'>, must be a string or Tensor. (Can not convert a float32 into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 282\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    283\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3338\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3339\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3427\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[1;32m-> 3428\u001b[1;33m                                                            types_str))\n\u001b[0m\u001b[0;32m   3429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a float32 into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-ae3d9292a095>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     accuracy, loss, guess, truth = sess.run([accuracy, loss_cat, max_softmax_val, y], \n\u001b[1;32m---> 12\u001b[1;33m                                             feed_dict={handle:val_handle, training:False})\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1085\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \"\"\"\n\u001b[0;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \"\"\"\n\u001b[0;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    284\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[0;32m    285\u001b[0m                         \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                         (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[0;32m    287\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument 1.0 has invalid type <class 'numpy.float32'>, must be a string or Tensor. (Can not convert a float32 into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_model_tan)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: 30, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    accuracy_val, loss, guess, truth = sess.run([accuracy, loss_cat, max_softmax_val, y], \n",
    "                                            feed_dict={handle:val_handle, training:False})\n",
    "                    \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the tan\n",
    "Here we need to test to see that the tan does a decent job of capturing the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_v2/final_model/out_of_set_net_tan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-297.53183"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_model_tan)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list_all, batch_size: 30, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    loss, guess, truth, like_loss, like = sess.run([loss_cat, max_softmax_val, y, tan_loss, tan_likelihoods], \n",
    "                                            feed_dict={handle: val_handle, training: False})\n",
    "                    \n",
    "like_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 299.57778931,  296.37915039,  299.57546997,  298.50131226,\n",
       "        293.61859131,  298.96325684,  293.16488647,  290.29077148,\n",
       "        299.57839966,  299.57778931,  298.50286865,  298.50244141,\n",
       "        302.20428467,  299.57891846,  288.58267212,  293.51165771,\n",
       "        299.08660889,  299.6696167 ,  298.50134277,  298.50131226,\n",
       "        301.94647217,  296.37768555,  299.6696167 ,  298.50140381,\n",
       "        301.95495605,  298.50244141,  298.50170898,  302.20742798,\n",
       "        288.57977295,  293.84423828], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 4, 7, 9, 5, 3, 5, 7, 7, 4, 6, 1, 5, 5, 4, 5, 0, 2, 5, 4, 1, 0,\n",
       "       2, 5, 4, 5, 1, 8, 9])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct Answer\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.scatter(x=x_array, y=y_array, c='r', s=40)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guess\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.scatter(x=x_guess, y=y_guess, c='r', s=40)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore graph from meta and restore variables\n",
    "    new_saver = tf.train.import_meta_graph(out_of_set_net_best + '.meta')\n",
    "    new_saver.restore(sess, out_of_set_net_best)\n",
    "    soft = tf.get_default_graph().get_tensor_by_name(\"Out_Of_Set_Classifier/Final_Layer/final_soft_max:0\")\n",
    "    input_tensor = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    val = soft.eval(feed_dict={input_tensor: cat, training: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_net_dict_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = np.reshape(last_layers, (num_epochs,batch_size,4320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, out_of_set_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    \n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" time.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
