{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#All neccesary classes for project\n",
    "\n",
    "#general\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#for preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "\n",
    "#for machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "#for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "from Utilities.utilities import display_scores\n",
    "from Utilities.utilities import pipeline_transform\n",
    "from Utilities.utilities import reset_graph\n",
    "from Utilities.models import DNN_Model\n",
    "from Utilities.models import cross_val_score_dnn\n",
    "from functools import partial\n",
    "\n",
    "#image manipulation\n",
    "from PIL import Image as PI\n",
    "from resizeimage import resizeimage\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.data_utils import get_file\n",
    "import imagenet_helper_files.vgg_preprocessing\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "import imagenet_helper_files.pnasnet as nas\n",
    "\n",
    "#Import Custom Functions\n",
    "from Utilities.model_builder import get_image\n",
    "from Utilities.model_builder import get_file_lists\n",
    "from Utilities.model_builder import parse_record\n",
    "from Utilities.model_builder import get_batch\n",
    "from Utilities.model_builder import build_iterator\n",
    "from Utilities.model_builder import build_dataset\n",
    "from Utilities.bounded_model_builder import build_bounded_iterator\n",
    "from Utilities.bounded_model_builder import build_bounded_iterator_points\n",
    "from Utilities.model_builder import get_values_imagenet\n",
    "from Utilities.model_builder import get_values_bounded\n",
    "from Utilities.model_builder import get_values_bounded_points\n",
    "from Utilities.models import log_dir_build\n",
    "from Utilities.utilities import generate_image\n",
    "from Utilities.utilities import generate_image_array\n",
    "from Utilities.cell_net_predictor import Binary_Categorical_Predictor\n",
    "from Utilities.build_image_data_notebook import process_dataset\n",
    "\n",
    "from Utilities.utilities import get_ground_truth_string\n",
    "from Utilities.utilities import find_new_imagenet_ground_truth_int\n",
    "from Utilities.cell_net_predictor import Binary_Categorical_Predictor\n",
    "from Utilities.build_image_data_notebook import process_dataset\n",
    "from Utilities.sample_counter import get_likelihood_avg\n",
    "from Utilities.sample_counter import get_sample_number\n",
    "from Utilities.sample_counter import get_likelihood_stdev\n",
    "\n",
    "from tan.tan_util import get_tan_nll as tan\n",
    "from tan.tan_util import get_tan_nll_cond as tan_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Load the data, classes, and check that the tf_records are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location for all the training Data\n",
    "#Set Variables\n",
    "\n",
    "output_directory = \"D:/Machine_Learning/Datasets/Cifar_80/tf_records\"\n",
    "labels_file = \"D:/Machine_Learning/Datasets/Cifar_80/labels.txt\"\n",
    "\n",
    "output_directory_all = \"D:/Machine_Learning/Datasets/Cifar_100/tf_records\"\n",
    "labels_file_all = \"D:/Machine_Learning/Datasets/Cifar_100/labels.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chair',\n",
       " 'chimpanzee',\n",
       " 'clock',\n",
       " 'cloud',\n",
       " 'cockroach',\n",
       " 'computer keyboard',\n",
       " 'couch',\n",
       " 'crab',\n",
       " 'crocodile',\n",
       " 'cups',\n",
       " 'dinosaur',\n",
       " 'dolphin',\n",
       " 'elephant',\n",
       " 'flatfish',\n",
       " 'forest',\n",
       " 'fox',\n",
       " 'girl',\n",
       " 'hamster',\n",
       " 'house',\n",
       " 'kangaroo',\n",
       " 'lamp',\n",
       " 'lawn-mower',\n",
       " 'leopard',\n",
       " 'lion',\n",
       " 'lizard',\n",
       " 'lobster',\n",
       " 'man',\n",
       " 'maple',\n",
       " 'motorcycle',\n",
       " 'mountain',\n",
       " 'mouse',\n",
       " 'mushrooms',\n",
       " 'oak',\n",
       " 'oranges',\n",
       " 'orchids',\n",
       " 'otter',\n",
       " 'palm',\n",
       " 'pears',\n",
       " 'pickup truck',\n",
       " 'pine',\n",
       " 'plain',\n",
       " 'plates',\n",
       " 'poppies',\n",
       " 'porcupine',\n",
       " 'possum',\n",
       " 'rabbit',\n",
       " 'raccoon',\n",
       " 'ray',\n",
       " 'road',\n",
       " 'rocket',\n",
       " 'roses',\n",
       " 'sea',\n",
       " 'seal',\n",
       " 'shark',\n",
       " 'shrew',\n",
       " 'skunk',\n",
       " 'skyscraper',\n",
       " 'snail',\n",
       " 'snake',\n",
       " 'spider',\n",
       " 'squirrel',\n",
       " 'streetcar',\n",
       " 'sunflowers',\n",
       " 'sweet peppers',\n",
       " 'table',\n",
       " 'tank',\n",
       " 'telephone',\n",
       " 'television',\n",
       " 'tiger',\n",
       " 'tractor',\n",
       " 'train',\n",
       " 'trout',\n",
       " 'tulips',\n",
       " 'turtle',\n",
       " 'wardrobe',\n",
       " 'whale',\n",
       " 'willow',\n",
       " 'wolf',\n",
       " 'woman',\n",
       " 'worm']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import TFRecords for both Training and Testing of the Dat\n",
    "#Use the build_image_data.py to create these sets from your data\n",
    "class_file = open(labels_file,'r')\n",
    "class_list = class_file.read().split('\\n')\n",
    "\n",
    "train_list, val_list = get_file_lists(output_directory)\n",
    "labels = class_list\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apples',\n",
       " 'aquarium fish',\n",
       " 'baby',\n",
       " 'bear',\n",
       " 'beaver',\n",
       " 'bed',\n",
       " 'bee',\n",
       " 'beetle',\n",
       " 'bicycle',\n",
       " 'bottles',\n",
       " 'bowls',\n",
       " 'boy',\n",
       " 'bridge',\n",
       " 'bus',\n",
       " 'butterfly',\n",
       " 'camel',\n",
       " 'cans',\n",
       " 'castle',\n",
       " 'caterpillar',\n",
       " 'cattle',\n",
       " 'chair',\n",
       " 'chimpanzee',\n",
       " 'clock',\n",
       " 'cloud',\n",
       " 'cockroach',\n",
       " 'computer keyboard',\n",
       " 'couch',\n",
       " 'crab',\n",
       " 'crocodile',\n",
       " 'cups',\n",
       " 'dinosaur',\n",
       " 'dolphin',\n",
       " 'elephant',\n",
       " 'flatfish',\n",
       " 'forest',\n",
       " 'fox',\n",
       " 'girl',\n",
       " 'hamster',\n",
       " 'house',\n",
       " 'kangaroo',\n",
       " 'lamp',\n",
       " 'lawn-mower',\n",
       " 'leopard',\n",
       " 'lion',\n",
       " 'lizard',\n",
       " 'lobster',\n",
       " 'man',\n",
       " 'maple',\n",
       " 'motorcycle',\n",
       " 'mountain',\n",
       " 'mouse',\n",
       " 'mushrooms',\n",
       " 'oak',\n",
       " 'oranges',\n",
       " 'orchids',\n",
       " 'otter',\n",
       " 'palm',\n",
       " 'pears',\n",
       " 'pickup truck',\n",
       " 'pine',\n",
       " 'plain',\n",
       " 'plates',\n",
       " 'poppies',\n",
       " 'porcupine',\n",
       " 'possum',\n",
       " 'rabbit',\n",
       " 'raccoon',\n",
       " 'ray',\n",
       " 'road',\n",
       " 'rocket',\n",
       " 'roses',\n",
       " 'sea',\n",
       " 'seal',\n",
       " 'shark',\n",
       " 'shrew',\n",
       " 'skunk',\n",
       " 'skyscraper',\n",
       " 'snail',\n",
       " 'snake',\n",
       " 'spider',\n",
       " 'squirrel',\n",
       " 'streetcar',\n",
       " 'sunflowers',\n",
       " 'sweet peppers',\n",
       " 'table',\n",
       " 'tank',\n",
       " 'telephone',\n",
       " 'television',\n",
       " 'tiger',\n",
       " 'tractor',\n",
       " 'train',\n",
       " 'trout',\n",
       " 'tulips',\n",
       " 'turtle',\n",
       " 'wardrobe',\n",
       " 'whale',\n",
       " 'willow',\n",
       " 'wolf',\n",
       " 'woman',\n",
       " 'worm']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training and Validation data for all classs\n",
    "\n",
    "class_file_all = open(labels_file_all,'r')\n",
    "class_list_all = class_file_all.read().split('\\n')\n",
    "train_list_all, val_list_all = get_file_lists(output_directory_all)\n",
    "labels_all = class_list_all\n",
    "labels_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the training Data is good\n",
    "\n",
    "Here we will import a tf reccord and ensure the data is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a workflow to extract the data \n",
    "reset_graph()\n",
    "\n",
    "filename = tf.placeholder(tf.string, shape=[None], name=\"tf_records\")\n",
    "batch_size = tf.placeholder(tf.int64, shape=[], name= \"Batch_Size\")\n",
    "num_epochs = tf.placeholder(tf.int64, shape=[], name= \"Num_epochs\")\n",
    "training = tf.placeholder_with_default(True, shape=(), name = 'training')\n",
    "handle = tf.placeholder(tf.string, shape=[], name=\"Dataset\")\n",
    "\n",
    "training_set = build_dataset(True, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "val_set = build_dataset(False, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "\n",
    "train_iterator = training_set.make_initializable_iterator()\n",
    "val_iterator = val_set.make_initializable_iterator()\n",
    "\n",
    "\n",
    "\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, training_set.output_types, training_set.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "X_data, y_data, file = next_element\n",
    "\n",
    "X = tf.placeholder_with_default(X_data, [None,331,331,3])\n",
    "y = tf.placeholder_with_default(y_data, [None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test pulling a piece of data out of the set to ensure that records were created properly\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list_all, batch_size: 1, num_epochs:1})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    \n",
    "    \n",
    "    X_test, y_test, file_test = sess.run([X,y,file], feed_dict={handle: train_handle,training: False})\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show and Image from set\n",
    "x_val = X_test.reshape(331,331,3)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Class\n",
    "labels_all[y_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the file\n",
    "file_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "\n",
    "Here we will build the Nas-Net and then stack our own network on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import pnas_net model\n",
    "\n",
    "#Nasnet Model Location\n",
    "nas_net_model = 'D:/AI/models/pnas_net/model.ckpt'\n",
    "\n",
    "#directory for logs in training\n",
    "out_of_set_net_logs = 'D:/AI/models/out_of_set_net_final/logs'\n",
    "model_path = log_dir_build(out_of_set_net_logs, \"out_of_set_final\")\n",
    "\n",
    "#directory for all the models saved during training\n",
    "out_of_set_net_model_class = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_class'\n",
    "out_of_set_net_model_tan = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_tan'\n",
    "out_of_set_net_model_ae = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_ae'\n",
    "out_of_set_net_best = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_best'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.\n",
      "Building tan Graph,\n",
      "\tconditioning Tensor(\"Out_Of_Set_Classifier/Tan/Sort_to_correct_Tan/cond/Merge:0\", shape=(?, 1), dtype=float32)\n",
      "Using conditional transformation...\n",
      "Using conditional transformation...\n",
      "[<function conditioning_transformation.<locals>.invmap at 0x0000028F85EA6BF8>, <function log_rescale.<locals>.invmap at 0x0000028F85EA6400>, <function log_rescale.<locals>.invmap at 0x0000028F85EA61E0>, <function log_rescale.<locals>.invmap at 0x0000028F85EA6048>, <function log_rescale.<locals>.invmap at 0x0000028F857DFD08>, <function log_rescale.<locals>.invmap at 0x0000028F857DFAE8>, <function log_rescale.<locals>.invmap at 0x0000028F857DF8C8>, <function log_rescale.<locals>.invmap at 0x0000028F857DF6A8>, <function log_rescale.<locals>.invmap at 0x0000028F857DF268>, <function conditioning_transformation.<locals>.invmap at 0x0000028F857DF048>, <function get_LU_map.<locals>.invmap at 0x0000028F84D02E18>]\n"
     ]
    }
   ],
   "source": [
    "#the graph \n",
    "reset_graph()\n",
    "\n",
    "number_of_classes = 80\n",
    "\n",
    "#Set up data Pipeline \n",
    "with tf.name_scope(\"Data_Retriever\"):\n",
    "    filename = tf.placeholder(tf.string, shape=[None], name=\"tf_records\")\n",
    "    batch_size = tf.placeholder(tf.int64, shape=[], name= \"Batch_Size\")\n",
    "    num_epochs = tf.placeholder(tf.int64, shape=[], name= \"Num_epochs\")\n",
    "    training = tf.placeholder_with_default(True, shape=(), name = 'training')\n",
    "    handle = tf.placeholder(tf.string, shape=[], name=\"Dataset\")\n",
    "\n",
    "    training_set = build_dataset(True, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "    val_set = build_dataset(False, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "\n",
    "    train_iterator = training_set.make_initializable_iterator()\n",
    "    val_iterator = val_set.make_initializable_iterator()\n",
    "\n",
    "\n",
    "\n",
    "    iterator = tf.data.Iterator.from_string_handle(\n",
    "        handle, training_set.output_types, training_set.output_shapes)\n",
    "    next_data = iterator.get_next()\n",
    "    X_data, y_data, file = next_data\n",
    "    X = tf.placeholder_with_default(X_data, shape=[None,331,331,3], name=\"Image_Data\")\n",
    "    y = tf.placeholder_with_default(y_data, shape=[None], name=\"Max_Class_Int\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"BN_Layer_AE_Layers\"):\n",
    "    #Define initalizer and batch normalization layers\n",
    "    bn_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "#Import the Nas_Net and build it\n",
    "with slim.arg_scope(nas.pnasnet_large_arg_scope()):\n",
    "    net, end_points = nas.build_pnasnet_large(X, num_classes=1001, is_training=False)\n",
    "    \n",
    "    #A saver to load the pretrained Data\n",
    "    saver = tf.train.Saver(name=\"Original_Saver\")\n",
    "    \n",
    "    #For getting predictions from Original Network\n",
    "    pnas_net_predictions = tf.get_default_graph().get_tensor_by_name(\"final_layer/predictions:0\")\n",
    "    \n",
    "    #get indicies for doing reduction\n",
    "    indices = tf.get_default_graph().get_tensor_by_name(\"final_layer/Mean/reduction_indices:0\")\n",
    "    \n",
    "    #Load in the noder where we are going to connect our own network\n",
    "    last_feature_node = tf.get_default_graph().get_tensor_by_name(\"cell_11/cell_output/concat:0\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Out_Of_Set_Classifier\"):\n",
    "    #Use a stop layer to freeze all the layers beneath in Nas-Net\n",
    " \n",
    "    with tf.name_scope(\"Isolate_Image_Features\"):\n",
    "        #get the output of the last cell layer\n",
    "\n",
    "        starting_relu = tf.nn.relu(last_feature_node, name=\"first_relu\")\n",
    "        mean_pool = tf.reduce_mean(starting_relu, indices, name=\"condensing_mean\")\n",
    "        cnn_code_layer = tf.stop_gradient(mean_pool)\n",
    "        \n",
    "    with tf.name_scope(\"Classifier\"):\n",
    "       \n",
    "        #Set constants for Classifier\n",
    "        dropout_rate = 0.3\n",
    "        n_hidden1 = 3000\n",
    "        n_hidden2 = 500\n",
    "        n_final_layer = number_of_classes\n",
    "        learning_rate_class = .1\n",
    "        \n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"Class_Hidden_Layer_1\"):\n",
    "            hidden1_cat = tf.layers.dense(mean_pool, n_hidden1, name=\"hidden1_cat\", kernel_initializer=he_init)\n",
    "            hidden1_drop = tf.layers.dropout(hidden1_cat, dropout_rate, training=training)\n",
    "            bn1_cat = bn_batch_norm_layer(hidden1_drop)\n",
    "            bn1_act_cat = tf.nn.relu(bn1_cat)\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.name_scope(\"Class_Hidden_Layer_3\"):\n",
    "            hidden3_cat = tf.layers.dense(cnn_code_layer, n_hidden2, name=\"hidden2_cat\", kernel_initializer=he_init)\n",
    "            hidden3_drop = tf.layers.dropout(hidden3_cat, dropout_rate, training=training)\n",
    "            bn3_cat = bn_batch_norm_layer(hidden3_drop)\n",
    "            bn3_act_cat = tf.nn.relu(bn3_cat) \n",
    "  \n",
    "        \n",
    "        with tf.name_scope(\"Final_Layer\"): \n",
    "            logits_before_bn = tf.layers.dense(bn3_act_cat, n_final_layer, name=\"outputs\")\n",
    "            logits = bn_batch_norm_layer(logits_before_bn, name=\"logits\")\n",
    "            softmax = tf.nn.softmax(logits, name=\"final_soft_max\")\n",
    "            max_softmax_val = tf.argmax(softmax,axis=1,name=\"softmax_Category_int\",output_type=tf.int32)\n",
    "            stop_max = tf.stop_gradient(max_softmax_val, name=\"Stop_Max\")\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "                xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "                loss_cat = tf.reduce_mean(xentropy, name=\"loss_cat\")\n",
    "                loss_summary_cat = tf.summary.scalar('loss_summary_cat', loss_cat)\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_class)\n",
    "\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            with tf.control_dependencies(extra_update_ops):\n",
    "                training_op = optimizer.minimize(loss_cat, global_step=global_step)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            accuracy_summary = tf.summary.scalar('accuracy_summary', accuracy)\n",
    "    \n",
    "    saver_class = tf.train.Saver(name=\"Class_Saver\")\n",
    "    \n",
    "\n",
    "    with tf.name_scope(\"Pooling_Reduction\"):\n",
    "        #Size of final Layer after CNNd\n",
    "        cnn_code_layer_size = 4320\n",
    "        \n",
    "        #AE Layer Sizes\n",
    "        pool_code_size = 1080\n",
    "        \n",
    "        with tf.name_scope(\"Pool_Layers\"):\n",
    "            #Pool and flatten\n",
    "            pre_code_layer = tf.expand_dims(cnn_code_layer,2)\n",
    "            pool_code_layer = tf.nn.pool(pre_code_layer,window_shape=[4],pooling_type='MAX', padding='SAME',strides=[4])\n",
    "            second_pool_layer = tf.nn.pool(pool_code_layer, window_shape=[10], pooling_type='MAX', padding='SAME',strides=[10])\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"Code_Layer\"):\n",
    "            PoolCode = tf.layers.flatten(second_pool_layer)\n",
    "            PoolOutput = tf.stop_gradient(PoolCode, name=\"Pool_Output\")\n",
    "            \n",
    "    with tf.name_scope(\"Autoencoder\"):\n",
    "        n_hidden4 = 108\n",
    "        ae_learning_rate = 1\n",
    "        with tf.name_scope(\"Original_Coding\"):\n",
    "            ae_code_layer = cnn_code_layer\n",
    "            batch_mean = tf.expand_dims(tf.reduce_mean(cnn_code_layer,0),0)\n",
    "            ae_code_normalized = tf.subtract(ae_code_layer, batch_mean)\n",
    "            \n",
    "        with tf.name_scope(\"PCA_Layer\"):\n",
    "            reduction_matrix = tf.Variable(tf.random_normal([cnn_code_layer_size, n_hidden4],stddev=.1),name=\"weights\")           \n",
    "            AutoCode = tf.matmul(ae_code_normalized,reduction_matrix)\n",
    "            AutoOutput = tf.stop_gradient(AutoCode, name=\"Autoencoder_Output\")\n",
    "            \n",
    "        \n",
    "        with tf.name_scope(\"Reconstruction_Layer_Final\"):\n",
    "            final_layer = tf.layers.dense(AutoCode, cnn_code_layer_size, name=\"Reconstruction_Layer\", \n",
    "                                                       kernel_initializer=he_init)\n",
    "            final_reconstruction_layer_bn = tf.matmul(AutoCode, tf.transpose(reduction_matrix))\n",
    "            final_reconstruction_layer = tf.add(final_reconstruction_layer_bn,batch_mean)\n",
    "            \n",
    "        with tf.name_scope(\"AutoEncoder_Loss\"):\n",
    "            ae_loss = tf.losses.mean_squared_error(final_reconstruction_layer,ae_code_layer)\n",
    "            ae_loss_summary = tf.summary.scalar('ae_loss_summary', ae_loss)\n",
    "            \n",
    "           \n",
    "        with tf.name_scope(\"AutoEncoder_Train\"):\n",
    "            \n",
    "            ae_optimizer = tf.train.AdamOptimizer(learning_rate=ae_learning_rate)\n",
    "            ae_global_step = tf.Variable(0, trainable=False, name='ae_global_step')\n",
    "            training_op_ae = ae_optimizer.minimize(ae_loss, global_step=ae_global_step)\n",
    " \n",
    "    saver_ae = tf.train.Saver(name=\"AutoEncoder_Saver\")\n",
    "            \n",
    "    with tf.name_scope(\"Tan\"):\n",
    "        likely_batch_size = 60\n",
    "        use_pca_ae = tf.placeholder_with_default(False, shape=(), name = 'Use_PCA_Encoding')\n",
    "        \n",
    "        with tf.name_scope(\"Sort_to_correct_Tan\"):\n",
    "            y_true = tf.to_float(tf.expand_dims(y,1))\n",
    "            y_guess = tf.to_float(tf.expand_dims(stop_max,1))\n",
    "            conditional = tf.cond(training, lambda: y_true, lambda: y_guess)\n",
    "            \n",
    "            desired_code = tf.cond(use_pca_ae, lambda: AutoOutput, lambda: PoolOutput)\n",
    "            tan_input = tf.expand_dims(desired_code,1)\n",
    "        with tf.name_scope(\"Tan_Network\"):\n",
    "            tan_loss, tan_likelihoods, samp = tan_cond(tan_input, conditional)\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"Tan_Trainer\"):\n",
    "            likelihood_loss_summary = tf.summary.scalar('likelihood_loss_summary_class', tan_loss)\n",
    "            \n",
    "            global_step_tan = tf.Variable(0, trainable=False, name=\"global_step_tan\")\n",
    "            \n",
    "            learning_rate = tf.train.exponential_decay( \n",
    "                learning_rate=0.001, \n",
    "                global_step=global_step_tan,\n",
    "                decay_steps=int( ( 50000 / ( 2 * likely_batch_size ) ) ), \n",
    "                decay_rate=0.99, \n",
    "                staircase=True\n",
    "            )\n",
    "            tan_train_op = tf.train.RMSPropOptimizer( learning_rate ).minimize( tan_loss, global_step=global_step_tan )\n",
    "            \n",
    "    with tf.name_scope(\"Tan_Threshold\"):\n",
    "        \n",
    "        with tf.name_scope(\"Likelihood_Average\"):\n",
    "            likelihood_list = tan_likelihoods\n",
    "            class_list = conditional\n",
    "            \n",
    "            \n",
    "            #likelihood_averages = tf.Variable([0,0],name=\"tan_distribution_lists\", trainable=False, dtype=tf.float32)\n",
    "            likelihood_averages = tf.get_variable('tan_distribution_lists', shape=(number_of_classes), trainable=False, dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            likelihood_stdev = tf.get_variable('tan_distribution_stdev', shape=(number_of_classes), trainable=False, dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            number_of_batches = tf.get_variable('sample_count', shape=(number_of_classes), trainable=False, dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "               \n",
    "            update_avg_like = tf.assign(likelihood_averages,get_likelihood_avg(y, number_of_classes, likelihood_list,likelihood_averages, number_of_batches))\n",
    "            update_stdev_like = tf.assign(likelihood_stdev, get_likelihood_stdev(y, number_of_classes, likelihood_list,likelihood_stdev, number_of_batches))\n",
    "            with tf.get_default_graph().control_dependencies([update_avg_like, update_stdev_like]):\n",
    "                update_batch_size = tf.assign(number_of_batches, get_sample_number(y, number_of_classes, likelihood_list, number_of_batches) )\n",
    "            \n",
    "            update_average_likelihood = tf.group(update_avg_like, update_stdev_like, update_batch_size)\n",
    "            \n",
    "        with tf.name_scope(\"Likelihood_Threshold\"):\n",
    "            bottom_quartile = likelihood_averages - .25 * likelihood_stdev\n",
    "            \n",
    "            \n",
    "#Variables for global initialization\n",
    "saver_tan = tf.train.Saver(name=\"Final_Saver\")\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Graph to log directory\n",
    "filewriter = tf.summary.FileWriter(model_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize PNAS Net\n",
    "\n",
    "Restore the Parameters from the pre-trained PNAS Net and initialize all the savers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/pnas_net/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Initialize all variables and restore the lower layer\n",
    "with tf.Session() as sess:\n",
    "    #Initalizer all variables\n",
    "    init.run()\n",
    "    \n",
    "    #Restore the pretrained variables from Nas-Net\n",
    "    saver.restore(sess, nas_net_model)\n",
    "    \n",
    "    \n",
    "    #Save all of these variables to the new Cell_Net Model\n",
    "    saver_class.save(sess, out_of_set_net_model_class)\n",
    "    saver_ae.save(sess, out_of_set_net_model_ae)\n",
    "    saver_tan.save(sess, out_of_set_net_model_tan)\n",
    "    saver_tan.save(sess, out_of_set_net_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "Here we are going to train the network. Accuracy/Loss is recorded\n",
    "Note for this version, a certain amount of the data is seen every training step. \n",
    "set train_size for how many images are trained on in each epoch\n",
    "set batch_size for how many images are trained at once.\n",
    "num_epochs is how many times the network sees that ammount of training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure settings for Session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the classifier\n",
    "\n",
    "Here we are going to train it to recognize the two classes of images that we have for that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_final/model/out_of_set_net_class\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/out_of_set_net_final/logs/out_of_set_final-run-20190110000303/\n",
      "Epoch: 1 Loss: 1.26332\n",
      "Epoch: 2 Loss: 1.12337\n",
      "Epoch: 3 Loss: 1.12963\n",
      "Epoch: 4 Loss: 0.985917\n",
      "Epoch: 5 Loss: 1.29833\n",
      "Epoch: 6 Loss: 1.09361\n",
      "Epoch: 7 Loss: 0.995808\n",
      "Epoch: 8 Loss: 1.60355\n",
      "Epoch: 9 Loss: 1.23174\n",
      "Epoch: 10 Loss: 0.655279\n",
      "Epoch: 11 Loss: 2.02475\n",
      "Epoch: 12 Loss: 0.707763\n",
      "Epoch: 13 Loss: 0.945974\n",
      "Epoch: 14 Loss: 1.53385\n",
      "Epoch: 15 Loss: 1.7752\n",
      "Epoch: 16 Loss: 1.3947\n",
      "Epoch: 17 Loss: 1.32972\n",
      "Epoch: 18 Loss: 1.59164\n",
      "Epoch: 19 Loss: 2.14029\n",
      "Epoch: 20 Loss: 1.63452\n",
      "Epoch: 21 Loss: 1.34545\n",
      "Epoch: 22 Loss: 1.75523\n",
      "Epoch: 23 Loss: 1.15035\n",
      "Epoch: 24 Loss: 1.52548\n",
      "Epoch: 25 Loss: 1.42884\n",
      "Epoch: 26 Loss: 1.92186\n",
      "Epoch: 27 Loss: 1.81008\n",
      "Epoch: 28 Loss: 1.98337\n",
      "Epoch: 29 Loss: 1.84782\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "steps_between_test_save = 1\n",
    "batch = 30\n",
    "train_size = 40000\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "lowest_loss = 10000\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_class.restore(sess, out_of_set_net_model_class)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 1\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([training_op], feed_dict={handle: train_handle})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0) :\n",
    "            loss_sum, loss_val, acc_sum = sess.run([loss_summary_cat, loss_cat, accuracy_summary], \n",
    "                                                   feed_dict = {handle: val_handle ,training: False})\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            saver_class.save(sess, out_of_set_net_model_class)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_class.save(sess, out_of_set_net_model_class)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AutoEncoder\n",
    "\n",
    "With our Classifier trained we can now train the AE with the Overlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "epochs = 60\n",
    "steps_between_test_save = 1\n",
    "batch = 30\n",
    "train_size = 40000\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "lowest_loss = 10000\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    init.run()\n",
    "    saver_class.restore(sess, out_of_set_net_model_class)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([training_op_ae], feed_dict={handle: train_handle})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0 and step != 0) :\n",
    "            loss_sum, loss_val = sess.run([ae_loss_summary, ae_loss], feed_dict = {handle: val_handle, training: False})\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            saver_ae.save(sess, out_of_set_net_model_ae)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_ae.save(sess, out_of_set_net_model_ae)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, ae_global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the TAN\n",
    "\n",
    "Next we train the TAN to get the proper likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_final/model/out_of_set_net_class\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/out_of_set_net_final/logs/out_of_set_final-run-20190110141730/\n",
      "Epoch: 0 Loss: 160.542\n",
      "Epoch: 1 Loss: 27.1398\n",
      "Epoch: 2 Loss: 19.4816\n",
      "Epoch: 3 Loss: 20.1872\n",
      "Epoch: 4 Loss: 16.3476\n",
      "Epoch: 5 Loss: 20.6347\n",
      "Did 3330 of loss minimized training in 4506.076171875 seconds.\n"
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "epochs = 6\n",
    "steps_between_test_save = 1\n",
    "batch = 60\n",
    "train_size = 40000\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "lowest_like = 10000\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    init.run()\n",
    "    saver_class.restore(sess, out_of_set_net_model_class)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    like_sum, like_val = sess.run([likelihood_loss_summary, tan_loss], feed_dict = {handle: val_handle, training: False})\n",
    "    filewriter.add_summary(like_sum, step)\n",
    "    print(\"Epoch: \" + str(step) + \" Loss: \" + str(like_val))\n",
    "    \n",
    "    step = step + 1\n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([tan_train_op], feed_dict={handle: train_handle})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0 and step != 0) :\n",
    "            like_sum, like_val = sess.run([likelihood_loss_summary, tan_loss], feed_dict = {handle: val_handle, training: False})\n",
    "            filewriter.add_summary(like_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(like_val))\n",
    "            saver_tan.save(sess, out_of_set_net_model_tan)\n",
    "            if lowest_like > like_val:\n",
    "                lowest_like = like_val\n",
    "                saver_tan.save(sess, out_of_set_net_best)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_tan.save(sess, out_of_set_net_model_tan)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step_tan)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Threshold\n",
    "\n",
    "Here we are going to learn the correct threshold for all classes trained by the TAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_final/model/out_of_set_net_best\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/out_of_set_net_final/logs/out_of_set_final-run-20190110141730/\n",
      "Done training averages in: 908.9003069400787\n",
      "Final Averages: [array([ 31.81084824,   9.81499481,  11.51175213,   2.74386501,\n",
      "         4.02430487, -17.06687164, -21.07948303, -22.61405754,\n",
      "        -9.91632748, -25.84106445, -11.27870846,   3.46301913,\n",
      "       -29.06341553, -29.09570503,   2.85691929, -36.36833954,\n",
      "       -16.77614784, -29.36024284, -19.91014099, -18.42787361,\n",
      "       -17.84228134, -17.21875191, -19.73221207, -11.82461262,\n",
      "       -22.67062187, -32.25492096, -38.15401459, -26.04125404,\n",
      "       -25.18882751, -15.63445187, -24.83739853, -24.50759697,\n",
      "       -21.60658264, -28.74054718, -33.57144165, -26.42733383,\n",
      "       -28.88410568, -28.8621254 , -20.81940842, -27.23272514,\n",
      "       -22.75150681, -24.89483643, -30.61316109, -20.02762222,\n",
      "       -18.71575165, -20.98169136, -18.97211838, -21.74357033,\n",
      "       -19.90183449,  -9.29535866, -32.52500534, -24.38040733,\n",
      "       -24.99948311, -13.17357063, -23.9590435 , -10.99210548,\n",
      "       -26.34570312, -27.21969032, -24.62911606, -23.82633018,\n",
      "       -21.98352242, -27.51404572, -29.80410576, -31.88154984,\n",
      "       -26.94120407, -33.07702255, -22.08308029, -29.29294205,\n",
      "       -21.21476936, -26.06631279, -26.38922691, -23.15083313,\n",
      "       -33.08617401, -21.30015755, -28.39855194, -13.15670872,\n",
      "       -30.44900322, -10.73868465, -38.55459976, -21.48721123], dtype=float32)]\n",
      "Final Std: [array([ 8.78358459,  8.86146641,  8.66390228,  8.44999409,  8.63873291,\n",
      "        5.28398228,  5.58110428,  4.27393103,  4.97536993,  3.57754135,\n",
      "        6.07208729,  9.63190746,  4.017735  ,  4.38144875,  8.37746906,\n",
      "        5.07650566,  5.29887724,  4.20722914,  4.83740711,  6.40874434,\n",
      "        6.24869919,  4.929111  ,  4.72611046,  6.10953712,  4.85769653,\n",
      "        5.49901724,  5.80581379,  4.3880682 ,  4.88688278,  4.62598324,\n",
      "        4.98695278,  4.15960503,  3.26771951,  4.36036968,  4.69223356,\n",
      "        5.20481348,  4.14914942,  4.59407997,  4.42878532,  4.39921904,\n",
      "        4.11530542,  4.48519611,  4.72849798,  3.92284369,  4.38115311,\n",
      "        4.49528265,  4.76325274,  5.76782656,  4.09502888,  6.52863216,\n",
      "        4.71495485,  4.90292168,  4.57740784,  5.93913031,  4.81108236,\n",
      "        6.18086004,  4.59777498,  4.22835398,  5.3086319 ,  5.91752577,\n",
      "        4.9890995 ,  4.34674454,  5.16431093,  5.02942944,  4.80014658,\n",
      "        4.04792166,  4.41305637,  4.81929779,  5.27565145,  4.36299086,\n",
      "        4.84018803,  4.41969633,  5.19976282,  5.3947053 ,  4.15704823,\n",
      "        5.65527296,  3.96638322,  5.34575939,  5.13513756,  6.05151367], dtype=float32)]\n",
      "Samples Counted: [array([ 499.,  500.,  500.,  500.,  499.,  500.,  500.,  500.,  500.,\n",
      "        497.,  500.,  500.,  500.,  499.,  499.,  499.,  500.,  500.,\n",
      "        499.,  500.,  500.,  499.,  500.,  500.,  499.,  499.,  498.,\n",
      "        499.,  500.,  500.,  500.,  500.,  500.,  500.,  500.,  500.,\n",
      "        500.,  499.,  499.,  500.,  499.,  499.,  499.,  500.,  500.,\n",
      "        500.,  500.,  500.,  500.,  498.,  500.,  500.,  499.,  500.,\n",
      "        500.,  499.,  499.,  500.,  499.,  500.,  500.,  499.,  499.,\n",
      "        500.,  499.,  500.,  500.,  499.,  499.,  499.,  499.,  500.,\n",
      "        498.,  499.,  500.,  500.,  500.,  499.,  498.,  499.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "epochs = 1\n",
    "steps_between_test_save = 1\n",
    "batch = 60\n",
    "train_size = 40000\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_tan.restore(sess, out_of_set_net_best)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([update_average_likelihood], feed_dict={handle: train_handle})\n",
    "        step = step + 1\n",
    "    average = sess.run([likelihood_averages],feed_dict={training:False})\n",
    "    std = sess.run([likelihood_stdev], feed_dict={training:False})\n",
    "    batch_count = sess.run([number_of_batches], feed_dict={training:False})    \n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_tan.save(sess, out_of_set_net_best)\n",
    "    end_time = time.time()\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Done training averages in: \" + str(final_time))\n",
    "    print(\"Final Averages: \" + str(average))\n",
    "    print('Final Std: ' + str(std))\n",
    "    print('Samples Counted: ' + str(batch_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do metric testing\n",
    "\n",
    "Here we will run through all of the training data and relate accuracy with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_final/model/out_of_set_net_best\n",
      "Finished Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "out_of_set_net_model = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_best'\n",
    "\n",
    "batch = 1\n",
    "epochs = 10000\n",
    "\n",
    "#Set up frame\n",
    "column_list = ['Item_Number', 'file', 'Correct_Category', 'Estimated_Category', 'Max_Softmax_Value','Likelihood',\n",
    "               'In-vs-Out', 'In-Likelihood', 'Out-Likelihood', 'Estimated-In-vs-Out', 'Category_Likelihood_Average', 'Category_Likelihood_Stdev', 'Category_Sample_Count']\n",
    "full_data_frame = pd.DataFrame(columns=column_list)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_best)\n",
    "    \n",
    "    #Initialize Data\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list_all, batch_size: batch, num_epochs:epochs})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        y_val_correct, filename_val, y_guess, likelihood_val, softmax_layer, quartile, l_avg, l_stddev, l_count = sess.run([y, file, max_softmax_val, tan_likelihoods, softmax, bottom_quartile, \n",
    "                                                                                                 likelihood_averages, likelihood_stdev, number_of_batches], \n",
    "                                                                   feed_dict={handle: val_handle, training:False})\n",
    "        \n",
    "        for j in range(batch):\n",
    "            file_string = filename_val[j].decode(\"utf-8\")\n",
    "\n",
    "            case_number = (i * batch) + j\n",
    "            correct_category_value = labels_all[y_val_correct[j]]\n",
    "            \n",
    "            guess_category = labels[y_guess[j]]\n",
    "            \n",
    "            \n",
    "            valid_quartile = quartile[y_guess[j]]\n",
    "            like_avgs = l_avg[y_guess[j]]\n",
    "            like_std = l_stddev[y_guess[j]]\n",
    "            like_count = l_count[y_guess[j]]\n",
    "            \n",
    "            softmax_value = softmax_layer[j][y_guess[j]]\n",
    "            likes_values = likelihood_val[j]\n",
    "            in_v_out = 'Out'\n",
    "            if labels_all[y_val_correct[j]] in labels:\n",
    "                in_v_out = 'In'\n",
    "\n",
    "            In_Like = 'NA'\n",
    "            Out_Like = 'NA'\n",
    "            \n",
    "            if in_v_out == 'Out':\n",
    "                Out_Like = likes_values\n",
    "            else:\n",
    "                In_Like = likes_values\n",
    "                \n",
    "            est_in_v_out = 'Out'\n",
    "            if likes_values > valid_quartile:\n",
    "                est_in_v_out = 'In'\n",
    "\n",
    "            to_add = pd.DataFrame([[case_number,file_string,correct_category_value,guess_category,softmax_value,likes_values, in_v_out, In_Like, Out_Like, est_in_v_out,\n",
    "                                   like_avgs, like_std, like_count]], columns = column_list)\n",
    "            full_data_frame = full_data_frame.append(to_add)\n",
    "        if ((i % 200) == 0):\n",
    "            print(\"Finished Epoch: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Number</th>\n",
       "      <th>file</th>\n",
       "      <th>Correct_Category</th>\n",
       "      <th>Estimated_Category</th>\n",
       "      <th>Max_Softmax_Value</th>\n",
       "      <th>Likelihood</th>\n",
       "      <th>In-vs-Out</th>\n",
       "      <th>In-Likelihood</th>\n",
       "      <th>Out-Likelihood</th>\n",
       "      <th>Estimated-In-vs-Out</th>\n",
       "      <th>Category_Likelihood_Average</th>\n",
       "      <th>Category_Likelihood_Stdev</th>\n",
       "      <th>Category_Sample_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>76.png</td>\n",
       "      <td>otter</td>\n",
       "      <td>otter</td>\n",
       "      <td>0.998008</td>\n",
       "      <td>-25.496887</td>\n",
       "      <td>In</td>\n",
       "      <td>-25.4969</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-26.427334</td>\n",
       "      <td>5.204813</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>79.png</td>\n",
       "      <td>wolf</td>\n",
       "      <td>wolf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.791168</td>\n",
       "      <td>In</td>\n",
       "      <td>16.7912</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-10.738685</td>\n",
       "      <td>5.345759</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>81.png</td>\n",
       "      <td>tulips</td>\n",
       "      <td>rabbit</td>\n",
       "      <td>0.994879</td>\n",
       "      <td>-23.882523</td>\n",
       "      <td>In</td>\n",
       "      <td>-23.8825</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-20.981691</td>\n",
       "      <td>4.495283</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>39.png</td>\n",
       "      <td>bee</td>\n",
       "      <td>sunflowers</td>\n",
       "      <td>0.631561</td>\n",
       "      <td>-29.560852</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-29.5609</td>\n",
       "      <td>In</td>\n",
       "      <td>-29.804106</td>\n",
       "      <td>5.164311</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>44.png</td>\n",
       "      <td>wolf</td>\n",
       "      <td>wolf</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.617004</td>\n",
       "      <td>In</td>\n",
       "      <td>0.617004</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-10.738685</td>\n",
       "      <td>5.345759</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>22.png</td>\n",
       "      <td>sunflowers</td>\n",
       "      <td>sunflowers</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-44.803879</td>\n",
       "      <td>In</td>\n",
       "      <td>-44.8039</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-29.804106</td>\n",
       "      <td>5.164311</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>53.png</td>\n",
       "      <td>palm</td>\n",
       "      <td>palm</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-36.769287</td>\n",
       "      <td>In</td>\n",
       "      <td>-36.7693</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-28.884106</td>\n",
       "      <td>4.149149</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99.png</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>mushrooms</td>\n",
       "      <td>0.960464</td>\n",
       "      <td>-21.032516</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-21.0325</td>\n",
       "      <td>In</td>\n",
       "      <td>-24.507597</td>\n",
       "      <td>4.159605</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>28.png</td>\n",
       "      <td>skyscraper</td>\n",
       "      <td>skyscraper</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.544937</td>\n",
       "      <td>In</td>\n",
       "      <td>-1.54494</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-26.345703</td>\n",
       "      <td>4.597775</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>52.png</td>\n",
       "      <td>tractor</td>\n",
       "      <td>tractor</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-40.556427</td>\n",
       "      <td>In</td>\n",
       "      <td>-40.5564</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-26.066313</td>\n",
       "      <td>4.362991</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Number    file Correct_Category Estimated_Category  Max_Softmax_Value  \\\n",
       "0           0  76.png            otter              otter           0.998008   \n",
       "0           1  79.png             wolf               wolf           1.000000   \n",
       "0           2  81.png           tulips             rabbit           0.994879   \n",
       "0           3  39.png              bee         sunflowers           0.631561   \n",
       "0           4  44.png             wolf               wolf           0.999986   \n",
       "0           5  22.png       sunflowers         sunflowers           1.000000   \n",
       "0           6  53.png             palm               palm           1.000000   \n",
       "0           7  99.png        butterfly          mushrooms           0.960464   \n",
       "0           8  28.png       skyscraper         skyscraper           1.000000   \n",
       "0           9  52.png          tractor            tractor           1.000000   \n",
       "\n",
       "   Likelihood In-vs-Out In-Likelihood Out-Likelihood Estimated-In-vs-Out  \\\n",
       "0  -25.496887        In      -25.4969             NA                  In   \n",
       "0   16.791168        In       16.7912             NA                  In   \n",
       "0  -23.882523        In      -23.8825             NA                 Out   \n",
       "0  -29.560852       Out            NA       -29.5609                  In   \n",
       "0    0.617004        In      0.617004             NA                  In   \n",
       "0  -44.803879        In      -44.8039             NA                 Out   \n",
       "0  -36.769287        In      -36.7693             NA                 Out   \n",
       "0  -21.032516       Out            NA       -21.0325                  In   \n",
       "0   -1.544937        In      -1.54494             NA                  In   \n",
       "0  -40.556427        In      -40.5564             NA                 Out   \n",
       "\n",
       "   Category_Likelihood_Average  Category_Likelihood_Stdev  \\\n",
       "0                   -26.427334                   5.204813   \n",
       "0                   -10.738685                   5.345759   \n",
       "0                   -20.981691                   4.495283   \n",
       "0                   -29.804106                   5.164311   \n",
       "0                   -10.738685                   5.345759   \n",
       "0                   -29.804106                   5.164311   \n",
       "0                   -28.884106                   4.149149   \n",
       "0                   -24.507597                   4.159605   \n",
       "0                   -26.345703                   4.597775   \n",
       "0                   -26.066313                   4.362991   \n",
       "\n",
       "   Category_Sample_Count  \n",
       "0                  500.0  \n",
       "0                  499.0  \n",
       "0                  500.0  \n",
       "0                  499.0  \n",
       "0                  499.0  \n",
       "0                  499.0  \n",
       "0                  500.0  \n",
       "0                  500.0  \n",
       "0                  499.0  \n",
       "0                  499.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the dataframe\n",
    "\n",
    "full_data_frame.to_csv('Out_Of_Set_Tan_Final.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Testing\n",
    "\n",
    "Here are some helpful scripts for doing additional testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory for all the models saved during training\n",
    "out_of_set_net_model_class = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_class'\n",
    "out_of_set_net_model_tan = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_tan'\n",
    "out_of_set_net_best = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_best'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Pooling\n",
    "\n",
    "Here we need to test to see that the pooling does a decent job of capturing the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_model_tan)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: 10, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    code, encode, cat = sess.run([cnn_code_layer,AutoCode,y],feed_dict={handle:val_handle, training:False})\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum(code[0])\n",
    "b = sum(encode[0])\n",
    "print(a,b)\n",
    "code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.array(range(0,cnn_code_layer_size))\n",
    "\n",
    "code_frame = pd.DataFrame([x_range,code[0]]).transpose()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "code_frame.plot(kind='scatter', x=0, y=1)\n",
    "#plt.axis('on', ylim=(0,1))\n",
    "#plt.axes(ylim=(0,1), xlim=(0,5000))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,108),encode[0])\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the code as a graph of tensor values coming out\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,cnn_code_layer_size),code[0])\n",
    "plt.axis('on')\n",
    "plt.savefig('original_code.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare to the decode as a graph of tensor values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,108),encode[0])\n",
    "plt.axis('on')\n",
    "plt.savefig('condensed_code.jpeg')\n",
    "plt.show()\n",
    "cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure Out Threshold\n",
    "\n",
    "Test code for finding a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_model_tan)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: train_list, batch_size: 30, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    \n",
    "    average_before = sess.run([likelihood_averages],feed_dict={handle:val_handle, training:False})\n",
    "    std_before = sess.run([likelihood_stdev], feed_dict={handle:val_handle, training:False})\n",
    "    sess.run([update_average_likelihood ], feed_dict={handle:val_handle, training:False})\n",
    "    std_after = sess.run([likelihood_stdev], feed_dict={handle:val_handle, training:False})\n",
    "    average_after = sess.run([likelihood_averages], feed_dict={handle:val_handle, training:False})\n",
    "    sess.run([update_average_likelihood ], feed_dict={handle:val_handle, training:False})\n",
    "    std_after_2 = sess.run([likelihood_stdev], feed_dict={handle:val_handle, training:False})\n",
    "    average_after_2 = sess.run([likelihood_averages], feed_dict={handle:val_handle, training:False})\n",
    "    bottom = sess.run([bottom_quartile], feed_dict={handle:val_handle, training:False})\n",
    "    counts = sess.run([number_of_batches], feed_dict={handle:val_handle, training:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_after_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_after_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the classifier\n",
    "\n",
    "Here we need to test to see that the classifier does a decent job of capturing the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_best)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: 30, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    accuracy_val, loss, guess, truth = sess.run([accuracy, loss_cat, max_softmax_val, y], \n",
    "                                            feed_dict={handle:val_handle, training:False})\n",
    "                    \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the tan\n",
    "Here we need to test to see that the tan does a decent job of capturing the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_best)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list_all, batch_size: 30, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    loss, guess, truth, like_loss, like = sess.run([loss_cat, max_softmax_val, y, tan_loss, tan_likelihoods], \n",
    "                                            feed_dict={handle: val_handle, training: False})\n",
    "                    \n",
    "like_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(like,truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data,columns=['true_class','likelihood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct Answer\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.scatter(x=x_array, y=y_array, c='r', s=40)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guess\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.scatter(x=x_guess, y=y_guess, c='r', s=40)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore graph from meta and restore variables\n",
    "    new_saver = tf.train.import_meta_graph(out_of_set_net_best + '.meta')\n",
    "    new_saver.restore(sess, out_of_set_net_best)\n",
    "    soft = tf.get_default_graph().get_tensor_by_name(\"Out_Of_Set_Classifier/Final_Layer/final_soft_max:0\")\n",
    "    input_tensor = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    val = soft.eval(feed_dict={input_tensor: cat, training: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_net_dict_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = np.reshape(last_layers, (num_epochs,batch_size,4320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, out_of_set_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    \n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" time.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
