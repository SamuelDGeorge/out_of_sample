{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed (for reproducibility)\n",
    "seed = 1000\n",
    "np.random.seed( seed )\n",
    "tf.set_random_seed( seed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm( input, is_train=True, name='batch_norm' ):\n",
    "    return tf.contrib.layers.batch_norm( \n",
    "        input, \n",
    "        decay=0.9, \n",
    "        updates_collections=None, \n",
    "        epsilon=1e-5, \n",
    "        scale=True, \n",
    "        is_training=is_train,\n",
    "        scope=name\n",
    "    )\n",
    "\n",
    "def lrelu( x, leak=0.2, name='lrelu' ):\n",
    "    return tf.maximum( x, leak * x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple convolutional layer\n",
    "Start off by creating a simple conv layer that uses relu activations and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d( input, num_channels, dh=2, dw=2, is_train=True, name='conv2d' ):\n",
    "    with tf.variable_scope( name ):\n",
    "        h = tf.get_variable( \n",
    "            'h', \n",
    "            [3,3,input.get_shape()[-1], num_channels], \n",
    "            initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        )\n",
    "        \n",
    "        conv = tf.nn.conv2d( \n",
    "            input, \n",
    "            h, \n",
    "            strides=[1,dh,dw,1], \n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "    return lrelu( batch_norm( conv, is_train=is_train, name=\"bn_\"+name ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear( input, output_size, name='linear' ):\n",
    "    with tf.variable_scope( name ):\n",
    "        input = tf.contrib.layers.flatten( input )\n",
    "        shape = input.get_shape().as_list()\n",
    "        \n",
    "        w = tf.get_variable( \n",
    "            'w', \n",
    "            [shape[1],output_size], \n",
    "            tf.float32, \n",
    "            tf.random_normal_initializer( stddev=0.01 )\n",
    "        )\n",
    "        \n",
    "        b = tf.get_variable( \n",
    "            'b', \n",
    "            [output_size], \n",
    "            initializer=tf.constant_initializer( 0 )\n",
    "        )\n",
    "        \n",
    "        return tf.matmul( input, w ) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize conv layers to create the encoder\n",
    "Several stacked conv layers will create the encoder.\n",
    "We reduce the number of pixels by a factor of 2 in both spatial dimensions and increase the number of channels.\n",
    "For simplicity, the code size is currently hard-coded at 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder( input, is_train=True ):\n",
    "    with tf.variable_scope( 'encoder' ):\n",
    "        ch = 32\n",
    "        conv = conv2d( input, ch, dh=1, dw=1, is_train=is_train, name='testConv00' )\n",
    "        ch *= 2\n",
    "        conv = conv2d( conv,  ch, dh=2, dw=2, is_train=is_train, name='testConv01' )\n",
    "        ch *= 2\n",
    "        conv = conv2d( conv,  ch, dh=2, dw=2, is_train=is_train, name='testConv02' )\n",
    "        ch *= 2\n",
    "        conv = conv2d( conv,  ch, dh=2, dw=2, is_train=is_train, name='testConv03' )\n",
    "        ch *= 2\n",
    "        conv = conv2d( conv,  ch, dh=2, dw=2, is_train=is_train, name='testConv04' )\n",
    "        \n",
    "        code = linear( conv, 128, name='lin_enc' )\n",
    "        \n",
    "    return code\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Some simple tests for the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = tf.random_normal( (64,32,32,3) )\n",
    "# code = encoder( input, is_train=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.train.MonitoredSession() as sess:\n",
    "#     code_ = sess.run( code )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple convolution transpose layer\n",
    "Convolution transpose is activated by relu and then batch-normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_tran( input, num_channels, dh=2, dw=2, is_train=True, name='conv2d_tran' ):\n",
    "    with tf.variable_scope( name ):\n",
    "        h = tf.get_variable( \n",
    "            'h', \n",
    "            [3,3,num_channels,input.get_shape()[-1]], \n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "        \n",
    "        shape = input.get_shape().as_list()\n",
    "        out_shape = tf.stack( [\n",
    "            tf.shape( input )[0], \n",
    "            shape[1] * dh, \n",
    "            shape[2] * dw, \n",
    "            num_channels\n",
    "        ] )\n",
    "        convt = tf.nn.conv2d_transpose( \n",
    "            input, \n",
    "            h, \n",
    "            output_shape=out_shape, \n",
    "            strides=[1,dh,dw,1]\n",
    "        )\n",
    "        \n",
    "    return tf.layers.batch_normalization( tf.nn.relu( convt ), training=is_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize conv^T layers to create the decoder\n",
    "Several stacked conv^T layers will create the decoder.\n",
    "The first few layers expand the pixels by a factor of 2 in both spatial dimensions and decreases the number of channels.\n",
    "After the original data size has been achieved, the output is activated by sigmoid so the reconstruction will be on the same scale as the input (0 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder( code, is_train=True ):\n",
    "    with tf.variable_scope( 'decoder' ):\n",
    "        convt = tf.expand_dims( tf.expand_dims( code, axis=1 ), axis=1 )\n",
    "        \n",
    "        convt = conv2d_tran( convt, 100, dh=2, dw=2, is_train=True, name='conv2dt00' )\n",
    "        convt = conv2d_tran( convt,  80, dh=2, dw=2, is_train=True, name='conv2dt01' )\n",
    "        convt = conv2d_tran( convt,  64, dh=2, dw=2, is_train=True, name='conv2dt02' )\n",
    "        convt = conv2d_tran( convt,  48, dh=2, dw=2, is_train=True, name='conv2dt03' )\n",
    "        convt = conv2d_tran( convt,  32, dh=2, dw=2, is_train=True, name='conv2dt04' )\n",
    "        \n",
    "        # This is simpler as a convolution than a convolution transpose\n",
    "        output = conv2d( convt, 3, dh=1, dw=1, is_train=True, name='output' )\n",
    "        \n",
    "    return tf.nn.sigmoid( output ) # smoothly \"clip\" the batch-normed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple code for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon = decoder( code )\n",
    "# with tf.train.MonitoredSession() as sess:\n",
    "#     recon_ = sess.run( recon )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder is just the encoder and the decoder in serial. We will output the reconstruction and the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder( input, is_train=True ):\n",
    "    with tf.variable_scope( 'autoencoder' ):\n",
    "        code = encoder( input )\n",
    "        recon = decoder( code )\n",
    "        \n",
    "        return recon, code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 2 cells are more testing. This time we will execute the graph. Note, this will not train the graph, just make sure that it will run. We still need to add a loss and optimizer before we can train. Additionally, we haven't loaded any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = tf.random_normal( (64,32,32,3) )\n",
    "# recon, code = autoencoder( input )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run( [tf.local_variables_initializer(), tf.global_variables_initializer()] )\n",
    "#     \n",
    "#     recon_, code_ = sess.run( [recon, code] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code2labels( code, is_train=True ):\n",
    "    with tf.variable_scope( 'code2labels' ):\n",
    "        lin    = lrelu( batch_norm( linear( code, 32, 'label00'), is_train=is_train ) )\n",
    "        labels = linear( code, 10, 'label01')\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 32\n",
    "WIDTH  = 32\n",
    "DEPTH  = 3\n",
    "\n",
    "TOT_TRAIN_EX = 10000\n",
    "TOT_TEST_EX  = 10000\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the tunable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size  = 64\n",
    "batch_size = tf.placeholder( tf.int64 )\n",
    "is_train   = tf.placeholder( tf.bool )\n",
    "train_on_full = False # True # train on our prebuilt, subset or the full set\n",
    "\n",
    "# if ( train_on_full ):\n",
    "#     num_classes = 10\n",
    "# else:\n",
    "#     num_classes = 2\n",
    "# print( \"Building one-hot vectors with {} classes\".format( num_classes ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the functions to read a single example and to augment a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser( example ):\n",
    "    features = tf.parse_single_example( \n",
    "        example, \n",
    "        features={\n",
    "            'image': tf.FixedLenFeature( [], tf.string ),\n",
    "            'label': tf.FixedLenFeature( [], tf.int64 )\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    image = tf.decode_raw( features[ 'image' ], tf.uint8 )\n",
    "    image.set_shape( [DEPTH * HEIGHT * WIDTH] )\n",
    "    \n",
    "    image = tf.cast( \n",
    "        tf.transpose( \n",
    "            tf.reshape( \n",
    "                image, [DEPTH, HEIGHT, WIDTH] ), \n",
    "            [1,2,0] ), # [0,1,2] ), #\n",
    "        tf.float32\n",
    "    ) / 255\n",
    "    label = tf.one_hot( tf.cast( features[ 'label' ], tf.int32 ), 10 )\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def augmenter( example ):\n",
    "    data, label = parser( example )\n",
    "    \n",
    "    # TODO: Augment the data\n",
    "    \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the various datasets from the tfrecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( train_on_full ):\n",
    "    training_fname = \"./data/full/cifar-010-bin/data.tfrecords\"\n",
    "    ckpt_fname = \"./ckpt/cifar-010-full_{}_{}.ckpt\".format( EPOCHS, batch_size )\n",
    "else:\n",
    "    training_fname = \"./data/partial/cifar-010-bin/data.tfrecords\"\n",
    "    ckpt_fname = \"./ckpt/cifar-010-partial_{}_{}.ckpt\".format( EPOCHS, batch_size )\n",
    "\n",
    "# Always test on the full set\n",
    "testing_fname  = \"./data/full/cifar-010-bin/test_batch.tfrecords\"\n",
    "\n",
    "training_dataset = tf.data.TFRecordDataset( training_fname )\n",
    "# Dropping the remainder makes my life easier but is suboptimal.\n",
    "# It would be better to build a network that can handle variable batch sizes.\n",
    "training_dataset = training_dataset.map( \n",
    "    augmenter,\n",
    "    num_parallel_calls=train_batch_size\n",
    ")\n",
    "training_dataset = training_dataset.batch( batch_size, drop_remainder=True )\n",
    "min_queue_ex = int( 0.4 * TOT_TRAIN_EX ) # This seems like a lot of examples\n",
    "training_dataset = training_dataset.shuffle( buffer_size=min_queue_ex + 3 * batch_size )\n",
    "\n",
    "testing_dataset = tf.data.TFRecordDataset( testing_fname )\n",
    "testing_dataset = testing_dataset.map( \n",
    "    parser,\n",
    "    num_parallel_calls=train_batch_size\n",
    ")\n",
    "testing_dataset = testing_dataset.batch( batch_size, drop_remainder=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the iterators from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure( \n",
    "    training_dataset.output_types, \n",
    "    training_dataset.output_shapes\n",
    ")\n",
    "\n",
    "images, labels = iterator.get_next()\n",
    "\n",
    "# We want to train the encoder so we can get access to the codes for likelihood estimation\n",
    "# The simplest way to do this is to train the encoder-decoder pair with some reconstruction loss\n",
    "code  = encoder( images, is_train=is_train )\n",
    "recon = decoder( code, is_train=is_train )\n",
    "est_labels = code2labels( code, is_train=is_train )\n",
    "\n",
    "recon_loss = tf.losses.mean_squared_error( recon, images )\n",
    "class_loss = tf.losses.softmax_cross_entropy( labels, est_labels )\n",
    "\n",
    "loss = 10 * recon_loss + class_loss\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay( \n",
    "    learning_rate=0.0005, \n",
    "    global_step=global_step,\n",
    "    decay_steps=int( ( 50000 / ( 2 * train_batch_size ) ) ), \n",
    "    decay_rate=0.95, \n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# train_op = tf.train.MomentumOptimizer( learning_rate=learning_rate, momentum=0.9 ).minimize( loss )\n",
    "# train_op = tf.train.AdamOptimizer( learning_rate=learning_rate ).minimize( loss )\n",
    "train_op = tf.train.RMSPropOptimizer( learning_rate ).minimize( loss )\n",
    "\n",
    "training_init_op = iterator.make_initializer( training_dataset )\n",
    "testing_init_op  = iterator.make_initializer( testing_dataset  )\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the session\n",
    "First, we run the training set. Then we run the test set. Wash, rinse, repeat until we run out of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    if not tf.train.checkpoint_exists( ckpt_fname ):\n",
    "        # Train the model if it doesn't already exist\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for ee in range( EPOCHS ):\n",
    "            \n",
    "            cnt = 0\n",
    "            train_loss = 0.0\n",
    "            # Run some number of training batches in this epoch\n",
    "            sess.run( training_init_op, feed_dict={batch_size: train_batch_size} )\n",
    "            while True:\n",
    "                try:\n",
    "                    _, loss_ = sess.run( [train_op, loss], feed_dict={is_train: True} )\n",
    "                    cnt += 1\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "                train_loss += loss_\n",
    "            \n",
    "            train_loss = train_loss / cnt\n",
    "            print( \"The average training loss at epoch {} was {} with {} steps.\".format( ee, train_loss, cnt ) )\n",
    "            \n",
    "            cnt = 0\n",
    "            test_loss = 0.0\n",
    "            # Run some number of testing batches in this epoch\n",
    "            sess.run( testing_init_op, feed_dict={batch_size: test_batch_size} )\n",
    "            while True:\n",
    "                try:\n",
    "                    loss_ = sess.run( [loss], feed_dict={is_train: False} )\n",
    "                    cnt += 1\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "                test_loss += loss_[0]\n",
    "            \n",
    "            test_loss = test_loss / cnt\n",
    "            print( \"The average testing loss at epoch {} was {} with {} steps\".format( ee, test_loss, cnt ) )\n",
    "        \n",
    "        save_path = saver.save( sess, ckpt_fname )\n",
    "        \n",
    "    else:\n",
    "        # Just load the model if we already have it\n",
    "        saver.restore( sess, ckpt_fname )\n",
    "        \n",
    "        # Run the training and testing once so we can see the error (we won't run the optimizer)\n",
    "        cnt = 0\n",
    "        train_loss = 0.0\n",
    "        sess.run( training_init_op, feed_dict={batch_size: train_batch_size} )\n",
    "        while True:\n",
    "            try:\n",
    "                loss_ = sess.run( [loss], feed_dict={is_train: False} )\n",
    "                cnt += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            train_loss += loss_[0]\n",
    "        \n",
    "        train_loss = train_loss / cnt\n",
    "        print( \"The average training loss is {} with {} steps.\".format( train_loss, cnt ) )\n",
    "        \n",
    "        cnt = 0\n",
    "        test_loss = 0.0\n",
    "        # Run some number of testing batches in this epoch\n",
    "        sess.run( testing_init_op, feed_dict={batch_size: test_batch_size} )\n",
    "        while True:\n",
    "            try:\n",
    "                loss_ = sess.run( [loss], feed_dict={is_train: False} )\n",
    "                cnt += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            test_loss += loss_[0]\n",
    "        \n",
    "        test_loss = test_loss / cnt\n",
    "        print( \"The average testing loss is {} with {} steps\".format( test_loss, cnt ) )\n",
    "    \n",
    "    sess.run( training_init_op , feed_dict={batch_size: train_batch_size})\n",
    "    train_im, train_re, train_rl, train_cl, train_lb, train_lb0 = sess.run( \n",
    "        [images, recon, recon_loss, class_loss, est_labels, labels], feed_dict={is_train: False} )\n",
    "    \n",
    "    sess.run( testing_init_op, feed_dict={batch_size: test_batch_size, is_train: False} )\n",
    "    test_im, test_re = sess.run( [images, recon], feed_dict={is_train: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( train_rl )\n",
    "print( train_cl )\n",
    "print( train_lb[3] )\n",
    "print( train_lb0[3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training with the full data set we achieve an average L2 training/testing loss of 0.0061/0.0065 after 15 epochs (with the random seed set to 1000).\n",
    "\n",
    "- With 30 epochs, the error was 0.0053/0.0055\n",
    "\n",
    "When training with the partial data set (dogs and cats only) we achieve an average L2 training/testing loss of 0.0178/0.0162 after 15 epochs (with the random seed set to 1000).\n",
    "\n",
    "We use a fifth of the data (a highly-correlated fifth) and achieve about 3x more error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize training reconstruction\n",
    "This may involve an extreme subset of the data depending on which flags were set and how the data was subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( 2, 10, figsize=(18, 3) )\n",
    "\n",
    "for nn in range( 10 ):\n",
    "    ax[0,nn].get_xaxis().set_visible( False )\n",
    "    ax[0,nn].get_yaxis().set_visible( False )\n",
    "    ax[1,nn].get_xaxis().set_visible( False )\n",
    "    ax[1,nn].get_yaxis().set_visible( False )\n",
    "    \n",
    "    img = np.squeeze( train_im[nn,:,:,:] )\n",
    "    rec = np.squeeze( train_re[nn,:,:,:] )\n",
    "    \n",
    "    ax[0,nn].imshow( img )\n",
    "    ax[1,nn].imshow( rec )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize testing reconstruction\n",
    "This always involves the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( 2, 10, figsize=(18, 3) )\n",
    "\n",
    "for nn in range( 10 ):\n",
    "    ax[0,nn].get_xaxis().set_visible( False )\n",
    "    ax[0,nn].get_yaxis().set_visible( False )\n",
    "    ax[1,nn].get_xaxis().set_visible( False )\n",
    "    ax[1,nn].get_yaxis().set_visible( False )\n",
    "    \n",
    "    img = np.squeeze( test_im[nn,:,:,:] )\n",
    "    rec = np.squeeze( test_re[nn,:,:,:] )\n",
    "    \n",
    "    ax[0,nn].imshow( img )\n",
    "    ax[1,nn].imshow( rec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore( sess, ckpt_fname )\n",
    "    \n",
    "    sess.run( training_init_op, feed_dict={batch_size: train_batch_size, is_train: False} )\n",
    "    images_, code_, recon_ = sess.run( [images, code, recon], feed_dict={is_train: False} )\n",
    "    \n",
    "# Now, we need to freeze the trained part of the network, attach a tan, and have it learn the likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the codes to try to get an idea of how separable they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore_var = tf.contrib.slim.get_variables( scope='encoder' )\n",
    "\n",
    "loader = tf.train.Saver() # restore_var )\n",
    "\n",
    "train_codes = [[] for i in range(10)]\n",
    "test_codes = [[] for i in range(10)]\n",
    "with tf.Session() as sess:\n",
    "    loader.restore( sess, ckpt_fname )\n",
    "    \n",
    "    cnt = 0\n",
    "    sess.run( training_init_op, feed_dict={batch_size: 1} )\n",
    "    while True:\n",
    "        cnt += 1\n",
    "        try:\n",
    "            code_, label_ = sess.run( [code, labels], feed_dict={is_train: False} )\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        ind = np.argmax( label_ )\n",
    "        # print( ind )\n",
    "        train_codes[ind].append( code_ )\n",
    "    \n",
    "    cnt = 0\n",
    "    sess.run( testing_init_op, feed_dict={batch_size: 1} )\n",
    "    while True:\n",
    "        cnt += 1\n",
    "        try:\n",
    "            code_, label_ = sess.run( [code, labels], feed_dict={is_train: False} )\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        ind = np.argmax( label_ )\n",
    "        # print( ind )\n",
    "        test_codes[ind].append( code_ )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = []\n",
    "test_tmp  = []\n",
    "for tt in range( 10 ):\n",
    "    try:\n",
    "        train_tmp.append( np.squeeze( np.asarray( train_codes[tt] ), 1 ).T )\n",
    "    except:\n",
    "        print( \"Bad class {}\".format( tt ) )\n",
    "    test_tmp.append( np.squeeze( np.asarray(  test_codes[tt] ), 1 ).T )\n",
    "\n",
    "train_codes = np.asarray( train_tmp )\n",
    "test_codes  = np.asarray(  test_tmp )\n",
    "\n",
    "train_W, train_U = np.linalg.eig( np.matmul( train_codes, train_codes.transpose( [0,2,1] ) ) )\n",
    "test_W,   test_U = np.linalg.eig( np.matmul(  test_codes,  test_codes.transpose( [0,2,1] ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( np.log10( test_W.T ) )\n",
    "\n",
    "# Project the test data into the training data eigenspace\n",
    "test_in_train = np.matmul( np.expand_dims( train_U, axis=0 ), np.expand_dims( test_codes, axis=1 ) )\n",
    "\n",
    "fig, ax = plt.subplots( 2, 10, figsize=(36, 3) )\n",
    "\n",
    "for nn in range( 10 ):\n",
    "    ax[0,nn].get_xaxis().set_visible( False )\n",
    "    ax[0,nn].get_yaxis().set_visible( False )\n",
    "    ax[1,nn].get_xaxis().set_visible( False )\n",
    "    ax[1,nn].get_yaxis().set_visible( False )\n",
    "    \n",
    "    cat = np.squeeze( test_in_train[nn,0,:,:] )\n",
    "    dog = np.squeeze( test_in_train[nn,1,:,:] )\n",
    "    \n",
    "    ax[0,nn].imshow( cat )\n",
    "    ax[1,nn].imshow( dog )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikely( code ):\n",
    "    with tf.variable_scope( 'loglikely' ):\n",
    "        stop_layer = tf.stop_gradient( code )\n",
    "        \n",
    "        nll, samp = tan_nll( stop_layer )\n",
    "        \n",
    "        return nll, samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll, _ = loglikely( code )\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay( \n",
    "    learning_rate=0.001, \n",
    "    global_step=global_step,\n",
    "    decay_steps=int( ( 50000 / ( 2 * train_batch_size ) ) ), \n",
    "    decay_rate=0.99, \n",
    "    staircase=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_op = tf.train.MomentumOptimizer( learning_rate=learning_rate, momentum=0.9 ).minimize( nll )\n",
    "# train_op = tf.train.AdamOptimizer( learning_rate=learning_rate ).minimize( nll )\n",
    "nll_op = tf.train.RMSPropOptimizer( learning_rate ).minimize( nll )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be preferable if we could batch the nll and get the result per sample (instead of averaged over sample).\n",
    "Obviously, for training, we don't care. But for testing per class, it would be much faster than running the tests with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLL_EPOCHS = 25\n",
    "nll_saver = tf.train.Saver()\n",
    "\n",
    "# We need to be able to track the result per class\n",
    "# The nll function is currently compressing over batch before we can get to it\n",
    "nll_test_batch_size = 1\n",
    "nll_train_batch_size = 64\n",
    "\n",
    "if ( train_on_full ):\n",
    "    nll_ckpt_fname = \"./ckpt/cifar-010-nll-full_{}_{}.ckpt\".format( NLL_EPOCHS, batch_size )\n",
    "else:\n",
    "    nll_ckpt_fname = \"./ckpt/cifar-010-nll-partial_{}_{}.ckpt\".format( NLL_EPOCHS, batch_size )\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the session\n",
    "    sess.run( tf.global_variables_initializer() )\n",
    "    \n",
    "    # Overload the encoder parameters with those already learned\n",
    "    loader.restore( sess, ckpt_fname )\n",
    "    \n",
    "    all_train_nll = np.zeros( (NLL_EPOCHS) )\n",
    "    all_test_nll = np.zeros( (10,NLL_EPOCHS) )\n",
    "    all_test_loss = np.zeros( (10,NLL_EPOCHS) )\n",
    "    for ee in range( NLL_EPOCHS ):\n",
    "        \n",
    "        cnt = 0\n",
    "        train_nll = 0.0\n",
    "        \n",
    "        # Set up the training data\n",
    "        sess.run( training_init_op, feed_dict={batch_size: nll_train_batch_size} )\n",
    "        while True:\n",
    "            try:\n",
    "                # Test the nll\n",
    "                _, nll_ = sess.run( [nll_op, nll], feed_dict={is_train: False} )\n",
    "                cnt += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            train_nll += nll_\n",
    "        \n",
    "        train_nll = train_nll / cnt\n",
    "        print( \"The average training nll at epoch {} was {} with {} steps.\".format( ee, train_nll, cnt ) )\n",
    "        all_train_nll[ee] = train_nll\n",
    "        \n",
    "        # if ee % 25 is not 0:\n",
    "        #     continue\n",
    "        \n",
    "        cnt = 0\n",
    "        # We can only track these stats at present if we use a batch size of 1\n",
    "        test_nll = np.zeros( (10) )\n",
    "        test_loss = np.zeros( (10) )\n",
    "        # Run some number of testing batches in this epoch\n",
    "        sess.run( testing_init_op, feed_dict={batch_size: nll_test_batch_size} )\n",
    "        while True:\n",
    "            try:\n",
    "                # TODO: I need the nll and label per test example\n",
    "                nll_, loss_, label_ = sess.run( [nll,loss,labels], feed_dict={is_train: False} )\n",
    "                cnt += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            ind = np.argmax( label_ )\n",
    "            test_nll[ind] += nll_\n",
    "            test_loss[ind] += loss_\n",
    "        \n",
    "        # x10 since the cnt is spread across all examples but we can only average per class\n",
    "        test_nll = 10 * test_nll / cnt\n",
    "        test_loss = 10 * test_loss / cnt\n",
    "        print( \"The average testing nll/loss at epoch {} was {}/{} with {} steps\".format( \n",
    "            ee, np.mean( test_nll ), np.mean( test_loss ), cnt ) )\n",
    "        all_test_nll[:,ee] = test_nll\n",
    "        all_test_loss[:,ee] = test_loss\n",
    "    \n",
    "    nll_save_path = nll_saver.save( sess, nll_ckpt_fname )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = np.zeros( (10) )\n",
    "print( np.shape( test_loss ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.shape( all_test_nll ) )\n",
    "print( label_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure( )\n",
    "ax = fig.add_axes( (0,0,1,1) )\n",
    "\n",
    "x = np.arange( 1, NLL_EPOCHS+1 )\n",
    "lin = ax.plot( x, np.array( all_train_nll ) )\n",
    "lin = ax.plot( x, np.mean( np.array( all_test_nll ), axis=0 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary test\n",
    "If this works as intended, we should see the nll go down for two of the lines and go up for the remaining lines.\n",
    "If it goes up for all lines it means we're overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.argmax( label_ ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure( )\n",
    "ax = fig.add_axes( (0,0,1,1) )\n",
    "\n",
    "lin = ax.plot( x, np.array( all_test_nll ).T )\n",
    "ax.legend( ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], loc='upper left' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell illustrates the loss (not nll) of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure( )\n",
    "ax = fig.add_axes( (0,0,1,1) )\n",
    "\n",
    "lin = ax.plot( x, np.mean( np.array( all_test_loss ), axis=0 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
