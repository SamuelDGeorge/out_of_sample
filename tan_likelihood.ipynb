{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a TAN on the cifar10 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tensorflow version is 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import howtotan\n",
    "tan_nll = howtotan.get_tan_nll\n",
    "\n",
    "print( \"The Tensorflow version is \" + tf.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "def read_cifar10( filename_queue ):\n",
    "    # Used cifar10_inputs.py as a guide\n",
    "    \n",
    "    height = 32\n",
    "    width  = 32\n",
    "    depth  = 3\n",
    "    \n",
    "    image_bytes = height * width * depth\n",
    "    label_bytes = 1\n",
    "    \n",
    "    reader   = tf.FixedLengthRecordReader( record_bytes=( image_bytes + label_bytes ) )\n",
    "    _, value = reader.read( filename_queue )\n",
    "    \n",
    "    data = tf.decode_raw( value, tf.uint8 )\n",
    "    \n",
    "    labels = tf.cast( \n",
    "        tf.strided_slice( data, [0], [label_bytes] ), tf.int32 )\n",
    "    data = tf.transpose( \n",
    "        tf.reshape( \n",
    "            tf.strided_slice( data, [label_bytes], [label_bytes + image_bytes] ),\n",
    "            [depth, height, width] ), \n",
    "        [1, 2, 0] )\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def build_batch( data, label, batch_size=32, shuffle=True, min_queue_ex=None):\n",
    "    # WAG at min_queue_ex - not sure I really understand what this is for...\n",
    "    \n",
    "    if ( min_queue_ex is None ):\n",
    "        min_queue_ex = 4 * batch_size\n",
    "    \n",
    "    num_pre_threads = 16\n",
    "    if ( shuffle ):\n",
    "        data_batch, label_batch = tf.train.shuffle_batch( \n",
    "            [data, label], \n",
    "            batch_size=batch_size, \n",
    "            num_threads=num_pre_threads, \n",
    "            capacity=min_queue_ex + batch_size, \n",
    "            min_after_dequeue=min_queue_ex\n",
    "        )\n",
    "    else:\n",
    "        data_batch, label_batch = tf.train.batch( \n",
    "            [data, label], \n",
    "            batch_size=batch_size, \n",
    "            num_threads=num_pre_threads, \n",
    "            capacity=min_queue_ex + batch_size\n",
    "        )\n",
    "    \n",
    "    # Visualize the training data\n",
    "    tf.summary.image( 'images', data_batch )\n",
    "    \n",
    "    return data_batch, tf.reshape( label_batch, [batch_size] )\n",
    "\n",
    "def creader( data_dir, batch_size, num_epochs ):\n",
    "    filenames = [os.path.join( data_dir, \"data_batch_{}.bin\".format( ii ) ) \n",
    "                 for ii in xrange( 1, 6 )]\n",
    "    \n",
    "    for ff in filenames:\n",
    "        if not tf.gfile.Exists( ff ):\n",
    "            raise ValueError( \"Could not find file {}\".format( ff ) )\n",
    "    \n",
    "    # It would be better not redefine this multiple times\n",
    "    height = 32\n",
    "    width  = 32\n",
    "    depth  = 3\n",
    "    \n",
    "    with tf.name_scope( 'read' ):\n",
    "        filename_queue = tf.train.string_input_producer( filenames, num_epochs=num_epochs )\n",
    "        \n",
    "        data, labels = read_cifar10( filename_queue )\n",
    "        data = tf.cast( data, tf.float32 ) / 255 # why do this here and not above?\n",
    "        # Data is between zero and 1\n",
    "        \n",
    "        # Zero mean, unit variance\n",
    "        # data = tf.image.per_image_standardization( data )\n",
    "        \n",
    "        data.set_shape( [height, width, depth])\n",
    "        labels.set_shape( [1] )\n",
    "        # What does set_shape do?\n",
    "        \n",
    "        # Return the data and the labels in the batch\n",
    "        return build_batch( data, labels, batch_size, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:Exception in QueueRunner: Attempting to use uninitialized value read/input_producer/limit_epochs/epochs\n",
      "\t [[Node: read/input_producer/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, limit=10000, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](read/input_producer/limit_epochs/epochs)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueRunnerThread-read/input_producer-read/input_producer/input_producer_EnqueueMany:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\n",
      "    enqueue_callable()\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1205, in _single_operation_run\n",
      "    self._call_tf_sessionrun(None, {}, [], target_list, None)\n",
      "  File \"/home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value read/input_producer/limit_epochs/epochs\n",
      "\t [[Node: read/input_producer/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, limit=10000, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](read/input_producer/limit_epochs/epochs)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session( )\n",
    "\n",
    "num_epochs  = 10000\n",
    "batch_size  = 32\n",
    "data_dir    = \"./data/partial/cifar-010-bin/\"\n",
    "code_length = 100;\n",
    "\n",
    "with tf.device( '/cpu:0' ):\n",
    "    data_batch, labels_batch = creader( data_dir, batch_size, num_epochs )\n",
    "    \n",
    "    # filename_queue = tf.train.string_input_producer( [\"./data/partial/cifar-010-bin/data_batch_1.bin\"] )\n",
    "    # \n",
    "    # reader = tf.FixedLengthRecordReader( record_bytes=( 3073 ) )\n",
    "    # _, value = reader.read( filename_queue )\n",
    "    # \n",
    "    # data = tf.decode_raw( value, tf.uint8 )\n",
    "\n",
    "data_flat = tf.reshape( data_batch, [-1,32*32*3] )\n",
    "\n",
    "inputs = data_flat\n",
    "inputs_shape = inputs.get_shape().as_list()[1:]\n",
    "\n",
    "\n",
    "tf.train.start_queue_runners(sess)\n",
    "\n",
    "tmp = sess.run( [data_flat] )\n",
    "\n",
    "print( tmp )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = True\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Global step\n",
    "    global_step = tf.Variable( 0, trainable=False )\n",
    "    \n",
    "    # Input batch\n",
    "    data_batch = tf.placeholder( tf.float32, shape=[batch_size,32,32,3] )\n",
    "    \n",
    "    # Conv layer 1\n",
    "    conv1 = tf.layers.conv2d( inputs=data_batch, \n",
    "                              filters=32, \n",
    "                              kernel_size=[3,3],\n",
    "                              padding='same', \n",
    "                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                              activation=tf.nn.relu\n",
    "                            )\n",
    "    \n",
    "    conv_output = tf.contrib.layers.flatten( tf.layers.batch_normalization( conv1, training=is_train ) )\n",
    "    \n",
    "    code_layer = tf.layers.dense( inputs=conv_output, \n",
    "                                  units=code_length, \n",
    "                                  activation=tf.nn.relu\n",
    "                                )\n",
    "    \n",
    "    code = tf.layers.batch_normalization( code_layer, training=is_training )\n",
    "    \n",
    "    code_output = tf.layers.dense( inputs=code, \n",
    "                                   units=32*32*3,\n",
    "                                   activation=tf.nn.relu\n",
    "                                 )\n",
    "    \n",
    "    tconv_input = tf.reshape( code_output, [batch_size,32,32,3] )\n",
    "    \n",
    "    tconv1 = tf.layers.conv2d_transpose( inputs=tconv_input,\n",
    "                                         filters=3, \n",
    "                                         padding='same', \n",
    "                                         kernel_size=[3,3], \n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                         activation=tf.nn.sigmoid\n",
    "                                       )\n",
    "    \n",
    "    loss = tf.nn.l2_loss( data_batch - tconv1 )\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay( learning_rate=1e-4, \n",
    "                                                global_step=global_step, \n",
    "                                                decay_steps=10000, \n",
    "                                                decay_rate=0.95, \n",
    "                                                staircase=True\n",
    "                                              )\n",
    "    \n",
    "    trainer = tf.train.AdamOptimizer( learning_rate )\n",
    "    training_step = trainer.minimize( loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run( [tf.local_variables_initializer(), tf.global_variables_initializer()] )\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners( sess=sess, coord=coord )\n",
    "    \n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            _, loss_step = sess.run( [training_step, loss], {data_batch: data_batch} )\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join( threads )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print( tmp )\n",
    "print( tf.shape( tconv1 ) )\n",
    "print( tf.shape( data_batch ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err+1\n",
    "\n",
    "data_rand = tf.random_normal(\n",
    "    [2,100],\n",
    "    mean=0.0,\n",
    "    stddev=1.0,\n",
    "    dtype=tf.float32,\n",
    "    seed=None,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "likeli, _ = tan_nll( data_rand )\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tmp, tmp_nll, samp = sess.run( [data_rand, likeli, _] )\n",
    "\n",
    "\n",
    "# with sess:\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(coord=coord)\n",
    "#     \n",
    "#     tmp = data_batch.eval( )\n",
    "#     # print( likeli.eval( ) )\n",
    "#     \n",
    "#     coord.request_stop()\n",
    "#     coord.join( threads )\n",
    "\n",
    "# print( value.eval() )\n",
    "# value = sess.run( [value] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass it through the tan\n",
    "# print( tmp )\n",
    "\n",
    "import numpy as np\n",
    "print( np.shape( tmp ) )\n",
    "print( np.shape( tmp_nll ) )\n",
    "\n",
    "print( tmp_nll )\n",
    "\n",
    "print( np.shape( samp ) )\n",
    "\n",
    "\n",
    "# Use some optimizer\n",
    "\n",
    "\n",
    "# Run the session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test how well we performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a LL bound (should use some held-out, non-test data)\n",
    "#   May need to modify the subsampling process to produce this bounding set\n",
    "\n",
    "# Run the (full) test set through the trained model\n",
    "\n",
    "\n",
    "# Plot the average likelihood per class\n",
    "#   We want to see high likelihoods for cats and dogs and low likelihoods on everything else"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
