{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib # since we are actively modifying the underlying modules it is very helpful \n",
    "                 # to be able to include changes without restarting the kernel with \n",
    "                 # importlib.reload( the_mod )\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import autoencoder as ae\n",
    "import c10_reader as rdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed (for reproducibility)\n",
    "seed = 1000\n",
    "np.random.seed( seed )\n",
    "tf.set_random_seed( seed )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Save the full model\n",
    "- Load the model and pass in the placeholder?\n",
    "- Clean up some of the visualization methods and make it easier to push the data into an eigenspace\n",
    "- Better autoencoder architecture?\n",
    "- Better loss functions? (not just L2 for recon...)\n",
    "- Create new training loop based on the fixed codes\n",
    "   - Make sure to save the ckpt in a recognizable way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = rdr.HEIGHT\n",
    "WIDTH  = rdr.WIDTH\n",
    "DEPTH  = rdr.DEPTH\n",
    "\n",
    "TOT_CLASS_NUM = rdr.TOT_CLASS_NUM\n",
    "TOT_TRAIN_EX  = rdr.TRAIN_EX\n",
    "TOT_TEST_EX   = rdr.TEST_EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_dim =  32 # fixed for now\n",
    "code_dim = 128 # fixed for now\n",
    "\n",
    "joint_epochs = 20\n",
    "code_epochs  = 1\n",
    "\n",
    "train_batch_size = 64\n",
    "valid_batch_size = 64\n",
    "test_batch_size  = 64\n",
    "batch_size = tf.placeholder( tf.int64 )\n",
    "is_train   = tf.placeholder( tf.bool )\n",
    "\n",
    "train_on_full = False # True # train on our prebuilt, subset or the full set\n",
    "train_classes = [ ii for ii in range( 8 ) ] # list of classes to train on\n",
    "test_classes  = None # [ 3 ] # list of (additional) classes to test on (the out-of-distribution)\n",
    "                      # None uses all classes\n",
    "\n",
    "is_joint_class = not True\n",
    "is_joint_ocnn  = not True\n",
    "squish_classes = True\n",
    "\n",
    "nu = 1.0 # WAG\n",
    "H  = 16 # OC hidden layer size\n",
    "# oc_nonlin = tf.nn.relu  # OC hidden activation\n",
    "oc_nonlin = tf.identity # OC hidden activation\n",
    "\n",
    "class_num = TOT_CLASS_NUM\n",
    "\n",
    "train_tfr_name = \"./data/tfr/cifar-010/train.tfrecords\"\n",
    "valid_tfr_name = \"./data/tfr/cifar-010/validate.tfrecords\"\n",
    "test_tfr_name  = \"./data/tfr/cifar-010/test.tfrecords\"\n",
    "\n",
    "# if ( train_on_full ):\n",
    "#     num_classes = 10\n",
    "# else:\n",
    "#     num_classes = 2\n",
    "# print( \"Building one-hot vectors with {} classes\".format( num_classes ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( is_joint_class ):\n",
    "    fol_name = 'yesClass_'\n",
    "else: \n",
    "    fol_name = 'noClass_'\n",
    "\n",
    "if ( is_joint_ocnn ):\n",
    "    fol_name += 'yesOc'\n",
    "else:\n",
    "    fol_name += 'noOc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path, train_name = os.path.split( train_tfr_name )\n",
    "_,         valid_name = os.path.split( valid_tfr_name )\n",
    "_,          test_name = os.path.split(  test_tfr_name )\n",
    "\n",
    "os.makedirs( os.path.join( data_path, fol_name ), exist_ok=True )\n",
    "train_code_tfr_name = os.path.join( data_path, fol_name, \"code_{}_\".format( code_dim ) + train_name )\n",
    "valid_code_tfr_name = os.path.join( data_path, fol_name, \"code_{}_\".format( code_dim ) + valid_name )\n",
    "test_code_tfr_name  = os.path.join( data_path, fol_name, \"code_{}_\".format( code_dim ) +  test_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = [ii for ii in range( TOT_CLASS_NUM )]\n",
    "\n",
    "if ( squish_classes and train_on_full ):\n",
    "    load_classes = all_classes\n",
    "    sep_classes = [ii for ii in range( len( train_classes ) )]\n",
    "    merge_classes = list( set( all_classes ) - set( train_classes ) )\n",
    "    cl_name = 'squished'\n",
    "    class_num = len( train_classes ) + 1\n",
    "elif ( train_on_full ):\n",
    "    load_classes = all_classes\n",
    "    sep_classes = all_classes\n",
    "    merge_classes = []\n",
    "    cl_name = 'full'\n",
    "    class_num = 10\n",
    "else:\n",
    "    load_classes = train_classes\n",
    "    sep_classes = [ii for ii in range( len( train_classes ) )]\n",
    "    merge_classes = []\n",
    "    cl_name = 'expanded'\n",
    "    class_num = len( train_classes ) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will erase any existing C10 TFR and replace them with the new data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdr.subset2tfr( [ 1, 2, 3, 4 ], class_nums=load_classes, output_file=train_tfr_name )            # first 4 binary files\n",
    "rdr.subset2tfr( [ 5 ],          class_nums=load_classes, output_file=valid_tfr_name )            # last binary file\n",
    "rdr.test2tfr( sep_classes=load_classes, retain_classes=test_classes, output_file=test_tfr_name ) # testing binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint file set to: ./ckpt_128/cifar-010/noClass_noOc/expanded_in01234567_20.ckpt.\n",
      "Image folder set to ./images/cifar-010/noClass_noOc.\n"
     ]
    }
   ],
   "source": [
    "out_folder = \"cifar-010/{}\".format( fol_name ) \n",
    "out_file   = \"{}_in{}_{}\".format( \n",
    "    cl_name, ''.join( str( _ ) for _ in load_classes ), joint_epochs\n",
    ")\n",
    "if ( is_joint_ocnn ):\n",
    "    if ( nu < 0.01 ) or ( nu > 1.0 ):\n",
    "        print( \"File name expects nu to be O(0.01)-O(1.0)\")\n",
    "    out_file = \"p{}_\".format( int( 100 * nu ) ) + out_file\n",
    "\n",
    "ckpt_folder  = os.path.join( './ckpt_{}'.format( code_dim ), out_folder )\n",
    "image_folder = os.path.join( './images', out_folder )\n",
    "\n",
    "os.makedirs(  ckpt_folder, exist_ok=True )\n",
    "os.makedirs( image_folder, exist_ok=True )\n",
    "\n",
    "ckpt_fname = os.path.join( ckpt_folder, out_file + \".ckpt\" )\n",
    "\n",
    "print( \"Checkpoint file set to: {}.\".format( ckpt_fname ) ) # os.path.join( ckpt_folder, out_file ) ) )\n",
    "print( \"Image folder set to {}.\".format( image_folder ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator, training_dataset, valid_dataset, testing_dataset = rdr.build_data_iterator( \n",
    "    train_tfr_name, \n",
    "    valid_tfr_name, \n",
    "    test_tfr_name, \n",
    "    sep_classes=sep_classes, \n",
    "    batch_size=batch_size, \n",
    "    train_batch_size=train_batch_size, \n",
    "    valid_batch_size=valid_batch_size, \n",
    "    test_batch_size=test_batch_size, \n",
    "    drop_remainder=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1class_dist( data, name='one_class_dist' ):\n",
    "    with tf.variable_scope( name ):\n",
    "        w_shape = [data.get_shape().as_list()[1],1];\n",
    "        \n",
    "        w = tf.random_normal( w_shape, mean=0, stddev=1, name='hyper_normal' )\n",
    "        \n",
    "    return tf.matmul( data, w ), w\n",
    "\n",
    "def get_1class_loss( dist, r, nu, name='one_class' ):\n",
    "    with tf.variable_scope( name ):\n",
    "        \n",
    "        main_loss = tf.reduce_mean( tf.nn.relu( r - dist ) ) / nu - r\n",
    "        \n",
    "    return main_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tf.Variable( 1.0, name='hyper_distance' )\n",
    "# r = tf.constant( 10.0, dtype=tf.float32, name='hyper_distance' )\n",
    "\n",
    "V = tf.get_variable( \n",
    "    'V', \n",
    "    [code_dim,H], \n",
    "    tf.float32, \n",
    "    tf.random_normal_initializer( stddev=0.01 )\n",
    ")\n",
    "\n",
    "V_norm = tf.norm( V, ord=2 ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_iter, labels_iter = iterator.get_next()\n",
    "\n",
    "images = tf.placeholder_with_default( images_iter, images_iter.get_shape(), name='images' )\n",
    "labels = tf.placeholder_with_default( labels_iter, labels_iter.get_shape(), name='labels' )\n",
    "\n",
    "# We want to train the encoder so we can get access to the codes for likelihood estimation\n",
    "# The simplest way to do this is to train the encoder-decoder pair with some reconstruction loss\n",
    "code  = ae.encoder( images, is_train=is_train, code_dim=code_dim )\n",
    "recon = ae.decoder( code, is_train=is_train )\n",
    "\n",
    "recon_loss = tf.losses.mean_squared_error( recon, images )\n",
    "\n",
    "loss = recon_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additional losses (based on classification or 1-class SVM ideas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( is_joint_class ):\n",
    "    cl_code = code\n",
    "else:\n",
    "    cl_code = tf.stop_gradient( code )\n",
    "\n",
    "est_labels = ae.code2labels( cl_code, is_train=is_train, class_num=class_num )\n",
    "class_loss = tf.losses.softmax_cross_entropy( labels, est_labels )\n",
    "\n",
    "if ( is_joint_class ):\n",
    "    loss += class_loss / 100\n",
    "    print( \"Jointly training a classifier on the codes.\" )\n",
    "\n",
    "    \n",
    "if ( is_joint_ocnn ):\n",
    "    oc_code = code\n",
    "else:\n",
    "    oc_code = tf.stop_gradient( code )\n",
    "# pre_one_class = ae.linear( code, 8 )\n",
    "\n",
    "with tf.variable_scope( 'one_class' ):\n",
    "    pre_one_class = oc_nonlin( tf.matmul( oc_code, V ) )\n",
    "    one_class_dist, oc_norm = get_1class_dist( pre_one_class )\n",
    "    one_class_loss = get_1class_loss( one_class_dist, r, nu )\n",
    "\n",
    "if ( is_joint_ocnn ):\n",
    "    loss += one_class_loss + V_norm + tf.norm( oc_norm, ord=2 ) / 2\n",
    "    print( \"Imposing a one-class SVM-like loss on the codes.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay( \n",
    "    learning_rate=0.0005, \n",
    "    global_step=global_step,\n",
    "    decay_steps=int( ( 50000 / ( 2 * train_batch_size ) ) ), \n",
    "    decay_rate=0.95, \n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# train_op = tf.train.MomentumOptimizer( learning_rate=learning_rate, momentum=0.9 ).minimize( loss )\n",
    "# train_op = tf.train.AdamOptimizer( learning_rate=learning_rate ).minimize( loss )\n",
    "train_op = tf.train.RMSPropOptimizer( learning_rate ).minimize( loss )\n",
    "\n",
    "training_init_op = iterator.make_initializer( training_dataset )\n",
    "valid_init_op    = iterator.make_initializer( valid_dataset )\n",
    "testing_init_op  = iterator.make_initializer( testing_dataset  )\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_ckpt_files = [ ff for ff in os.listdir( ckpt_folder ) if os.path.isfile( os.path.join( ckpt_folder, ff ) ) ]\n",
    "\n",
    "# for ff in cand_ckpt_files:\n",
    "#     if ( out_file in ff ):    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to be able to save checkpoints as I go\n",
    "\n",
    "What I'll do, is create a tmp folder based on the name of the final ckpt file\n",
    "\n",
    "I'll save intermediate ckpts to that file\n",
    "When the final checkpoint has been saved, I'll delete the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parameter.\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_128/cifar-010/noClass_noOc/expanded_in01234567_20.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if not tf.train.checkpoint_exists( ckpt_fname ):\n",
    "        # Train the model if it doesn't already exist\n",
    "        print( 'Checkpoint not found.')\n",
    "        \n",
    "        print( 'Initializing global variables.' )\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for ee in range( joint_epochs ):\n",
    "            print( 'Beginning epoch {} of {}.'.format( ee+1, joint_epochs ) )\n",
    "            \n",
    "            cnt = 0\n",
    "            train_loss = 0.0\n",
    "            # Run some number of training batches in this epoch\n",
    "            sess.run( training_init_op, feed_dict={batch_size: train_batch_size} )\n",
    "            while True:\n",
    "                try:\n",
    "                    _, loss_, cl_, ocl_ = sess.run( [train_op, loss, class_loss, one_class_loss], feed_dict={is_train: True} )\n",
    "                    cnt += 1\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "                train_loss += loss_\n",
    "            \n",
    "            train_loss = train_loss / cnt\n",
    "            print( \"  The average training loss at epoch {} was {} with {} steps.\".format( ee+1, train_loss, cnt ) )\n",
    "            \n",
    "            cnt = 0\n",
    "            test_loss = 0.0\n",
    "            # Run some number of testing batches in this epoch\n",
    "            sess.run( testing_init_op, feed_dict={batch_size: test_batch_size} )\n",
    "            while True:\n",
    "                try:\n",
    "                    loss_ = sess.run( [loss], feed_dict={is_train: False} )\n",
    "                    cnt += 1\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "                test_loss += loss_[0]\n",
    "            \n",
    "            test_loss = test_loss / cnt\n",
    "            print( \"  The average testing loss at epoch {} was {} with {} steps\".format( ee+1, test_loss, cnt ) )\n",
    "        \n",
    "        save_path = saver.save( sess, ckpt_fname )\n",
    "        \n",
    "    else:\n",
    "        # Just load the model if we already have it\n",
    "        print( \"Loading parameter.\" )\n",
    "        saver.restore( sess, ckpt_fname )\n",
    "    \n",
    "    sess.run( training_init_op , feed_dict={batch_size: train_batch_size})\n",
    "    train_im, train_re, train_rl, train_lb0 = sess.run( \n",
    "        [images, recon, recon_loss, labels], feed_dict={is_train: False} )\n",
    "    \n",
    "    sess.run( testing_init_op, feed_dict={batch_size: test_batch_size} )\n",
    "    test_im, test_re, test_lb0 = sess.run( [images, recon, labels], feed_dict={is_train: False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Do I need to check that the model has been restored or is that guaranteed above?\n",
    "- i.e. do I need to do the next cell in a session that has reloaded the checkpoint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt_128/cifar-010/noClass_noOc/expanded_in01234567_20.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore( sess, ckpt_fname )\n",
    "    \n",
    "    sess.run( training_init_op, feed_dict={batch_size: train_batch_size} )\n",
    "    train_codes = []\n",
    "    train_lbls  = []\n",
    "    while True: \n",
    "        try:\n",
    "            train_codes_, train_lbls_, im_, re_ = sess.run( [code,labels,images,recon], feed_dict={is_train: False} )\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        train_codes.append( np.squeeze( train_codes_ ) )\n",
    "        train_lbls.append(  np.argmax(  train_lbls_, axis=1 ) )\n",
    "    \n",
    "    \n",
    "    sess.run( testing_init_op, feed_dict={batch_size: train_batch_size} )\n",
    "    test_codes = []\n",
    "    test_lbls  = []\n",
    "    while True: \n",
    "        try:\n",
    "            test_codes_, test_lbls_ = sess.run( [code,labels], feed_dict={is_train: False} )\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        test_codes.append( np.squeeze( test_codes_ ) )\n",
    "        test_lbls.append(  np.argmax(  test_lbls_, axis=1 ) )\n",
    "    \n",
    "train_codes = np.concatenate( train_codes, axis=0 )\n",
    "train_lbls  = np.concatenate( train_lbls,  axis=0 )\n",
    "\n",
    "test_codes  = np.concatenate( test_codes, axis=0 )\n",
    "test_lbls   = np.concatenate( test_lbls,  axis=0 )\n",
    "\n",
    "np.save(    './scikit_train_codes.npy', train_codes )\n",
    "np.savetxt( './scikit_train_lbls.csv',  train_lbls.astype( np.int32 ), delimiter=',', fmt='%d' )\n",
    "\n",
    "np.save(    './scikit_test_codes.npy',  test_codes  )\n",
    "np.savetxt( './scikit_test_lbls.csv',   test_lbls.astype(  np.int32 ), delimiter=',', fmt='%d' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt_128/cifar-010/noClass_noOc/expanded_in01234567_20.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_128/cifar-010/noClass_noOc/expanded_in01234567_20.ckpt\n"
     ]
    }
   ],
   "source": [
    "rdr.save_codes( code, labels, training_init_op, batch_size, is_train, ckpt_fname, train_code_tfr_name )\n",
    "rdr.save_codes( code, labels,    valid_init_op, batch_size, is_train, ckpt_fname, valid_code_tfr_name )\n",
    "rdr.save_codes( code, labels,  testing_init_op, batch_size, is_train, ckpt_fname,  test_code_tfr_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recon( num, truth, recon ):\n",
    "    fig, ax = plt.subplots( 2 , num, figsize=(18,3) )\n",
    "    \n",
    "    for nn in range( num ):\n",
    "        ax[0,nn].get_xaxis().set_visible( False )\n",
    "        ax[0,nn].get_yaxis().set_visible( False )\n",
    "        ax[1,nn].get_xaxis().set_visible( False )\n",
    "        ax[1,nn].get_yaxis().set_visible( False )\n",
    "        \n",
    "        ax[0,nn].imshow( truth[nn] )\n",
    "        ax[1,nn].imshow( recon[nn] )\n",
    "        \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_train = plot_recon( 10, train_im, train_re )\n",
    "plt_train[0].savefig( \n",
    "    os.path.join( image_folder, out_file + \"_train_recon.png\" ), \n",
    "    dpi=216\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_test = plot_recon( 10, test_im, test_re )\n",
    "plt_test[0].savefig( \n",
    "    os.path.join( image_folder, out_file + \"_test_recon.png\" ), \n",
    "    dpi=216\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_save = plot_recon( 10, im_, re_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ser_example in tf.python_io.tf_record_iterator( train_code_tfr_name ):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString( ser_example )\n",
    "#     print( example )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_iterator, training_codeset, valid_codeset, testing_codeset = rdr.build_code_iterator( \n",
    "    train_code_tfr_name, \n",
    "    valid_code_tfr_name, \n",
    "    test_code_tfr_name, \n",
    "    code.get_shape().as_list()[-1], \n",
    "    class_num, \n",
    "    batch_size=batch_size, \n",
    "    train_batch_size=train_batch_size, \n",
    "    valid_batch_size=valid_batch_size, \n",
    "    test_batch_size=test_batch_size, \n",
    "    drop_remainder=True # this may not really be desirable...\n",
    ")\n",
    "\n",
    "training_init_code = code_iterator.make_initializer( training_codeset )\n",
    "valid_init_code    = code_iterator.make_initializer( valid_codeset )\n",
    "testing_init_code  = code_iterator.make_initializer( testing_codeset )\n",
    "\n",
    "# training_init_code = training_init_op\n",
    "# valid_init_code    = valid_init_op\n",
    "# testing_init_code  = testing_init_op\n",
    "\n",
    "fcode, flabel = code_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sess.run( training_init_code, feed_dict={batch_size: train_batch_size} )\n",
    "    \n",
    "    fixed_codes = []\n",
    "    fixed_lbls  = []\n",
    "    while True: \n",
    "        try:\n",
    "            fixed_codes_, fixed_lbls_ = sess.run( [fcode,flabel], feed_dict={is_train: False} )\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        fixed_codes.append( np.squeeze( fixed_codes_ ) )\n",
    "        fixed_lbls.append(  np.argmax(  fixed_lbls_, axis=1 ) )\n",
    "        \n",
    "fixed_codes = np.concatenate( fixed_codes, axis=0 )\n",
    "fixed_lbls  = np.concatenate( fixed_lbls,  axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_loss = 0\n",
    "if ( not is_joint_class ):\n",
    "#     code_loss += class_loss / 100\n",
    "    \n",
    "    print( \"Jointly training a classifier on the fixed codes.\" )\n",
    "\n",
    "if ( not is_joint_ocnn ):\n",
    "    code_loss += one_class_loss # + V_norm + tf.norm( oc_norm, ord=2 ) / 2\n",
    "    \n",
    "    print( \"Imposing a one-class SVM-like loss on the fixed codes.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_recon = ae.decoder( fixed_code, name='fixed' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run( training_init_code, feed_dict={batch_size: train_batch_size} )\n",
    "    \n",
    "#     saver.restore( sess, ckpt_fname )\n",
    "#     fixed_recon_, fixed_code_ = sess.run( [fixed_recon, fixed_code], feed_dict={is_train: False} )\n",
    "\n",
    "# # Check that we have saved and loaded the codes correctly\n",
    "# plot_recon( 10, fixed_recon_, fixed_recon_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_code = tf.train.exponential_decay( \n",
    "    learning_rate=0.00005, \n",
    "    global_step=global_step,\n",
    "    decay_steps=int( ( 50000 / ( 2 * train_batch_size ) ) ), \n",
    "    decay_rate=0.99, \n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# train_op = tf.train.MomentumOptimizer( learning_rate=learning_rate, momentum=0.9 ).minimize( loss )\n",
    "# train_op = tf.train.AdamOptimizer( learning_rate=learning_rate ).minimize( loss )\n",
    "train_code_op = tf.train.RMSPropOptimizer( learning_rate_code ).minimize( code_loss )\n",
    "\n",
    "saver_code = tf.train.Saver()\n",
    "\n",
    "os.makedirs( os.path.join( ckpt_folder, 'remainder' ), exist_ok=True )\n",
    "ckpt_code_fname = os.path.join( ckpt_folder, 'remainder', out_file+\".ckpt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore( sess, ckpt_fname )\n",
    "    \n",
    "    if not tf.train.checkpoint_exists( ckpt_code_fname ):\n",
    "        # Train the model if it doesn't already exist\n",
    "        print( 'Checkpoint not found.')\n",
    "        \n",
    "        print( 'Initializing global variables.' )\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for ee in range( code_epochs ):\n",
    "            print( 'Beginning epoch {} of {}.'.format( ee+1, code_epochs ) )\n",
    "            \n",
    "            cnt = 0\n",
    "            train_loss = 0.0\n",
    "            # Run some number of training batches in this epoch\n",
    "            sess.run( training_init_op, feed_dict={batch_size: train_batch_size} )\n",
    "            while True:\n",
    "                try:\n",
    "                    _, loss_, images_, recon_ = sess.run( [train_code_op, code_loss, images, recon], feed_dict={is_train: True} )\n",
    "                    cnt += 1\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "                train_loss += loss_\n",
    "            \n",
    "            train_loss = train_loss / cnt\n",
    "            print( \"  The average training loss at epoch {} was {} with {} steps.\".format( ee+1, train_loss, cnt ) )\n",
    "            \n",
    "            cnt = 0\n",
    "            test_loss = 0.0\n",
    "            # Run some number of testing batches in this epoch\n",
    "            sess.run( testing_init_op, feed_dict={batch_size: test_batch_size} )\n",
    "            while True:\n",
    "                try:\n",
    "                    loss_ = sess.run( [code_loss], feed_dict={is_train: False} )\n",
    "                    cnt += 1\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "                test_loss += loss_[0]\n",
    "            \n",
    "            test_loss = test_loss / cnt\n",
    "            print( \"  The average testing loss at epoch {} was {} with {} steps\".format( ee+1, test_loss, cnt ) )\n",
    "        \n",
    "        save_path = saver_code.save( sess, ckpt_code_fname )\n",
    "        \n",
    "    else:\n",
    "        # Just load the model if we already have it\n",
    "        print( \"Loading parameter.\" )\n",
    "        saver_code.restore( sess, ckpt_code_fname )\n",
    "    \n",
    "    sess.run( training_init_code , feed_dict={batch_size: train_batch_size})\n",
    "#     train_im, train_re, train_rl, train_lb0 = sess.run( \n",
    "#         [images, recon, recon_loss, labels], feed_dict={is_train: False} )\n",
    "    \n",
    "    sess.run( testing_init_code, feed_dict={batch_size: test_batch_size} )\n",
    "#     test_im, test_re, test_lb0 = sess.run( [images, recon, labels], feed_dict={is_train: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_recon( 10, images_, recon_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( not is_joint_ocnn ):\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore( sess, ckpt_fname )\n",
    "        saver_code.restore( sess, ckpt_code_fname )\n",
    "        \n",
    "        sess.run( training_init_code , feed_dict={batch_size: train_batch_size})\n",
    "        \n",
    "        r_all = []\n",
    "        oc_dist_train = np.zeros( (0,1) )\n",
    "        labels_train = np.zeros( (0,class_num) )\n",
    "        while True:\n",
    "            try:\n",
    "                oc_dist_, lbls_, r_ = sess.run( [one_class_dist, labels, r], feed_dict={is_train: False} )\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            \n",
    "            r_all.append( r_ )\n",
    "            oc_dist_train = np.concatenate( ( oc_dist_train, oc_dist_ ), axis=0 )\n",
    "            labels_train  = np.concatenate( ( labels_train,  lbls_ ), axis=0 )\n",
    "            \n",
    "        sess.run( testing_init_code , feed_dict={batch_size: train_batch_size})\n",
    "        \n",
    "        oc_dist_test = np.zeros( (0,1) )\n",
    "        labels_test  = np.zeros( (0,class_num) )\n",
    "        while True:\n",
    "            try:\n",
    "                oc_dist_, lbls_ = sess.run( [one_class_dist, labels], feed_dict={is_train: False} )\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            \n",
    "            oc_dist_test = np.concatenate( ( oc_dist_test, oc_dist_ ), axis=0 )\n",
    "            labels_test  = np.concatenate( ( labels_test,  lbls_ ), axis=0 )\n",
    "            \n",
    "oc_dist_train = np.squeeze( oc_dist_train )\n",
    "oc_dist_test  = np.squeeze( oc_dist_test )\n",
    "\n",
    "labels_train_num = np.argmax( labels_train, axis=1 )\n",
    "labels_test_num  = np.argmax( labels_test,  axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( 1 , 2, figsize=(12,4) )\n",
    "_ = ax[0].hist( oc_dist_train, bins=100 )\n",
    "_ = ax[0].set_title( 'Train ({} In)'.format( len( oc_dist_train ) ) )\n",
    "\n",
    "in_test  = oc_dist_test[ labels_test_num != class_num-1 ]\n",
    "out_test = oc_dist_test[ labels_test_num == class_num-1 ]\n",
    "_ = ax[1].hist(  in_test, bins=100 )\n",
    "_ = ax[1].hist( out_test, bins=100 )\n",
    "_ = ax[1].set_title( 'Test ({} In/{} Out)'.format( len( in_test ), len( out_test ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figc, axc = plt.subplots( class_num , 2, figsize=(12,4*class_num) )\n",
    "\n",
    "# axc[cc,0].set_title( 'Train ({} In)'.format( len( oc_dist_train ) ) )\n",
    "for cc in range( class_num-1 ):\n",
    "    axc[cc,0].hist( oc_dist_train[np.argmax( labels_train, axis=1 ) == cc], bins=100 )\n",
    "    axc[cc,1].hist( oc_dist_test[ np.argmax( labels_test,  axis=1 ) == cc], bins=100 )\n",
    "    \n",
    "    axc[cc,0].set_xlim( [-1.5,1.5] )\n",
    "    axc[cc,1].set_xlim( [-1.5,1.5] )\n",
    "\n",
    "out_test = oc_dist_test[ labels_test_num == class_num-1 ]\n",
    "axc[-1,1].hist( out_test, bins=100 )\n",
    "_ = axc[-1,1].set_xlim( [-1.5,1.5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
