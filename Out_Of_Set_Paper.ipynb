{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All neccesary classes for project\n",
    "\n",
    "#general\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#for preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "\n",
    "#for machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "#for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "from Utilities.utilities import display_scores\n",
    "from Utilities.utilities import pipeline_transform\n",
    "from Utilities.utilities import reset_graph\n",
    "from Utilities.models import DNN_Model\n",
    "from Utilities.models import cross_val_score_dnn\n",
    "from functools import partial\n",
    "\n",
    "#image manipulation\n",
    "from PIL import Image as PI\n",
    "from resizeimage import resizeimage\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.data_utils import get_file\n",
    "import imagenet_helper_files.vgg_preprocessing\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "import imagenet_helper_files.pnasnet as nas\n",
    "\n",
    "#Import Custom Functions\n",
    "from Utilities.model_builder import get_image\n",
    "from Utilities.model_builder import get_file_lists\n",
    "from Utilities.model_builder import parse_record\n",
    "from Utilities.model_builder import get_batch\n",
    "from Utilities.model_builder import build_iterator\n",
    "from Utilities.model_builder import build_dataset\n",
    "from Utilities.bounded_model_builder import build_bounded_iterator\n",
    "from Utilities.bounded_model_builder import build_bounded_iterator_points\n",
    "from Utilities.model_builder import get_values_imagenet\n",
    "from Utilities.model_builder import get_values_bounded\n",
    "from Utilities.model_builder import get_values_bounded_points\n",
    "from Utilities.models import log_dir_build\n",
    "from Utilities.utilities import generate_image\n",
    "from Utilities.utilities import generate_image_array\n",
    "from Utilities.cell_net_predictor import Binary_Categorical_Predictor\n",
    "from Utilities.build_image_data_notebook import process_dataset\n",
    "\n",
    "from Utilities.utilities import get_ground_truth_string\n",
    "from Utilities.utilities import find_new_imagenet_ground_truth_int\n",
    "from Utilities.cell_net_predictor import Binary_Categorical_Predictor\n",
    "from Utilities.build_image_data_notebook import process_dataset\n",
    "from Utilities.sample_counter import get_likelihood_avg\n",
    "from Utilities.sample_counter import get_sample_number\n",
    "from Utilities.sample_counter import get_likelihood_stdev\n",
    "\n",
    "from tan.tan_util import get_tan_nll as tan\n",
    "from tan.tan_util import get_tan_nll_cond as tan_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Load the data, classes, and check that the tf_records are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location for all the training Data\n",
    "#Set Variables\n",
    "\n",
    "output_directory = \"D:/Machine_Learning/Datasets/Cifar_80/tf_records\"\n",
    "labels_file = \"D:/Machine_Learning/Datasets/Cifar_80/labels.txt\"\n",
    "\n",
    "output_directory_all = \"D:/Machine_Learning/Datasets/Cifar_100/tf_records\"\n",
    "labels_file_all = \"D:/Machine_Learning/Datasets/Cifar_100/labels.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import TFRecords for both Training and Testing of the Dat\n",
    "#Use the build_image_data.py to create these sets from your data\n",
    "class_file = open(labels_file,'r')\n",
    "class_list = class_file.read().split('\\n')\n",
    "\n",
    "train_list, val_list = get_file_lists(output_directory)\n",
    "labels = class_list\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Validation data for all classs\n",
    "\n",
    "class_file_all = open(labels_file_all,'r')\n",
    "class_list_all = class_file_all.read().split('\\n')\n",
    "train_list_all, val_list_all = get_file_lists(output_directory_all)\n",
    "labels_all = class_list_all\n",
    "labels_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the training Data is good\n",
    "\n",
    "Here we will import a tf reccord and ensure the data is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a workflow to extract the data \n",
    "reset_graph()\n",
    "\n",
    "filename = tf.placeholder(tf.string, shape=[None], name=\"tf_records\")\n",
    "batch_size = tf.placeholder(tf.int64, shape=[], name= \"Batch_Size\")\n",
    "num_epochs = tf.placeholder(tf.int64, shape=[], name= \"Num_epochs\")\n",
    "training = tf.placeholder_with_default(True, shape=(), name = 'training')\n",
    "handle = tf.placeholder(tf.string, shape=[], name=\"Dataset\")\n",
    "\n",
    "training_set = build_dataset(True, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "val_set = build_dataset(False, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "\n",
    "train_iterator = training_set.make_initializable_iterator()\n",
    "val_iterator = val_set.make_initializable_iterator()\n",
    "\n",
    "\n",
    "\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, training_set.output_types, training_set.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "X_data, y_data, file = next_element\n",
    "\n",
    "X = tf.placeholder_with_default(X_data, [None,331,331,3])\n",
    "y = tf.placeholder_with_default(y_data, [None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test pulling a piece of data out of the set to ensure that records were created properly\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list_all, batch_size: 1, num_epochs:1})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    \n",
    "    \n",
    "    X_test, y_test, file_test = sess.run([X,y,file], feed_dict={handle: train_handle,training: False})\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show and Image from set\n",
    "x_val = X_test.reshape(331,331,3)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Class\n",
    "labels_all[y_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the file\n",
    "file_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "\n",
    "Here we will build the Nas-Net and then stack our own network on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import pnas_net model\n",
    "\n",
    "#Nasnet Model Location\n",
    "nas_net_model = 'D:/AI/models/pnas_net/model.ckpt'\n",
    "\n",
    "#directory for logs in training\n",
    "out_of_set_net_logs = 'D:/AI/models/out_of_set_net_final/logs'\n",
    "model_path = log_dir_build(out_of_set_net_logs, \"out_of_set_final\")\n",
    "\n",
    "#directory for all the models saved during training\n",
    "out_of_set_net_model_class = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_class'\n",
    "out_of_set_net_model_tan = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_tan'\n",
    "out_of_set_net_model_ae = 'D:/AI/models/out_of_set_net_v2/final_model/' + 'out_of_set_net_ae'\n",
    "out_of_set_net_best = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_best'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.\n",
      "Building tan Graph,\n",
      "\tconditioning Tensor(\"Out_Of_Set_Classifier/Tan/Sort_to_correct_Tan/cond/Merge:0\", shape=(?, 80), dtype=float32)\n",
      "Using conditional transformation...\n",
      "Using conditional transformation...\n",
      "[<function conditioning_transformation.<locals>.invmap at 0x0000028FF1D85C80>, <function log_rescale.<locals>.invmap at 0x0000028FF1D857B8>, <function log_rescale.<locals>.invmap at 0x0000028FF1D85598>, <function log_rescale.<locals>.invmap at 0x0000028FF1D85378>, <function log_rescale.<locals>.invmap at 0x0000028FF1D85158>, <function log_rescale.<locals>.invmap at 0x0000028FF1CEDEA0>, <function log_rescale.<locals>.invmap at 0x0000028FF1CEDC80>, <function log_rescale.<locals>.invmap at 0x0000028FF1CEDA60>, <function log_rescale.<locals>.invmap at 0x0000028FF1CED620>, <function conditioning_transformation.<locals>.invmap at 0x0000028FF1CED400>, <function get_LU_map.<locals>.invmap at 0x0000028FF1CED268>]\n"
     ]
    }
   ],
   "source": [
    "#the graph \n",
    "reset_graph()\n",
    "\n",
    "number_of_classes = 80\n",
    "\n",
    "#Set up data Pipeline \n",
    "with tf.name_scope(\"Data_Retriever\"):\n",
    "    filename = tf.placeholder(tf.string, shape=[None], name=\"tf_records\")\n",
    "    batch_size = tf.placeholder(tf.int64, shape=[], name= \"Batch_Size\")\n",
    "    num_epochs = tf.placeholder(tf.int64, shape=[], name= \"Num_epochs\")\n",
    "    training = tf.placeholder_with_default(True, shape=(), name = 'training')\n",
    "    handle = tf.placeholder(tf.string, shape=[], name=\"Dataset\")\n",
    "\n",
    "    training_set = build_dataset(True, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "    val_set = build_dataset(False, filename, batch_size, num_epochs, num_parallel_calls=8)\n",
    "\n",
    "    train_iterator = training_set.make_initializable_iterator()\n",
    "    val_iterator = val_set.make_initializable_iterator()\n",
    "\n",
    "\n",
    "\n",
    "    iterator = tf.data.Iterator.from_string_handle(\n",
    "        handle, training_set.output_types, training_set.output_shapes)\n",
    "    next_data = iterator.get_next()\n",
    "    X_data, y_data, file = next_data\n",
    "    X = tf.placeholder_with_default(X_data, shape=[None,331,331,3], name=\"Image_Data\")\n",
    "    y = tf.placeholder_with_default(y_data, shape=[None], name=\"Max_Class_Int\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"BN_Layer_AE_Layers\"):\n",
    "    #Define initalizer and batch normalization layers\n",
    "    bn_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "#Import the Nas_Net and build it\n",
    "with slim.arg_scope(nas.pnasnet_large_arg_scope()):\n",
    "    net, end_points = nas.build_pnasnet_large(X, num_classes=1001, is_training=False)\n",
    "    \n",
    "    #A saver to load the pretrained Data\n",
    "    saver = tf.train.Saver(name=\"Original_Saver\")\n",
    "    \n",
    "    #For getting predictions from Original Network\n",
    "    pnas_net_predictions = tf.get_default_graph().get_tensor_by_name(\"final_layer/predictions:0\")\n",
    "    \n",
    "    #get indicies for doing reduction\n",
    "    indices = tf.get_default_graph().get_tensor_by_name(\"final_layer/Mean/reduction_indices:0\")\n",
    "    \n",
    "    #Load in the noder where we are going to connect our own network\n",
    "    last_feature_node = tf.get_default_graph().get_tensor_by_name(\"cell_11/cell_output/concat:0\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Out_Of_Set_Classifier\"):\n",
    "    #Use a stop layer to freeze all the layers beneath in Nas-Net\n",
    " \n",
    "    with tf.name_scope(\"Isolate_Image_Features\"):\n",
    "        #get the output of the last cell layer\n",
    "\n",
    "        starting_relu = tf.nn.relu(last_feature_node, name=\"first_relu\")\n",
    "        mean_pool = tf.reduce_mean(starting_relu, indices, name=\"condensing_mean\")\n",
    "        cnn_code_layer = tf.stop_gradient(mean_pool)\n",
    "        \n",
    "    with tf.name_scope(\"Classifier\"):\n",
    "       \n",
    "        #Set constants for Classifier\n",
    "        dropout_rate = 0.3\n",
    "        n_hidden1 = 1000\n",
    "        n_hidden2 = 500\n",
    "        n_final_layer = number_of_classes\n",
    "        learning_rate_class = 1\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"Class_Hidden_Layer_1\"):\n",
    "            hidden1_cat = tf.layers.dense(cnn_code_layer, n_hidden1, name=\"hidden1_cat\", kernel_initializer=he_init)\n",
    "            hidden1_drop = tf.layers.dropout(hidden1_cat, dropout_rate, training=training)\n",
    "            bn1_cat = bn_batch_norm_layer(hidden1_drop)\n",
    "            bn1_act_cat = tf.nn.relu(bn1_cat)\n",
    "\n",
    "        with tf.name_scope(\"Class_Hidden_Layer_3\"):\n",
    "            hidden3_cat = tf.layers.dense(bn1_act_cat, n_hidden2, name=\"hidden2_cat\", kernel_initializer=he_init)\n",
    "            hidden3_drop = tf.layers.dropout(hidden3_cat, dropout_rate, training=training)\n",
    "            bn3_cat = bn_batch_norm_layer(hidden3_drop)\n",
    "            bn3_act_cat = tf.nn.relu(bn3_cat) \n",
    "  \n",
    "        \n",
    "        with tf.name_scope(\"Final_Layer\"): \n",
    "            logits_before_bn = tf.layers.dense(bn3_act_cat, n_final_layer, name=\"outputs\")\n",
    "            logits = bn_batch_norm_layer(logits_before_bn, name=\"logits\")\n",
    "            softmax = tf.nn.softmax(logits, name=\"final_soft_max\")\n",
    "            max_softmax_val = tf.argmax(softmax,axis=1,name=\"softmax_Category_int\",output_type=tf.int32)\n",
    "            stop_max = tf.stop_gradient(max_softmax_val, name=\"Stop_Max\")\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "                xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "                loss_cat = tf.reduce_mean(xentropy, name=\"loss_cat\")\n",
    "                loss_summary_cat = tf.summary.scalar('loss_summary_cat', loss_cat)\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_class)\n",
    "\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            with tf.control_dependencies(extra_update_ops):\n",
    "                training_op = optimizer.minimize(loss_cat, global_step=global_step)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            accuracy_summary = tf.summary.scalar('accuracy_summary', accuracy)\n",
    "    \n",
    "    saver_class = tf.train.Saver(name=\"Class_Saver\")\n",
    "    \n",
    "            \n",
    "    with tf.name_scope(\"Autoencoder\"):\n",
    "        n_hidden4 = 108\n",
    "        ae_learning_rate = 1\n",
    "        with tf.name_scope(\"Original_Coding\"):\n",
    "            ae_code_layer = cnn_code_layer\n",
    "            batch_mean = tf.expand_dims(tf.reduce_mean(cnn_code_layer,0),0)\n",
    "            ae_code_normalized = tf.subtract(ae_code_layer, batch_mean)\n",
    "            \n",
    "        with tf.name_scope(\"PCA_Layer\"):\n",
    "            reduction_matrix = tf.Variable(tf.random_normal([cnn_code_layer_size, n_hidden4],stddev=.1),name=\"weights\")           \n",
    "            AutoCode = tf.matmul(ae_code_normalized,reduction_matrix)\n",
    "            AutoOutput = tf.stop_gradient(AutoCode, name=\"Autoencoder_Output\")\n",
    "            \n",
    "        \n",
    "        with tf.name_scope(\"Reconstruction_Layer_Final\"):\n",
    "            final_layer = tf.layers.dense(AutoCode, cnn_code_layer_size, name=\"Reconstruction_Layer\", \n",
    "                                                       kernel_initializer=he_init)\n",
    "            final_reconstruction_layer_bn = tf.matmul(AutoCode, tf.transpose(reduction_matrix))\n",
    "            final_reconstruction_layer = tf.add(final_reconstruction_layer_bn,batch_mean)\n",
    "            \n",
    "        with tf.name_scope(\"AutoEncoder_Loss\"):\n",
    "            ae_loss = tf.losses.mean_squared_error(final_reconstruction_layer,ae_code_layer)\n",
    "            ae_loss_summary = tf.summary.scalar('ae_loss_summary', ae_loss)\n",
    "            \n",
    "           \n",
    "        with tf.name_scope(\"AutoEncoder_Train\"):\n",
    "            \n",
    "            ae_optimizer = tf.train.AdamOptimizer(learning_rate=ae_learning_rate)\n",
    "            ae_global_step = tf.Variable(0, trainable=False, name='ae_global_step')\n",
    "            training_op_ae = ae_optimizer.minimize(ae_loss, global_step=ae_global_step)\n",
    " \n",
    "    saver_ae = tf.train.Saver(name=\"AutoEncoder_Saver\")\n",
    "            \n",
    "    with tf.name_scope(\"Tan\"):\n",
    "        likely_batch_size = 60\n",
    "        \n",
    "        with tf.name_scope(\"Sort_to_correct_Tan\"):\n",
    "            depth = number_of_classes\n",
    "            y_true = tf.one_hot(y, depth)\n",
    "            y_guess = tf.one_hot(stop_max, depth)\n",
    "            \n",
    "            conditional = tf.cond(training, lambda: y_true, lambda: y_guess)\n",
    "            \n",
    "            tan_input = AutoOutput\n",
    "        with tf.name_scope(\"Tan_Network\"):\n",
    "            tan_loss, tan_likelihoods, samp = tan_cond(tan_input, conditional)\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"Tan_Trainer\"):\n",
    "            likelihood_loss_summary = tf.summary.scalar('likelihood_loss_summary_class', tan_loss)\n",
    "            \n",
    "            global_step_tan = tf.Variable(0, trainable=False, name=\"global_step_tan\")\n",
    "            \n",
    "            learning_rate = tf.train.exponential_decay( \n",
    "                learning_rate=0.001, \n",
    "                global_step=global_step_tan,\n",
    "                decay_steps=int( ( 50000 / ( 2 * likely_batch_size ) ) ), \n",
    "                decay_rate=0.99, \n",
    "                staircase=True\n",
    "            )\n",
    "            tan_train_op = tf.train.RMSPropOptimizer( learning_rate ).minimize( tan_loss, global_step=global_step_tan )\n",
    "            \n",
    "    with tf.name_scope(\"Tan_Threshold\"):\n",
    "        \n",
    "        with tf.name_scope(\"Likelihood_Average\"):\n",
    "            likelihood_list = tan_likelihoods\n",
    "            class_list = conditional\n",
    "            \n",
    "            \n",
    "            #likelihood_averages = tf.Variable([0,0],name=\"tan_distribution_lists\", trainable=False, dtype=tf.float32)\n",
    "            likelihood_averages = tf.get_variable('tan_distribution_lists', shape=(number_of_classes), trainable=False, dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            likelihood_stdev = tf.get_variable('tan_distribution_stdev', shape=(number_of_classes), trainable=False, dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            number_of_batches = tf.get_variable('sample_count', shape=(number_of_classes), trainable=False, dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "               \n",
    "            update_avg_like = tf.assign(likelihood_averages,get_likelihood_avg(y, number_of_classes, likelihood_list,likelihood_averages, number_of_batches))\n",
    "            update_stdev_like = tf.assign(likelihood_stdev, get_likelihood_stdev(y, number_of_classes, likelihood_list,likelihood_stdev, number_of_batches))\n",
    "            with tf.get_default_graph().control_dependencies([update_avg_like, update_stdev_like]):\n",
    "                update_batch_size = tf.assign(number_of_batches, get_sample_number(y, number_of_classes, likelihood_list, number_of_batches) )\n",
    "            \n",
    "            update_average_likelihood = tf.group(update_avg_like, update_stdev_like, update_batch_size)\n",
    "            \n",
    "        with tf.name_scope(\"Likelihood_Threshold\"):\n",
    "            bottom_quartile = likelihood_averages - .25 * likelihood_stdev\n",
    "            \n",
    "            \n",
    "#Variables for global initialization\n",
    "saver_tan = tf.train.Saver(name=\"Final_Saver\")\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Graph to log directory\n",
    "filewriter = tf.summary.FileWriter(model_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize PNAS Net\n",
    "\n",
    "Restore the Parameters from the pre-trained PNAS Net and initialize all the savers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/pnas_net/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Initialize all variables and restore the lower layer\n",
    "with tf.Session() as sess:\n",
    "    #Initalizer all variables\n",
    "    init.run()\n",
    "    \n",
    "    #Restore the pretrained variables from Nas-Net\n",
    "    saver.restore(sess, nas_net_model)\n",
    "    \n",
    "    \n",
    "    #Save all of these variables to the new Cell_Net Model\n",
    "    saver_class.save(sess, out_of_set_net_model_class)\n",
    "    saver_ae.save(sess, out_of_set_net_model_ae)\n",
    "    saver_tan.save(sess, out_of_set_net_model_tan)\n",
    "    saver_tan.save(sess, out_of_set_net_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "Here we are going to train the network. Accuracy/Loss is recorded\n",
    "Note for this version, a certain amount of the data is seen every training step. \n",
    "set train_size for how many images are trained on in each epoch\n",
    "set batch_size for how many images are trained at once.\n",
    "num_epochs is how many times the network sees that ammount of training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure settings for Session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the classifier\n",
    "\n",
    "Here we are going to train it to recognize the two classes of images that we have for that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_final/model/out_of_set_net_class\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/out_of_set_net_final/logs/out_of_set_final-run-20190110220740/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-1292e3c63e18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m#run Training Op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_handle\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#see if we are improving on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "steps_between_test_save = 1\n",
    "batch = 30\n",
    "train_size = 40000\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "lowest_loss = 10000\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_class.restore(sess, out_of_set_net_model_class)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 1\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([training_op], feed_dict={handle: train_handle})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0) :\n",
    "            loss_sum, loss_val, acc_sum = sess.run([loss_summary_cat, loss_cat, accuracy_summary], \n",
    "                                                   feed_dict = {handle: val_handle ,training: False})\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            saver_class.save(sess, out_of_set_net_model_class)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_class.save(sess, out_of_set_net_model_class)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AutoEncoder\n",
    "\n",
    "With our Classifier trained we can now train the AE with the Overlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "epochs = 60\n",
    "steps_between_test_save = 1\n",
    "batch = 30\n",
    "train_size = 40000\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "lowest_loss = 10000\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    init.run()\n",
    "    saver_class.restore(sess, out_of_set_net_model_class)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([training_op_ae], feed_dict={handle: train_handle})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0 and step != 0) :\n",
    "            loss_sum, loss_val = sess.run([ae_loss_summary, ae_loss], feed_dict = {handle: val_handle, training: False})\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            saver_ae.save(sess, out_of_set_net_model_ae)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_ae.save(sess, out_of_set_net_model_ae)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, ae_global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the TAN\n",
    "\n",
    "Next we train the TAN to get the proper likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_final/model/out_of_set_net_class\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/out_of_set_net_final/logs/out_of_set_final-run-20190110141730/\n",
      "Epoch: 0 Loss: 160.542\n",
      "Epoch: 1 Loss: 27.1398\n",
      "Epoch: 2 Loss: 19.4816\n",
      "Epoch: 3 Loss: 20.1872\n",
      "Epoch: 4 Loss: 16.3476\n",
      "Epoch: 5 Loss: 20.6347\n",
      "Did 3330 of loss minimized training in 4506.076171875 seconds.\n"
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "epochs = 6\n",
    "steps_between_test_save = 1\n",
    "batch = 60\n",
    "train_size = 40000\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "lowest_like = 10000\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    init.run()\n",
    "    saver_class.restore(sess, out_of_set_net_model_class)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    like_sum, like_val = sess.run([likelihood_loss_summary, tan_loss], feed_dict = {handle: val_handle, training: False})\n",
    "    filewriter.add_summary(like_sum, step)\n",
    "    print(\"Epoch: \" + str(step) + \" Loss: \" + str(like_val))\n",
    "    \n",
    "    step = step + 1\n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([tan_train_op], feed_dict={handle: train_handle})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0 and step != 0) :\n",
    "            like_sum, like_val = sess.run([likelihood_loss_summary, tan_loss], feed_dict = {handle: val_handle, training: False})\n",
    "            filewriter.add_summary(like_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(like_val))\n",
    "            saver_tan.save(sess, out_of_set_net_model_tan)\n",
    "            if lowest_like > like_val:\n",
    "                lowest_like = like_val\n",
    "                saver_tan.save(sess, out_of_set_net_best)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_tan.save(sess, out_of_set_net_model_tan)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step_tan)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Threshold\n",
    "\n",
    "Here we are going to learn the correct threshold for all classes trained by the TAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_final/model/out_of_set_net_best\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/out_of_set_net_final/logs/out_of_set_final-run-20190110141730/\n",
      "Done training averages in: 908.9003069400787\n",
      "Final Averages: [array([ 31.81084824,   9.81499481,  11.51175213,   2.74386501,\n",
      "         4.02430487, -17.06687164, -21.07948303, -22.61405754,\n",
      "        -9.91632748, -25.84106445, -11.27870846,   3.46301913,\n",
      "       -29.06341553, -29.09570503,   2.85691929, -36.36833954,\n",
      "       -16.77614784, -29.36024284, -19.91014099, -18.42787361,\n",
      "       -17.84228134, -17.21875191, -19.73221207, -11.82461262,\n",
      "       -22.67062187, -32.25492096, -38.15401459, -26.04125404,\n",
      "       -25.18882751, -15.63445187, -24.83739853, -24.50759697,\n",
      "       -21.60658264, -28.74054718, -33.57144165, -26.42733383,\n",
      "       -28.88410568, -28.8621254 , -20.81940842, -27.23272514,\n",
      "       -22.75150681, -24.89483643, -30.61316109, -20.02762222,\n",
      "       -18.71575165, -20.98169136, -18.97211838, -21.74357033,\n",
      "       -19.90183449,  -9.29535866, -32.52500534, -24.38040733,\n",
      "       -24.99948311, -13.17357063, -23.9590435 , -10.99210548,\n",
      "       -26.34570312, -27.21969032, -24.62911606, -23.82633018,\n",
      "       -21.98352242, -27.51404572, -29.80410576, -31.88154984,\n",
      "       -26.94120407, -33.07702255, -22.08308029, -29.29294205,\n",
      "       -21.21476936, -26.06631279, -26.38922691, -23.15083313,\n",
      "       -33.08617401, -21.30015755, -28.39855194, -13.15670872,\n",
      "       -30.44900322, -10.73868465, -38.55459976, -21.48721123], dtype=float32)]\n",
      "Final Std: [array([ 8.78358459,  8.86146641,  8.66390228,  8.44999409,  8.63873291,\n",
      "        5.28398228,  5.58110428,  4.27393103,  4.97536993,  3.57754135,\n",
      "        6.07208729,  9.63190746,  4.017735  ,  4.38144875,  8.37746906,\n",
      "        5.07650566,  5.29887724,  4.20722914,  4.83740711,  6.40874434,\n",
      "        6.24869919,  4.929111  ,  4.72611046,  6.10953712,  4.85769653,\n",
      "        5.49901724,  5.80581379,  4.3880682 ,  4.88688278,  4.62598324,\n",
      "        4.98695278,  4.15960503,  3.26771951,  4.36036968,  4.69223356,\n",
      "        5.20481348,  4.14914942,  4.59407997,  4.42878532,  4.39921904,\n",
      "        4.11530542,  4.48519611,  4.72849798,  3.92284369,  4.38115311,\n",
      "        4.49528265,  4.76325274,  5.76782656,  4.09502888,  6.52863216,\n",
      "        4.71495485,  4.90292168,  4.57740784,  5.93913031,  4.81108236,\n",
      "        6.18086004,  4.59777498,  4.22835398,  5.3086319 ,  5.91752577,\n",
      "        4.9890995 ,  4.34674454,  5.16431093,  5.02942944,  4.80014658,\n",
      "        4.04792166,  4.41305637,  4.81929779,  5.27565145,  4.36299086,\n",
      "        4.84018803,  4.41969633,  5.19976282,  5.3947053 ,  4.15704823,\n",
      "        5.65527296,  3.96638322,  5.34575939,  5.13513756,  6.05151367], dtype=float32)]\n",
      "Samples Counted: [array([ 499.,  500.,  500.,  500.,  499.,  500.,  500.,  500.,  500.,\n",
      "        497.,  500.,  500.,  500.,  499.,  499.,  499.,  500.,  500.,\n",
      "        499.,  500.,  500.,  499.,  500.,  500.,  499.,  499.,  498.,\n",
      "        499.,  500.,  500.,  500.,  500.,  500.,  500.,  500.,  500.,\n",
      "        500.,  499.,  499.,  500.,  499.,  499.,  499.,  500.,  500.,\n",
      "        500.,  500.,  500.,  500.,  498.,  500.,  500.,  499.,  500.,\n",
      "        500.,  499.,  499.,  500.,  499.,  500.,  500.,  499.,  499.,\n",
      "        500.,  499.,  500.,  500.,  499.,  499.,  499.,  499.,  500.,\n",
      "        498.,  499.,  500.,  500.,  500.,  499.,  498.,  499.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "epochs = 1\n",
    "steps_between_test_save = 1\n",
    "batch = 60\n",
    "train_size = 40000\n",
    "all_data_steps = np.int(np.floor(train_size/batch))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_tan.restore(sess, out_of_set_net_best)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([update_average_likelihood], feed_dict={handle: train_handle})\n",
    "        step = step + 1\n",
    "    average = sess.run([likelihood_averages],feed_dict={training:False})\n",
    "    std = sess.run([likelihood_stdev], feed_dict={training:False})\n",
    "    batch_count = sess.run([number_of_batches], feed_dict={training:False})    \n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_tan.save(sess, out_of_set_net_best)\n",
    "    end_time = time.time()\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Done training averages in: \" + str(final_time))\n",
    "    print(\"Final Averages: \" + str(average))\n",
    "    print('Final Std: ' + str(std))\n",
    "    print('Samples Counted: ' + str(batch_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do metric testing\n",
    "\n",
    "Here we will run through all of the training data and relate accuracy with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/out_of_set_net_final/model/out_of_set_net_best\n",
      "Finished Epoch: 0\n",
      "Finished Epoch: 200\n",
      "Finished Epoch: 400\n",
      "Finished Epoch: 600\n",
      "Finished Epoch: 800\n",
      "Finished Epoch: 1000\n",
      "Finished Epoch: 1200\n",
      "Finished Epoch: 1400\n",
      "Finished Epoch: 1600\n",
      "Finished Epoch: 1800\n",
      "Finished Epoch: 2000\n",
      "Finished Epoch: 2200\n",
      "Finished Epoch: 2400\n",
      "Finished Epoch: 2600\n",
      "Finished Epoch: 2800\n",
      "Finished Epoch: 3000\n",
      "Finished Epoch: 3200\n",
      "Finished Epoch: 3400\n",
      "Finished Epoch: 3600\n",
      "Finished Epoch: 3800\n",
      "Finished Epoch: 4000\n",
      "Finished Epoch: 4200\n",
      "Finished Epoch: 4400\n",
      "Finished Epoch: 4600\n",
      "Finished Epoch: 4800\n",
      "Finished Epoch: 5000\n",
      "Finished Epoch: 5200\n",
      "Finished Epoch: 5400\n",
      "Finished Epoch: 5600\n",
      "Finished Epoch: 5800\n",
      "Finished Epoch: 6000\n",
      "Finished Epoch: 6200\n",
      "Finished Epoch: 6400\n",
      "Finished Epoch: 6600\n",
      "Finished Epoch: 6800\n",
      "Finished Epoch: 7000\n",
      "Finished Epoch: 7200\n",
      "Finished Epoch: 7400\n",
      "Finished Epoch: 7600\n",
      "Finished Epoch: 7800\n",
      "Finished Epoch: 8000\n",
      "Finished Epoch: 8200\n",
      "Finished Epoch: 8400\n",
      "Finished Epoch: 8600\n",
      "Finished Epoch: 8800\n",
      "Finished Epoch: 9000\n",
      "Finished Epoch: 9200\n",
      "Finished Epoch: 9400\n",
      "Finished Epoch: 9600\n",
      "Finished Epoch: 9800\n"
     ]
    }
   ],
   "source": [
    "out_of_set_net_model = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_best'\n",
    "\n",
    "batch = 1\n",
    "epochs = 10000\n",
    "\n",
    "#Set up frame\n",
    "column_list = ['Item_Number', 'file', 'Correct_Category', 'Estimated_Category', 'Max_Softmax_Value','Likelihood',\n",
    "               'In-vs-Out', 'In-Likelihood', 'Out-Likelihood', 'Estimated-In-vs-Out', 'Category_Likelihood_Average', 'Category_Likelihood_Stdev', 'Category_Sample_Count']\n",
    "full_data_frame = pd.DataFrame(columns=column_list)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_best)\n",
    "    \n",
    "    #Initialize Data\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list_all, batch_size: batch, num_epochs:epochs})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        y_val_correct, filename_val, y_guess, likelihood_val, softmax_layer, quartile, l_avg, l_stddev, l_count = sess.run([y, file, max_softmax_val, tan_likelihoods, softmax, bottom_quartile, \n",
    "                                                                                                 likelihood_averages, likelihood_stdev, number_of_batches], \n",
    "                                                                   feed_dict={handle: val_handle, training:False})\n",
    "        \n",
    "        for j in range(batch):\n",
    "            file_string = filename_val[j].decode(\"utf-8\")\n",
    "\n",
    "            case_number = (i * batch) + j\n",
    "            correct_category_value = labels_all[y_val_correct[j]]\n",
    "            \n",
    "            guess_category = labels[y_guess[j]]\n",
    "            \n",
    "            \n",
    "            valid_quartile = quartile[y_guess[j]]\n",
    "            like_avgs = l_avg[y_guess[j]]\n",
    "            like_std = l_stddev[y_guess[j]]\n",
    "            like_count = l_count[y_guess[j]]\n",
    "            \n",
    "            softmax_value = softmax_layer[j][y_guess[j]]\n",
    "            likes_values = likelihood_val[j]\n",
    "            in_v_out = 'Out'\n",
    "            if labels_all[y_val_correct[j]] in labels:\n",
    "                in_v_out = 'In'\n",
    "\n",
    "            In_Like = 'NA'\n",
    "            Out_Like = 'NA'\n",
    "            \n",
    "            if in_v_out == 'Out':\n",
    "                Out_Like = likes_values\n",
    "            else:\n",
    "                In_Like = likes_values\n",
    "                \n",
    "            est_in_v_out = 'Out'\n",
    "            if likes_values > valid_quartile:\n",
    "                est_in_v_out = 'In'\n",
    "\n",
    "            to_add = pd.DataFrame([[case_number,file_string,correct_category_value,guess_category,softmax_value,likes_values, in_v_out, In_Like, Out_Like, est_in_v_out,\n",
    "                                   like_avgs, like_std, like_count]], columns = column_list)\n",
    "            full_data_frame = full_data_frame.append(to_add)\n",
    "        if ((i % 200) == 0):\n",
    "            print(\"Finished Epoch: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Number</th>\n",
       "      <th>file</th>\n",
       "      <th>Correct_Category</th>\n",
       "      <th>Estimated_Category</th>\n",
       "      <th>Max_Softmax_Value</th>\n",
       "      <th>Likelihood</th>\n",
       "      <th>In-vs-Out</th>\n",
       "      <th>In-Likelihood</th>\n",
       "      <th>Out-Likelihood</th>\n",
       "      <th>Estimated-In-vs-Out</th>\n",
       "      <th>Category_Likelihood_Average</th>\n",
       "      <th>Category_Likelihood_Stdev</th>\n",
       "      <th>Category_Sample_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>76.png</td>\n",
       "      <td>otter</td>\n",
       "      <td>otter</td>\n",
       "      <td>0.998008</td>\n",
       "      <td>-25.496887</td>\n",
       "      <td>In</td>\n",
       "      <td>-25.4969</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-26.427334</td>\n",
       "      <td>5.204813</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>79.png</td>\n",
       "      <td>wolf</td>\n",
       "      <td>wolf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.791168</td>\n",
       "      <td>In</td>\n",
       "      <td>16.7912</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-10.738685</td>\n",
       "      <td>5.345759</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>81.png</td>\n",
       "      <td>tulips</td>\n",
       "      <td>rabbit</td>\n",
       "      <td>0.994879</td>\n",
       "      <td>-23.882523</td>\n",
       "      <td>In</td>\n",
       "      <td>-23.8825</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-20.981691</td>\n",
       "      <td>4.495283</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>39.png</td>\n",
       "      <td>bee</td>\n",
       "      <td>sunflowers</td>\n",
       "      <td>0.631561</td>\n",
       "      <td>-29.560852</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-29.5609</td>\n",
       "      <td>In</td>\n",
       "      <td>-29.804106</td>\n",
       "      <td>5.164311</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>44.png</td>\n",
       "      <td>wolf</td>\n",
       "      <td>wolf</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.617004</td>\n",
       "      <td>In</td>\n",
       "      <td>0.617004</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-10.738685</td>\n",
       "      <td>5.345759</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>22.png</td>\n",
       "      <td>sunflowers</td>\n",
       "      <td>sunflowers</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-44.803879</td>\n",
       "      <td>In</td>\n",
       "      <td>-44.8039</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-29.804106</td>\n",
       "      <td>5.164311</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>53.png</td>\n",
       "      <td>palm</td>\n",
       "      <td>palm</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-36.769287</td>\n",
       "      <td>In</td>\n",
       "      <td>-36.7693</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-28.884106</td>\n",
       "      <td>4.149149</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99.png</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>mushrooms</td>\n",
       "      <td>0.960464</td>\n",
       "      <td>-21.032516</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-21.0325</td>\n",
       "      <td>In</td>\n",
       "      <td>-24.507597</td>\n",
       "      <td>4.159605</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>28.png</td>\n",
       "      <td>skyscraper</td>\n",
       "      <td>skyscraper</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.544937</td>\n",
       "      <td>In</td>\n",
       "      <td>-1.54494</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-26.345703</td>\n",
       "      <td>4.597775</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>52.png</td>\n",
       "      <td>tractor</td>\n",
       "      <td>tractor</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-40.556427</td>\n",
       "      <td>In</td>\n",
       "      <td>-40.5564</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-26.066313</td>\n",
       "      <td>4.362991</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>56.png</td>\n",
       "      <td>oak</td>\n",
       "      <td>pine</td>\n",
       "      <td>0.780948</td>\n",
       "      <td>-12.989319</td>\n",
       "      <td>In</td>\n",
       "      <td>-12.9893</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-27.232725</td>\n",
       "      <td>4.399219</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>31.png</td>\n",
       "      <td>pickup truck</td>\n",
       "      <td>pickup truck</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>-34.199738</td>\n",
       "      <td>In</td>\n",
       "      <td>-34.1997</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-20.819408</td>\n",
       "      <td>4.428785</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>86.png</td>\n",
       "      <td>tulips</td>\n",
       "      <td>orchids</td>\n",
       "      <td>0.872058</td>\n",
       "      <td>-45.192780</td>\n",
       "      <td>In</td>\n",
       "      <td>-45.1928</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-33.571442</td>\n",
       "      <td>4.692234</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>84.png</td>\n",
       "      <td>leopard</td>\n",
       "      <td>leopard</td>\n",
       "      <td>0.999569</td>\n",
       "      <td>5.363068</td>\n",
       "      <td>In</td>\n",
       "      <td>5.36307</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-19.732212</td>\n",
       "      <td>4.726110</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>68.png</td>\n",
       "      <td>lion</td>\n",
       "      <td>lion</td>\n",
       "      <td>0.593255</td>\n",
       "      <td>-3.927185</td>\n",
       "      <td>In</td>\n",
       "      <td>-3.92719</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-11.824613</td>\n",
       "      <td>6.109537</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>40.png</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-20.717072</td>\n",
       "      <td>In</td>\n",
       "      <td>-20.7171</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-25.188828</td>\n",
       "      <td>4.886883</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>41.png</td>\n",
       "      <td>tank</td>\n",
       "      <td>clock</td>\n",
       "      <td>0.996243</td>\n",
       "      <td>-24.116852</td>\n",
       "      <td>In</td>\n",
       "      <td>-24.1169</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>11.511752</td>\n",
       "      <td>8.663902</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>73.png</td>\n",
       "      <td>flatfish</td>\n",
       "      <td>flatfish</td>\n",
       "      <td>0.773025</td>\n",
       "      <td>-24.635101</td>\n",
       "      <td>In</td>\n",
       "      <td>-24.6351</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-29.095705</td>\n",
       "      <td>4.381449</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>34.png</td>\n",
       "      <td>tractor</td>\n",
       "      <td>tractor</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>-37.840912</td>\n",
       "      <td>In</td>\n",
       "      <td>-37.8409</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-26.066313</td>\n",
       "      <td>4.362991</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>56.png</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>sunflowers</td>\n",
       "      <td>0.570224</td>\n",
       "      <td>-36.367264</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-36.3673</td>\n",
       "      <td>Out</td>\n",
       "      <td>-29.804106</td>\n",
       "      <td>5.164311</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>74.png</td>\n",
       "      <td>bridge</td>\n",
       "      <td>hamster</td>\n",
       "      <td>0.829098</td>\n",
       "      <td>-23.938461</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-23.9385</td>\n",
       "      <td>In</td>\n",
       "      <td>-29.360243</td>\n",
       "      <td>4.207229</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>99.png</td>\n",
       "      <td>wardrobe</td>\n",
       "      <td>wardrobe</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>-8.422867</td>\n",
       "      <td>In</td>\n",
       "      <td>-8.42287</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-28.398552</td>\n",
       "      <td>4.157048</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>96.png</td>\n",
       "      <td>kangaroo</td>\n",
       "      <td>kangaroo</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>-37.370667</td>\n",
       "      <td>In</td>\n",
       "      <td>-37.3707</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-18.427874</td>\n",
       "      <td>6.408744</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>48.png</td>\n",
       "      <td>snail</td>\n",
       "      <td>lizard</td>\n",
       "      <td>0.951651</td>\n",
       "      <td>0.743240</td>\n",
       "      <td>In</td>\n",
       "      <td>0.74324</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-22.670622</td>\n",
       "      <td>4.857697</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>22.png</td>\n",
       "      <td>spider</td>\n",
       "      <td>spider</td>\n",
       "      <td>0.940815</td>\n",
       "      <td>-31.864838</td>\n",
       "      <td>In</td>\n",
       "      <td>-31.8648</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-23.826330</td>\n",
       "      <td>5.917526</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>33.png</td>\n",
       "      <td>pine</td>\n",
       "      <td>pine</td>\n",
       "      <td>0.466506</td>\n",
       "      <td>-28.490707</td>\n",
       "      <td>In</td>\n",
       "      <td>-28.4907</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-27.232725</td>\n",
       "      <td>4.399219</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>21.png</td>\n",
       "      <td>tulips</td>\n",
       "      <td>roses</td>\n",
       "      <td>0.896798</td>\n",
       "      <td>-24.099548</td>\n",
       "      <td>In</td>\n",
       "      <td>-24.0995</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-32.525005</td>\n",
       "      <td>4.714955</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>13.png</td>\n",
       "      <td>elephant</td>\n",
       "      <td>elephant</td>\n",
       "      <td>0.999868</td>\n",
       "      <td>-35.253845</td>\n",
       "      <td>In</td>\n",
       "      <td>-35.2538</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-29.063416</td>\n",
       "      <td>4.017735</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>60.png</td>\n",
       "      <td>maple</td>\n",
       "      <td>maple</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>-49.755127</td>\n",
       "      <td>In</td>\n",
       "      <td>-49.7551</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-26.041254</td>\n",
       "      <td>4.388068</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>27.png</td>\n",
       "      <td>dinosaur</td>\n",
       "      <td>whale</td>\n",
       "      <td>0.878854</td>\n",
       "      <td>6.967148</td>\n",
       "      <td>In</td>\n",
       "      <td>6.96715</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-13.156709</td>\n",
       "      <td>5.655273</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9970</td>\n",
       "      <td>3.png</td>\n",
       "      <td>leopard</td>\n",
       "      <td>leopard</td>\n",
       "      <td>0.999034</td>\n",
       "      <td>2.980255</td>\n",
       "      <td>In</td>\n",
       "      <td>2.98026</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-19.732212</td>\n",
       "      <td>4.726110</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9971</td>\n",
       "      <td>3.png</td>\n",
       "      <td>trout</td>\n",
       "      <td>trout</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-35.788177</td>\n",
       "      <td>In</td>\n",
       "      <td>-35.7882</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-23.150833</td>\n",
       "      <td>4.419696</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9972</td>\n",
       "      <td>48.png</td>\n",
       "      <td>skyscraper</td>\n",
       "      <td>skyscraper</td>\n",
       "      <td>0.999687</td>\n",
       "      <td>-40.601227</td>\n",
       "      <td>In</td>\n",
       "      <td>-40.6012</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-26.345703</td>\n",
       "      <td>4.597775</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9973</td>\n",
       "      <td>88.png</td>\n",
       "      <td>otter</td>\n",
       "      <td>otter</td>\n",
       "      <td>0.998643</td>\n",
       "      <td>-16.221024</td>\n",
       "      <td>In</td>\n",
       "      <td>-16.221</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-26.427334</td>\n",
       "      <td>5.204813</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9974</td>\n",
       "      <td>1.png</td>\n",
       "      <td>bottles</td>\n",
       "      <td>lamp</td>\n",
       "      <td>0.734099</td>\n",
       "      <td>-36.860489</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-36.8605</td>\n",
       "      <td>Out</td>\n",
       "      <td>-17.842281</td>\n",
       "      <td>6.248699</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9975</td>\n",
       "      <td>20.png</td>\n",
       "      <td>bottles</td>\n",
       "      <td>raccoon</td>\n",
       "      <td>0.466283</td>\n",
       "      <td>-12.251404</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-12.2514</td>\n",
       "      <td>In</td>\n",
       "      <td>-18.972118</td>\n",
       "      <td>4.763253</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976</td>\n",
       "      <td>12.png</td>\n",
       "      <td>crocodile</td>\n",
       "      <td>crocodile</td>\n",
       "      <td>0.888478</td>\n",
       "      <td>-16.000397</td>\n",
       "      <td>In</td>\n",
       "      <td>-16.0004</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-9.916327</td>\n",
       "      <td>4.975370</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9977</td>\n",
       "      <td>51.png</td>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>0.965993</td>\n",
       "      <td>-33.288513</td>\n",
       "      <td>In</td>\n",
       "      <td>-33.2885</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-21.214769</td>\n",
       "      <td>5.275651</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9978</td>\n",
       "      <td>32.png</td>\n",
       "      <td>ray</td>\n",
       "      <td>ray</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-23.324097</td>\n",
       "      <td>In</td>\n",
       "      <td>-23.3241</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-21.743570</td>\n",
       "      <td>5.767827</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9979</td>\n",
       "      <td>28.png</td>\n",
       "      <td>maple</td>\n",
       "      <td>oak</td>\n",
       "      <td>0.549066</td>\n",
       "      <td>-10.807922</td>\n",
       "      <td>In</td>\n",
       "      <td>-10.8079</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-21.606583</td>\n",
       "      <td>3.267720</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9980</td>\n",
       "      <td>32.png</td>\n",
       "      <td>shark</td>\n",
       "      <td>shark</td>\n",
       "      <td>0.971814</td>\n",
       "      <td>-1.740036</td>\n",
       "      <td>In</td>\n",
       "      <td>-1.74004</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-13.173571</td>\n",
       "      <td>5.939130</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9981</td>\n",
       "      <td>73.png</td>\n",
       "      <td>cloud</td>\n",
       "      <td>cloud</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>-15.263672</td>\n",
       "      <td>In</td>\n",
       "      <td>-15.2637</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>2.743865</td>\n",
       "      <td>8.449994</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9982</td>\n",
       "      <td>24.png</td>\n",
       "      <td>rocket</td>\n",
       "      <td>rocket</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>13.780296</td>\n",
       "      <td>In</td>\n",
       "      <td>13.7803</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-9.295359</td>\n",
       "      <td>6.528632</td>\n",
       "      <td>498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9983</td>\n",
       "      <td>69.png</td>\n",
       "      <td>wolf</td>\n",
       "      <td>wolf</td>\n",
       "      <td>0.977339</td>\n",
       "      <td>-8.357254</td>\n",
       "      <td>In</td>\n",
       "      <td>-8.35725</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-10.738685</td>\n",
       "      <td>5.345759</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9984</td>\n",
       "      <td>99.png</td>\n",
       "      <td>man</td>\n",
       "      <td>raccoon</td>\n",
       "      <td>0.998207</td>\n",
       "      <td>-37.487793</td>\n",
       "      <td>In</td>\n",
       "      <td>-37.4878</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-18.972118</td>\n",
       "      <td>4.763253</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9985</td>\n",
       "      <td>16.png</td>\n",
       "      <td>crab</td>\n",
       "      <td>crab</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-30.288376</td>\n",
       "      <td>In</td>\n",
       "      <td>-30.2884</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-22.614058</td>\n",
       "      <td>4.273931</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9986</td>\n",
       "      <td>22.png</td>\n",
       "      <td>skyscraper</td>\n",
       "      <td>skyscraper</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-10.778641</td>\n",
       "      <td>In</td>\n",
       "      <td>-10.7786</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-26.345703</td>\n",
       "      <td>4.597775</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9987</td>\n",
       "      <td>86.png</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>0.843785</td>\n",
       "      <td>-43.950958</td>\n",
       "      <td>In</td>\n",
       "      <td>-43.951</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-26.389227</td>\n",
       "      <td>4.840188</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9988</td>\n",
       "      <td>8.png</td>\n",
       "      <td>palm</td>\n",
       "      <td>palm</td>\n",
       "      <td>0.981409</td>\n",
       "      <td>-9.941574</td>\n",
       "      <td>In</td>\n",
       "      <td>-9.94157</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-28.884106</td>\n",
       "      <td>4.149149</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9989</td>\n",
       "      <td>93.png</td>\n",
       "      <td>castle</td>\n",
       "      <td>hamster</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-28.773453</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-28.7735</td>\n",
       "      <td>In</td>\n",
       "      <td>-29.360243</td>\n",
       "      <td>4.207229</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9990</td>\n",
       "      <td>31.png</td>\n",
       "      <td>willow</td>\n",
       "      <td>pine</td>\n",
       "      <td>0.556615</td>\n",
       "      <td>-20.077667</td>\n",
       "      <td>In</td>\n",
       "      <td>-20.0777</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-27.232725</td>\n",
       "      <td>4.399219</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9991</td>\n",
       "      <td>25.png</td>\n",
       "      <td>snail</td>\n",
       "      <td>shark</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>-32.260925</td>\n",
       "      <td>In</td>\n",
       "      <td>-32.2609</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-13.173571</td>\n",
       "      <td>5.939130</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9992</td>\n",
       "      <td>21.png</td>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>0.996367</td>\n",
       "      <td>-42.012772</td>\n",
       "      <td>In</td>\n",
       "      <td>-42.0128</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-21.214769</td>\n",
       "      <td>5.275651</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9993</td>\n",
       "      <td>0.png</td>\n",
       "      <td>baby</td>\n",
       "      <td>fox</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-33.709015</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-33.709</td>\n",
       "      <td>In</td>\n",
       "      <td>-36.368340</td>\n",
       "      <td>5.076506</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9994</td>\n",
       "      <td>83.png</td>\n",
       "      <td>otter</td>\n",
       "      <td>crab</td>\n",
       "      <td>0.691278</td>\n",
       "      <td>-21.971405</td>\n",
       "      <td>In</td>\n",
       "      <td>-21.9714</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-22.614058</td>\n",
       "      <td>4.273931</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9995</td>\n",
       "      <td>31.png</td>\n",
       "      <td>couch</td>\n",
       "      <td>couch</td>\n",
       "      <td>0.737044</td>\n",
       "      <td>-34.844650</td>\n",
       "      <td>In</td>\n",
       "      <td>-34.8447</td>\n",
       "      <td>NA</td>\n",
       "      <td>Out</td>\n",
       "      <td>-21.079483</td>\n",
       "      <td>5.581104</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9996</td>\n",
       "      <td>88.png</td>\n",
       "      <td>beaver</td>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>0.522268</td>\n",
       "      <td>-20.096176</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-20.0962</td>\n",
       "      <td>Out</td>\n",
       "      <td>9.814995</td>\n",
       "      <td>8.861466</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9997</td>\n",
       "      <td>15.png</td>\n",
       "      <td>apples</td>\n",
       "      <td>pears</td>\n",
       "      <td>0.664011</td>\n",
       "      <td>-32.287735</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-32.2877</td>\n",
       "      <td>Out</td>\n",
       "      <td>-28.862125</td>\n",
       "      <td>4.594080</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9998</td>\n",
       "      <td>87.png</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>orchids</td>\n",
       "      <td>0.999859</td>\n",
       "      <td>-58.645844</td>\n",
       "      <td>Out</td>\n",
       "      <td>NA</td>\n",
       "      <td>-58.6458</td>\n",
       "      <td>Out</td>\n",
       "      <td>-33.571442</td>\n",
       "      <td>4.692234</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999</td>\n",
       "      <td>58.png</td>\n",
       "      <td>whale</td>\n",
       "      <td>whale</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-9.294067</td>\n",
       "      <td>In</td>\n",
       "      <td>-9.29407</td>\n",
       "      <td>NA</td>\n",
       "      <td>In</td>\n",
       "      <td>-13.156709</td>\n",
       "      <td>5.655273</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Item_Number    file Correct_Category Estimated_Category  Max_Softmax_Value  \\\n",
       "0            0  76.png            otter              otter           0.998008   \n",
       "0            1  79.png             wolf               wolf           1.000000   \n",
       "0            2  81.png           tulips             rabbit           0.994879   \n",
       "0            3  39.png              bee         sunflowers           0.631561   \n",
       "0            4  44.png             wolf               wolf           0.999986   \n",
       "0            5  22.png       sunflowers         sunflowers           1.000000   \n",
       "0            6  53.png             palm               palm           1.000000   \n",
       "0            7  99.png        butterfly          mushrooms           0.960464   \n",
       "0            8  28.png       skyscraper         skyscraper           1.000000   \n",
       "0            9  52.png          tractor            tractor           1.000000   \n",
       "0           10  56.png              oak               pine           0.780948   \n",
       "0           11  31.png     pickup truck       pickup truck           0.999979   \n",
       "0           12  86.png           tulips            orchids           0.872058   \n",
       "0           13  84.png          leopard            leopard           0.999569   \n",
       "0           14  68.png             lion               lion           0.593255   \n",
       "0           15  40.png       motorcycle         motorcycle           1.000000   \n",
       "0           16  41.png             tank              clock           0.996243   \n",
       "0           17  73.png         flatfish           flatfish           0.773025   \n",
       "0           18  34.png          tractor            tractor           0.999972   \n",
       "0           19  56.png        butterfly         sunflowers           0.570224   \n",
       "0           20  74.png           bridge            hamster           0.829098   \n",
       "0           21  99.png         wardrobe           wardrobe           0.999969   \n",
       "0           22  96.png         kangaroo           kangaroo           0.999993   \n",
       "0           23  48.png            snail             lizard           0.951651   \n",
       "0           24  22.png           spider             spider           0.940815   \n",
       "0           25  33.png             pine               pine           0.466506   \n",
       "0           26  21.png           tulips              roses           0.896798   \n",
       "0           27  13.png         elephant           elephant           0.999868   \n",
       "0           28  60.png            maple              maple           0.999979   \n",
       "0           29  27.png         dinosaur              whale           0.878854   \n",
       "..         ...     ...              ...                ...                ...   \n",
       "0         9970   3.png          leopard            leopard           0.999034   \n",
       "0         9971   3.png            trout              trout           1.000000   \n",
       "0         9972  48.png       skyscraper         skyscraper           0.999687   \n",
       "0         9973  88.png            otter              otter           0.998643   \n",
       "0         9974   1.png          bottles               lamp           0.734099   \n",
       "0         9975  20.png          bottles            raccoon           0.466283   \n",
       "0         9976  12.png        crocodile          crocodile           0.888478   \n",
       "0         9977  51.png            tiger              tiger           0.965993   \n",
       "0         9978  32.png              ray                ray           1.000000   \n",
       "0         9979  28.png            maple                oak           0.549066   \n",
       "0         9980  32.png            shark              shark           0.971814   \n",
       "0         9981  73.png            cloud              cloud           0.982011   \n",
       "0         9982  24.png           rocket             rocket           0.999980   \n",
       "0         9983  69.png             wolf               wolf           0.977339   \n",
       "0         9984  99.png              man            raccoon           0.998207   \n",
       "0         9985  16.png             crab               crab           1.000000   \n",
       "0         9986  22.png       skyscraper         skyscraper           0.999998   \n",
       "0         9987  86.png            train              train           0.843785   \n",
       "0         9988   8.png             palm               palm           0.981409   \n",
       "0         9989  93.png           castle            hamster           1.000000   \n",
       "0         9990  31.png           willow               pine           0.556615   \n",
       "0         9991  25.png            snail              shark           0.998097   \n",
       "0         9992  21.png            tiger              tiger           0.996367   \n",
       "0         9993   0.png             baby                fox           1.000000   \n",
       "0         9994  83.png            otter               crab           0.691278   \n",
       "0         9995  31.png            couch              couch           0.737044   \n",
       "0         9996  88.png           beaver         chimpanzee           0.522268   \n",
       "0         9997  15.png           apples              pears           0.664011   \n",
       "0         9998  87.png        butterfly            orchids           0.999859   \n",
       "0         9999  58.png            whale              whale           1.000000   \n",
       "\n",
       "    Likelihood In-vs-Out In-Likelihood Out-Likelihood Estimated-In-vs-Out  \\\n",
       "0   -25.496887        In      -25.4969             NA                  In   \n",
       "0    16.791168        In       16.7912             NA                  In   \n",
       "0   -23.882523        In      -23.8825             NA                 Out   \n",
       "0   -29.560852       Out            NA       -29.5609                  In   \n",
       "0     0.617004        In      0.617004             NA                  In   \n",
       "0   -44.803879        In      -44.8039             NA                 Out   \n",
       "0   -36.769287        In      -36.7693             NA                 Out   \n",
       "0   -21.032516       Out            NA       -21.0325                  In   \n",
       "0    -1.544937        In      -1.54494             NA                  In   \n",
       "0   -40.556427        In      -40.5564             NA                 Out   \n",
       "0   -12.989319        In      -12.9893             NA                  In   \n",
       "0   -34.199738        In      -34.1997             NA                 Out   \n",
       "0   -45.192780        In      -45.1928             NA                 Out   \n",
       "0     5.363068        In       5.36307             NA                  In   \n",
       "0    -3.927185        In      -3.92719             NA                  In   \n",
       "0   -20.717072        In      -20.7171             NA                  In   \n",
       "0   -24.116852        In      -24.1169             NA                 Out   \n",
       "0   -24.635101        In      -24.6351             NA                  In   \n",
       "0   -37.840912        In      -37.8409             NA                 Out   \n",
       "0   -36.367264       Out            NA       -36.3673                 Out   \n",
       "0   -23.938461       Out            NA       -23.9385                  In   \n",
       "0    -8.422867        In      -8.42287             NA                  In   \n",
       "0   -37.370667        In      -37.3707             NA                 Out   \n",
       "0     0.743240        In       0.74324             NA                  In   \n",
       "0   -31.864838        In      -31.8648             NA                 Out   \n",
       "0   -28.490707        In      -28.4907             NA                 Out   \n",
       "0   -24.099548        In      -24.0995             NA                  In   \n",
       "0   -35.253845        In      -35.2538             NA                 Out   \n",
       "0   -49.755127        In      -49.7551             NA                 Out   \n",
       "0     6.967148        In       6.96715             NA                  In   \n",
       "..         ...       ...           ...            ...                 ...   \n",
       "0     2.980255        In       2.98026             NA                  In   \n",
       "0   -35.788177        In      -35.7882             NA                 Out   \n",
       "0   -40.601227        In      -40.6012             NA                 Out   \n",
       "0   -16.221024        In       -16.221             NA                  In   \n",
       "0   -36.860489       Out            NA       -36.8605                 Out   \n",
       "0   -12.251404       Out            NA       -12.2514                  In   \n",
       "0   -16.000397        In      -16.0004             NA                 Out   \n",
       "0   -33.288513        In      -33.2885             NA                 Out   \n",
       "0   -23.324097        In      -23.3241             NA                 Out   \n",
       "0   -10.807922        In      -10.8079             NA                  In   \n",
       "0    -1.740036        In      -1.74004             NA                  In   \n",
       "0   -15.263672        In      -15.2637             NA                 Out   \n",
       "0    13.780296        In       13.7803             NA                  In   \n",
       "0    -8.357254        In      -8.35725             NA                  In   \n",
       "0   -37.487793        In      -37.4878             NA                 Out   \n",
       "0   -30.288376        In      -30.2884             NA                 Out   \n",
       "0   -10.778641        In      -10.7786             NA                  In   \n",
       "0   -43.950958        In       -43.951             NA                 Out   \n",
       "0    -9.941574        In      -9.94157             NA                  In   \n",
       "0   -28.773453       Out            NA       -28.7735                  In   \n",
       "0   -20.077667        In      -20.0777             NA                  In   \n",
       "0   -32.260925        In      -32.2609             NA                 Out   \n",
       "0   -42.012772        In      -42.0128             NA                 Out   \n",
       "0   -33.709015       Out            NA        -33.709                  In   \n",
       "0   -21.971405        In      -21.9714             NA                  In   \n",
       "0   -34.844650        In      -34.8447             NA                 Out   \n",
       "0   -20.096176       Out            NA       -20.0962                 Out   \n",
       "0   -32.287735       Out            NA       -32.2877                 Out   \n",
       "0   -58.645844       Out            NA       -58.6458                 Out   \n",
       "0    -9.294067        In      -9.29407             NA                  In   \n",
       "\n",
       "    Category_Likelihood_Average  Category_Likelihood_Stdev  \\\n",
       "0                    -26.427334                   5.204813   \n",
       "0                    -10.738685                   5.345759   \n",
       "0                    -20.981691                   4.495283   \n",
       "0                    -29.804106                   5.164311   \n",
       "0                    -10.738685                   5.345759   \n",
       "0                    -29.804106                   5.164311   \n",
       "0                    -28.884106                   4.149149   \n",
       "0                    -24.507597                   4.159605   \n",
       "0                    -26.345703                   4.597775   \n",
       "0                    -26.066313                   4.362991   \n",
       "0                    -27.232725                   4.399219   \n",
       "0                    -20.819408                   4.428785   \n",
       "0                    -33.571442                   4.692234   \n",
       "0                    -19.732212                   4.726110   \n",
       "0                    -11.824613                   6.109537   \n",
       "0                    -25.188828                   4.886883   \n",
       "0                     11.511752                   8.663902   \n",
       "0                    -29.095705                   4.381449   \n",
       "0                    -26.066313                   4.362991   \n",
       "0                    -29.804106                   5.164311   \n",
       "0                    -29.360243                   4.207229   \n",
       "0                    -28.398552                   4.157048   \n",
       "0                    -18.427874                   6.408744   \n",
       "0                    -22.670622                   4.857697   \n",
       "0                    -23.826330                   5.917526   \n",
       "0                    -27.232725                   4.399219   \n",
       "0                    -32.525005                   4.714955   \n",
       "0                    -29.063416                   4.017735   \n",
       "0                    -26.041254                   4.388068   \n",
       "0                    -13.156709                   5.655273   \n",
       "..                          ...                        ...   \n",
       "0                    -19.732212                   4.726110   \n",
       "0                    -23.150833                   4.419696   \n",
       "0                    -26.345703                   4.597775   \n",
       "0                    -26.427334                   5.204813   \n",
       "0                    -17.842281                   6.248699   \n",
       "0                    -18.972118                   4.763253   \n",
       "0                     -9.916327                   4.975370   \n",
       "0                    -21.214769                   5.275651   \n",
       "0                    -21.743570                   5.767827   \n",
       "0                    -21.606583                   3.267720   \n",
       "0                    -13.173571                   5.939130   \n",
       "0                      2.743865                   8.449994   \n",
       "0                     -9.295359                   6.528632   \n",
       "0                    -10.738685                   5.345759   \n",
       "0                    -18.972118                   4.763253   \n",
       "0                    -22.614058                   4.273931   \n",
       "0                    -26.345703                   4.597775   \n",
       "0                    -26.389227                   4.840188   \n",
       "0                    -28.884106                   4.149149   \n",
       "0                    -29.360243                   4.207229   \n",
       "0                    -27.232725                   4.399219   \n",
       "0                    -13.173571                   5.939130   \n",
       "0                    -21.214769                   5.275651   \n",
       "0                    -36.368340                   5.076506   \n",
       "0                    -22.614058                   4.273931   \n",
       "0                    -21.079483                   5.581104   \n",
       "0                      9.814995                   8.861466   \n",
       "0                    -28.862125                   4.594080   \n",
       "0                    -33.571442                   4.692234   \n",
       "0                    -13.156709                   5.655273   \n",
       "\n",
       "    Category_Sample_Count  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   499.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   499.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "..                    ...  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   498.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "0                   499.0  \n",
       "0                   500.0  \n",
       "0                   500.0  \n",
       "\n",
       "[10000 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the dataframe\n",
    "\n",
    "full_data_frame.to_csv('Out_Of_Set_Tan_Final.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Testing\n",
    "\n",
    "Here are some helpful scripts for doing additional testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory for all the models saved during training\n",
    "out_of_set_net_model_class = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_class'\n",
    "out_of_set_net_model_tan = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_tan'\n",
    "out_of_set_net_best = 'D:/AI/models/out_of_set_net_final/model/' + 'out_of_set_net_best'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Pooling\n",
    "\n",
    "Here we need to test to see that the pooling does a decent job of capturing the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_model_tan)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: 10, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    code, encode, cat = sess.run([cnn_code_layer,AutoCode,y],feed_dict={handle:val_handle, training:False})\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum(code[0])\n",
    "b = sum(encode[0])\n",
    "print(a,b)\n",
    "code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.array(range(0,cnn_code_layer_size))\n",
    "\n",
    "code_frame = pd.DataFrame([x_range,code[0]]).transpose()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "code_frame.plot(kind='scatter', x=0, y=1)\n",
    "#plt.axis('on', ylim=(0,1))\n",
    "#plt.axes(ylim=(0,1), xlim=(0,5000))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,108),encode[0])\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the code as a graph of tensor values coming out\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,cnn_code_layer_size),code[0])\n",
    "plt.axis('on')\n",
    "plt.savefig('original_code.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare to the decode as a graph of tensor values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(0,108),encode[0])\n",
    "plt.axis('on')\n",
    "plt.savefig('condensed_code.jpeg')\n",
    "plt.show()\n",
    "cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure Out Threshold\n",
    "\n",
    "Test code for finding a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_model_tan)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: train_list, batch_size: 30, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    \n",
    "    average_before = sess.run([likelihood_averages],feed_dict={handle:val_handle, training:False})\n",
    "    std_before = sess.run([likelihood_stdev], feed_dict={handle:val_handle, training:False})\n",
    "    sess.run([update_average_likelihood ], feed_dict={handle:val_handle, training:False})\n",
    "    std_after = sess.run([likelihood_stdev], feed_dict={handle:val_handle, training:False})\n",
    "    average_after = sess.run([likelihood_averages], feed_dict={handle:val_handle, training:False})\n",
    "    sess.run([update_average_likelihood ], feed_dict={handle:val_handle, training:False})\n",
    "    std_after_2 = sess.run([likelihood_stdev], feed_dict={handle:val_handle, training:False})\n",
    "    average_after_2 = sess.run([likelihood_averages], feed_dict={handle:val_handle, training:False})\n",
    "    bottom = sess.run([bottom_quartile], feed_dict={handle:val_handle, training:False})\n",
    "    counts = sess.run([number_of_batches], feed_dict={handle:val_handle, training:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_after_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_after_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the classifier\n",
    "\n",
    "Here we need to test to see that the classifier does a decent job of capturing the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_best)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: 30, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    accuracy_val, loss, guess, truth = sess.run([accuracy, loss_cat, max_softmax_val, y], \n",
    "                                            feed_dict={handle:val_handle, training:False})\n",
    "                    \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the tan\n",
    "Here we need to test to see that the tan does a decent job of capturing the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Restore File\n",
    "    saver_tan.restore(sess, out_of_set_net_best)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list_all, batch_size: 30, num_epochs:1})\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    loss, guess, truth, like_loss, like = sess.run([loss_cat, max_softmax_val, y, tan_loss, tan_likelihoods], \n",
    "                                            feed_dict={handle: val_handle, training: False})\n",
    "                    \n",
    "like_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(like,truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data,columns=['true_class','likelihood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct Answer\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.scatter(x=x_array, y=y_array, c='r', s=40)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guess\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.scatter(x=x_guess, y=y_guess, c='r', s=40)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore graph from meta and restore variables\n",
    "    new_saver = tf.train.import_meta_graph(out_of_set_net_best + '.meta')\n",
    "    new_saver.restore(sess, out_of_set_net_best)\n",
    "    soft = tf.get_default_graph().get_tensor_by_name(\"Out_Of_Set_Classifier/Final_Layer/final_soft_max:0\")\n",
    "    input_tensor = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    val = soft.eval(feed_dict={input_tensor: cat, training: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_net_dict_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = np.reshape(last_layers, (num_epochs,batch_size,4320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, out_of_set_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    \n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" time.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
