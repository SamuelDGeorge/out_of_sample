{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All neccesary classes for project\n",
    "\n",
    "#general\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#for preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "\n",
    "#for machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "#for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "from Utilities.utilities import display_scores\n",
    "from Utilities.utilities import pipeline_transform\n",
    "from Utilities.utilities import reset_graph\n",
    "from Utilities.models import DNN_Model\n",
    "from Utilities.models import cross_val_score_dnn\n",
    "from functools import partial\n",
    "\n",
    "#image manipulation\n",
    "from PIL import Image as PI\n",
    "from PIL import ImageEnhance\n",
    "from resizeimage import resizeimage\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.data_utils import get_file\n",
    "import vgg_preprocessing\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "import pnasnet as nas\n",
    "\n",
    "#Import Custom Functions\n",
    "from Utilities.model_builder import get_image\n",
    "from Utilities.model_builder import get_file_lists\n",
    "from Utilities.model_builder import parse_record\n",
    "from Utilities.model_builder import get_batch\n",
    "from Utilities.model_builder import build_iterator\n",
    "from Utilities.model_builder import get_values\n",
    "from Utilities.models import log_dir_build\n",
    "from Utilities.utilities import generate_image\n",
    "from Utilities.utilities import generate_image_array\n",
    "from Utilities.cell_net_predictor import Binary_Categorical_Predictor\n",
    "from Utilities.build_image_data_notebook import process_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Here we will load the training and validation data in order to do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build tf_records\n",
    "validation_directory = \"C:/AI/Brain_Net/training_data/validation\"\n",
    "train_directory = \"C:/AI/Brain_Net/training_data/train\"\n",
    "output_directory = \"C:/AI/Brain_Net/training_data/tf_records\"\n",
    "labels_file = \"C:/AI/Brain_Net/training_data/labels.txt\"\n",
    "num_threads = 2\n",
    "num_shards = 2\n",
    "\n",
    "#make validation records\n",
    "process_dataset('validation', validation_directory, num_shards, labels_file, num_threads, output_directory)\n",
    "\n",
    "#make validation records\n",
    "process_dataset('train', train_directory, num_shards, labels_file, num_threads, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import TFRecords for both Training and Testing of the Dat\n",
    "#Use the build_image_data.py to create these sets from your data\n",
    "train_list, val_list = get_file_lists(\"C:/AI/Cell_Net_v2/training_data/tf_records\")\n",
    "labels = [\"Punctate\", \"Striated\"]\n",
    "\n",
    "\n",
    "#Nasnet Model Location\n",
    "nas_net_model = 'C:/AI/pnas_net/model.ckpt'\n",
    "cell_net_logs = 'C:/AI/Brain_Net/logs'\n",
    "cell_net_model = 'C:/AI/Brain_Net/model/cell_net'\n",
    "cell_net_best = 'C:/AI/Brain_Net/model/cell_net_best'\n",
    "\n",
    "test_image = generate_image_array('C:/AI/Brain_Net/test_image/test.jpg', 331, 331)\n",
    "filename = 'C:/AI/Brain_Net/test_image/test.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network on top of Large Nas-Net\n",
    "\n",
    "Here we will build the Nas-Net and then stack our own network on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.\n"
     ]
    }
   ],
   "source": [
    "#Reset the graph \n",
    "reset_graph()\n",
    "\n",
    "#Set constants for Neural Network\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "n_hidden1 = 500\n",
    "n_final_layer = 2\n",
    "\n",
    "\n",
    "#Placeholder for input data\n",
    "X = tf.placeholder(tf.float32, shape=[None, 331, 331, 3], name=\"input\")\n",
    "y = tf.placeholder(tf.int32, shape=[None], name=\"output\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name = 'training')\n",
    "\n",
    "#Define initalizer and batch normalization layers\n",
    "bn_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "#Import the Nas_Net and build it\n",
    "with slim.arg_scope(nas.pnasnet_large_arg_scope()):\n",
    "    net, end_points = nas.build_pnasnet_large(X, num_classes=1001, is_training=False)\n",
    "    \n",
    "    #A saver to load the pretrained Data\n",
    "    saver = tf.train.Saver(name=\"Original_Saver\")\n",
    "    \n",
    "    #For getting predictions from Original Network\n",
    "    soft_max_pna = tf.get_default_graph().get_tensor_by_name(\"final_layer/predictions:0\")\n",
    "    \n",
    "    #Load in the noder where we are going to connect our own network\n",
    "    last_feature_node = tf.get_default_graph().get_tensor_by_name(\"final_layer/dropout/Identity:0\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"DNN_Classifier\"):\n",
    "    #Use a stop layer to freeze all the layers beneath in Nas-Net\n",
    "    with tf.name_scope(\"Hidden_Layer_1\"):\n",
    "        #stop_layer = tf.stop_gradient(last_feature_node)\n",
    "        #hidden1 = tf.layers.dense(stop_layer, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "        hidden1 = tf.layers.dense(last_feature_node, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "        hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "        bn1 = bn_batch_norm_layer(hidden1_drop)\n",
    "        bn1_act = tf.nn.relu(bn1)\n",
    "    \n",
    "    with tf.name_scope(\"Final_Layer\"):\n",
    "        logits_before_bn = tf.layers.dense(bn1_act, n_final_layer, name=\"outputs\")\n",
    "        logits = bn_batch_norm_layer(logits_before_bn, name=\"logits\")\n",
    "        softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "            loss_summary = tf.summary.scalar('loss_summary', loss)\n",
    "            \n",
    "    with tf.name_scope(\"train\"):\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        decay_steps = 800\n",
    "        decay_rate = 1/8\n",
    "        learning_decay = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_decay, momentum=0.9, use_nesterov=True)\n",
    "        training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        \n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        accuracy_summary = tf.summary.scalar('accuracy_summary', accuracy)\n",
    "        \n",
    "        \n",
    "            \n",
    "#Variables for global initialization\n",
    "saver2 = tf.train.Saver(name=\"Full_Saver\")\n",
    "init = tf.global_variables_initializer()\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "#filewriter = tf.summary.FileWriter(cell_net_logs, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/AI/pnas_net/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Initialize all variables and restore the lower layer\n",
    "with tf.Session() as sess:\n",
    "    #Initalizer all variables\n",
    "    init.run()\n",
    "    \n",
    "    #Restore the pretrained variables from Nas-Net\n",
    "    saver.restore(sess, nas_net_model)\n",
    "    \n",
    "    \n",
    "    #Save all of these variables to the new Cell_Net Model\n",
    "    saver2.save(sess, cell_net_model)\n",
    "    saver2.save(sess, cell_net_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Exporting Graph\n",
    "filewriter = tf.summary.FileWriter(cell_net_logs, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Testing\n",
    "\n",
    "Here we will make sure the pre-trained network loaded and is effective with Imagenet.\n",
    "If correct using first of Imagenet test it should print 286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/AI/Cell_Net_v3/model/cell_net\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver2.restore(sess, cell_net_model)\n",
    "    y_raw = soft_max_pna.eval(feed_dict={X: test_image})\n",
    "    \n",
    "frame = pd.DataFrame(y_raw)\n",
    "item = frame.iloc[0,:]\n",
    "item = pd.Series.idxmax(item)\n",
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "Here we are going to train the network. Accuracy/Loss is recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Graph into a file with Filewriter and add summaries for this session\n",
    "#This will be used for all the following\n",
    "model_path = log_dir_build(cell_net_logs, \"cell_out\")\n",
    "filewriter = tf.summary.FileWriter(model_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/AI/Cell_Net_v3/model/cell_net\n",
      "Loaded model. Training network initially. Logs into: C:/AI/Cell_Net_v3/logs/cell_out-run-20180613221426/\n",
      "Epoch: 0 Loss: 0.700584\n",
      "Epoch: 5 Loss: 0.0897908\n",
      "Epoch: 10 Loss: 0.0985085\n",
      "Epoch: 15 Loss: 0.123223\n",
      "Epoch: 20 Loss: 0.103708\n",
      "Epoch: 25 Loss: 0.0937683\n",
      "Epoch: 30 Loss: 0.100111\n",
      "Epoch: 35 Loss: 0.0764613\n",
      "Epoch: 40 Loss: 0.0938847\n",
      "Epoch: 45 Loss: 0.105796\n",
      "Epoch: 50 Loss: 0.097321\n",
      "Epoch: 55 Loss: 0.0952291\n",
      "Epoch: 60 Loss: 0.0530423\n",
      "Epoch: 65 Loss: 0.0707191\n",
      "Epoch: 70 Loss: 0.101922\n",
      "Epoch: 75 Loss: 0.0734559\n",
      "Epoch: 80 Loss: 0.0768943\n",
      "Epoch: 85 Loss: 0.0700465\n",
      "Epoch: 90 Loss: 0.079259\n",
      "Epoch: 95 Loss: 0.0817259\n",
      "Did 800 of loss minimized training in 885.0762724876404 seconds.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "num_epochs = 100\n",
    "steps_between_test_save = 5\n",
    "batch_size = 5\n",
    "train_size = 40\n",
    "all_data_steps = np.int(np.floor(train_size/batch_size))\n",
    "lowest_loss = 100\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, cell_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    iterator_train = build_iterator(True, train_list, batch_size, num_epochs=num_epochs, num_parallel_calls=8)\n",
    "    iterator_test = build_iterator(False, val_list, batch_size, num_epochs=num_epochs, num_parallel_calls=4)\n",
    "    next_train = iterator_train.get_next()\n",
    "    next_test = iterator_test.get_next()\n",
    "    \n",
    "    #Get initial loss\n",
    "    X_val, y_val = next_test\n",
    "    X_val, y_val = get_values(sess, X_val, y_val)\n",
    "    acc_sum, loss_sum, loss_val = sess.run([accuracy_summary, loss_summary, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "    filewriter.add_summary(acc_sum, step)\n",
    "    filewriter.add_summary(loss_sum, step)\n",
    "    print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < num_epochs:\n",
    "        for i in range(all_data_steps):\n",
    "            #Get training data\n",
    "            X_val, y_val = next_train\n",
    "            X_val, y_val = get_values(sess, X_val, y_val)\n",
    "        \n",
    "            #run Training Op\n",
    "            sess.run([training_op, extra_update_ops, accuracy_summary], feed_dict={X: X_val, y: y_val, training: True})\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0 and step != 0) :\n",
    "            X_val, y_val = next_test\n",
    "            X_val, y_val = get_values(sess, X_val, y_val)\n",
    "            acc_sum, loss_sum, loss_val = sess.run([accuracy_summary, loss_summary, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            saver2.save(sess, cell_net_model)\n",
    "            if lowest_loss > loss_val:\n",
    "                lowest_loss = loss_val\n",
    "                saver2.save(sess, cell_net_best)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver2.save(sess, cell_net_model)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/AI/Cell_Net_v3/model/cell_net\n",
      "Loaded model. Starting training from epoch: 100, Logging in directory: C:/AI/Cell_Net_v3/logs/cell_out-run-20180613221426/\n",
      "Epoch: 100 Loss: 0.0780504\n",
      "Epoch: 105 Loss: 0.0652211\n",
      "Epoch: 110 Loss: 0.0816667\n",
      "Epoch: 115 Loss: 0.0709497\n",
      "Epoch: 120 Loss: 0.0760638\n",
      "Epoch: 125 Loss: 0.0457527\n",
      "Epoch: 130 Loss: 0.0746388\n",
      "Epoch: 135 Loss: 0.0496457\n",
      "Epoch: 140 Loss: 0.069042\n",
      "Epoch: 145 Loss: 0.0635618\n",
      "Epoch: 150 Loss: 0.0795265\n",
      "Epoch: 155 Loss: 0.0508347\n",
      "Epoch: 160 Loss: 0.0590328\n",
      "Epoch: 165 Loss: 0.0642946\n",
      "Epoch: 170 Loss: 0.0720378\n",
      "Epoch: 175 Loss: 0.0594232\n",
      "Epoch: 180 Loss: 0.078906\n",
      "Epoch: 185 Loss: 0.0606648\n",
      "Epoch: 190 Loss: 0.0584681\n",
      "Epoch: 195 Loss: 0.078923\n",
      "Did 1600 of loss minimized training in 877.5314371585846 seconds.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#For picking up with additional training\n",
    "#Total number of epochs to train\n",
    "num_epochs = 200\n",
    "steps_between_test_save = 5\n",
    "batch_size = 5\n",
    "train_size = 40\n",
    "all_data_steps = np.int(np.floor(train_size/batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, cell_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = tf.train.global_step(sess, global_step)\n",
    "    step = np.int(np.floor(step/all_data_steps))\n",
    "    print(\"Loaded model. Starting training from epoch: \" + str(step) + \", Logging in directory: \" + model_path)\n",
    "    \n",
    "    #Build iterators to pull train and test data\n",
    "    iterator_train = build_iterator(True, train_list, batch_size, num_epochs=num_epochs - step, num_parallel_calls=8)\n",
    "    iterator_test = build_iterator(False, val_list, batch_size, num_epochs=num_epochs, num_parallel_calls=4)\n",
    "    next_train = iterator_train.get_next()\n",
    "    next_test = iterator_test.get_next()\n",
    "    \n",
    "\n",
    "\n",
    "    #Iterate through training \n",
    "    while step < num_epochs:\n",
    "        #Go over all the data\n",
    "        for i in range(all_data_steps):\n",
    "            #Get training data\n",
    "            X_val, y_val = next_train\n",
    "            X_val, y_val = get_values(sess, X_val, y_val)\n",
    "        \n",
    "            #run Training Op\n",
    "            sess.run([training_op, extra_update_ops, accuracy_summary], feed_dict={X: X_val, y: y_val, training: True})\n",
    "        \n",
    "        #Maybe Test Accuracy\n",
    "        if (step % steps_between_test_save) == 0:\n",
    "            X_val, y_val = next_test\n",
    "            X_val, y_val = get_values(sess, X_val, y_val)\n",
    "            acc_sum, loss_sum, loss_val = sess.run([accuracy_summary, loss_summary, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            saver2.save(sess, cell_net_model)\n",
    "            print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            if lowest_loss > loss_val:\n",
    "                lowest_loss = loss_val\n",
    "                saver2.save(sess, cell_net_best)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver2.save(sess, cell_net_model)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/AI/Cell_Net_v2/model/cell_net\n",
      "Loaded model. Training network initially. Logs into: C:/AI/Cell_Net_v2/logs/cell_out-run-20180612210412/\n",
      "Epoch: 0 Loss: 1.31359e+09\n",
      "Epoch: 1 Loss: 15.631\n",
      "Epoch: 2 Loss: 5.5989\n",
      "Epoch: 3 Loss: 5.56974\n",
      "Epoch: 4 Loss: 4.8441\n",
      "Epoch: 5 Loss: 7.23256\n",
      "Epoch: 6 Loss: 3.44824\n",
      "Epoch: 7 Loss: 2.50847\n",
      "Epoch: 8 Loss: 0.838452\n",
      "Epoch: 9 Loss: 3.99542\n",
      "Epoch: 10 Loss: 1.92004\n",
      "Epoch: 11 Loss: 1.92797\n",
      "Epoch: 12 Loss: 1.56632\n",
      "Epoch: 13 Loss: 0.806522\n",
      "Epoch: 14 Loss: 0.531574\n",
      "Epoch: 15 Loss: 0.945605\n",
      "Epoch: 16 Loss: 0.670011\n",
      "Epoch: 17 Loss: 1.25973\n",
      "Epoch: 18 Loss: 1.77117\n",
      "Epoch: 19 Loss: 0.644664\n",
      "Epoch: 20 Loss: 0.574298\n",
      "Epoch: 21 Loss: 1.59496\n",
      "Epoch: 22 Loss: 1.13622\n",
      "Epoch: 23 Loss: 1.12575\n",
      "Epoch: 24 Loss: 0.905774\n",
      "Epoch: 25 Loss: 0.931094\n",
      "Epoch: 26 Loss: 0.919483\n",
      "Epoch: 27 Loss: 0.682884\n",
      "Epoch: 28 Loss: 1.16043\n",
      "Epoch: 29 Loss: 0.67443\n",
      "Epoch: 30 Loss: 1.02094\n",
      "Epoch: 31 Loss: 0.542822\n",
      "Epoch: 32 Loss: 0.835304\n",
      "Epoch: 33 Loss: 0.625661\n",
      "Epoch: 34 Loss: 0.59771\n",
      "Epoch: 35 Loss: 0.770304\n",
      "Epoch: 36 Loss: 0.601968\n",
      "Epoch: 37 Loss: 0.940268\n",
      "Epoch: 38 Loss: 0.961525\n",
      "Epoch: 39 Loss: 0.754072\n",
      "Epoch: 40 Loss: 0.518708\n",
      "Epoch: 41 Loss: 1.1215\n",
      "Epoch: 42 Loss: 0.534972\n",
      "Epoch: 43 Loss: 1.04331\n",
      "Epoch: 44 Loss: 0.684363\n",
      "Epoch: 45 Loss: 0.890081\n",
      "Epoch: 46 Loss: 0.479848\n",
      "Epoch: 47 Loss: 1.07004\n",
      "Epoch: 48 Loss: 0.862142\n",
      "Epoch: 49 Loss: 0.658599\n",
      "Did 50 of loss minimized training in 62.6284396648407 time.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Testing\n",
    "#Total number of epochs to train\n",
    "num_epochs = 50\n",
    "steps_between_test_save = 20\n",
    "batch_size = 5\n",
    "train_size = 40\n",
    "all_data_steps = np.int(np.floor(train_size/batch_size))\n",
    "last_layers = np.array([])\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    start_time = time.time()\n",
    "    saver2.restore(sess, cell_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    iterator_train = build_iterator(True, train_list, batch_size, num_epochs=num_epochs, num_parallel_calls=8)\n",
    "    iterator_test = build_iterator(False, val_list, batch_size, num_epochs=num_epochs, num_parallel_calls=4)\n",
    "    next_train = iterator_train.get_next()\n",
    "    next_test = iterator_test.get_next()\n",
    "\n",
    "    #Iterate through training \n",
    "    while step < num_epochs:\n",
    "        \n",
    "        #Get training data\n",
    "        X_val, y_val = next_train\n",
    "        X_val, y_val = get_values(sess, X_val, y_val)\n",
    "\n",
    "        #run Training Op\n",
    "        sess.run([training_op, extra_update_ops, accuracy_summary], feed_dict={X: X_val, y: y_val, training: True})\n",
    "        current_last = last_feature_node.eval(feed_dict={X:X_val})\n",
    "        last_layers = np.append(last_layers,current_last)\n",
    "        #Maybe Test Accuracy\n",
    "        X_val, y_val = next_test\n",
    "        X_val, y_val = get_values(sess, X_val, y_val)\n",
    "        acc_sum, loss_sum, loss_val = sess.run([accuracy_summary, loss_summary, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "        filewriter.add_summary(acc_sum, step)\n",
    "        filewriter.add_summary(loss_sum, step)\n",
    "        print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    final_time = end_time - start_time\n",
    "    saver2.save(sess, cell_net_model)\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" time.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For doing the initial training\n",
    "#Testing\n",
    "#Total number of epochs to train\n",
    "num_epochs = 1\n",
    "batch_size = 5\n",
    "train_size = 40\n",
    "all_data_steps = np.int(np.floor(train_size/batch_size))\n",
    "last_layers = np.array([])\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    start_time = time.time()\n",
    "    saver2.restore(sess, cell_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    iterator_train = build_iterator(True, train_list, batch_size, num_epochs=num_epochs, num_parallel_calls=8)\n",
    "    iterator_test = build_iterator(False, val_list, batch_size, num_epochs=num_epochs, num_parallel_calls=4)\n",
    "    next_train = iterator_train.get_next()\n",
    "    next_test = iterator_test.get_next()\n",
    "\n",
    "    #Iterate through training \n",
    "    while step < num_epochs:\n",
    "        \n",
    "        #Get training data\n",
    "        X_val, y_val = next_train\n",
    "        X_val, y_val = get_values(sess, X_val, y_val)\n",
    "\n",
    "        #run Training Op\n",
    "        sess.run([training_op, extra_update_ops, accuracy_summary], feed_dict={X: X_val, y: y_val, training: True})\n",
    "        current_last = last_feature_node.eval(feed_dict={X:X_val})\n",
    "        last_layers = np.append(last_layers,current_last)\n",
    "        #Maybe Test Accuracy\n",
    "        X_val, y_val = next_test\n",
    "        X_val, y_val = get_values(sess, X_val, y_val)\n",
    "        acc_sum, loss_sum, loss_val = sess.run([accuracy_summary, loss_summary, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "        filewriter.add_summary(acc_sum, step)\n",
    "        filewriter.add_summary(loss_sum, step)\n",
    "        print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    final_time = end_time - start_time\n",
    "    saver2.save(sess, cell_net_model)\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" time.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model\n",
    "\n",
    "Use this to run a sample through the network and get a softmax estimate for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = generate_image_array('C:/AI/Cell_Net_v2/test_image/test_2.jpg', 331, 331)\n",
    "dog = generate_image_array('C:/AI/Cell_Net_v2/test_image/test.jpg', 331, 331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore graph from meta and restore variables\n",
    "    new_saver = tf.train.import_meta_graph(cell_net_model + '.meta')\n",
    "    new_saver.restore(sess, cell_net_model)\n",
    "    soft = tf.get_default_graph().get_tensor_by_name(\"DNN_Classifier/Softmax:0\")\n",
    "    steps = tf.get_default_graph().get_tensor_by_name(\"DNN_Classifier/train/global_step:0\")\n",
    "    current_step = steps.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/AI/Cell_Net_v3/model/cell_net_best\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore graph from meta and restore variables\n",
    "    new_saver = tf.train.import_meta_graph(cell_net_best + '.meta')\n",
    "    new_saver.restore(sess, cell_net_best)\n",
    "    soft = tf.get_default_graph().get_tensor_by_name(\"DNN_Classifier/Final_Layer/Softmax:0\")\n",
    "    input_tensor = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    val = soft.eval(feed_dict={input_tensor: cat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04368506,  0.95631492]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Binary_Categorical_Predictor(cell_net_model, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/AI/Cell_Net_v2/model/cell_net\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The name 'DNN_Classifier/Softmax:0' refers to a Tensor which does not exist. The operation, 'DNN_Classifier/Softmax', does not exist in the graph.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1cbb1ccf9bf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_image_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/tmp/Cell_Net_v2/test_image/test.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Dropbox\\Own\\Programming\\Inception\\Utilities\\cell_net_predictor.py\u001b[0m in \u001b[0;36mprint_image_probability\u001b[1;34m(self, input_image_location)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mnew_saver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.meta'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mnew_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0msoft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DNN_Classifier/Softmax:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"input:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_image_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m299\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m299\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3764\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\n\u001b[0;32m   3765\u001b[0m                       type(name).__name__)\n\u001b[1;32m-> 3766\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3768\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_tensor_by_tf_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3630\u001b[0m           raise KeyError(\"The name %s refers to a Tensor which does not \"\n\u001b[0;32m   3631\u001b[0m                          \u001b[1;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3632\u001b[1;33m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[0;32m   3633\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3634\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout_n\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The name 'DNN_Classifier/Softmax:0' refers to a Tensor which does not exist. The operation, 'DNN_Classifier/Softmax', does not exist in the graph.\""
     ]
    }
   ],
   "source": [
    "predictor.print_image_probability('C:/tmp/Cell_Net_v2/test_image/test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = np.reshape(last_layers, (num_epochs,batch_size,4320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/AI/Cell_Net_v2/model/cell_net\n",
      "Did 488 of loss minimized training in 672.7426071166992 time.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, cell_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    \n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" time.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672.7426071166992"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
