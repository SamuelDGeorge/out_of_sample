{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#All neccesary classes for project\n",
    "\n",
    "#general\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#for preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "\n",
    "#for machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "#for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "from Utilities.utilities import display_scores\n",
    "from Utilities.utilities import pipeline_transform\n",
    "from Utilities.utilities import reset_graph\n",
    "from Utilities.models import DNN_Model\n",
    "from Utilities.models import cross_val_score_dnn\n",
    "from functools import partial\n",
    "\n",
    "#image manipulation\n",
    "from PIL import Image as PI\n",
    "from resizeimage import resizeimage\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.data_utils import get_file\n",
    "import vgg_preprocessing\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "import pnasnet as nas\n",
    "\n",
    "#Import Custom Functions\n",
    "from Utilities.model_builder import get_image\n",
    "from Utilities.model_builder import get_file_lists\n",
    "from Utilities.model_builder import parse_record\n",
    "from Utilities.model_builder import get_batch\n",
    "from Utilities.model_builder import build_iterator\n",
    "from Utilities.model_builder import get_values\n",
    "from Utilities.models import log_dir_build\n",
    "from Utilities.utilities import generate_image\n",
    "from Utilities.utilities import generate_image_array\n",
    "from Utilities.cell_net_predictor import Binary_Categorical_Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Here we will load the training and validation data in order to do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import TFRecords for both Training and Testing of the Dat\n",
    "#Use the build_image_data.py to create these sets from your data\n",
    "#train_list, val_list = get_file_lists(\"C:/tmp/model_training_data/Dog_Cat/tf_record\")\n",
    "labels = [\"Cat\", \"Dog\"]\n",
    "\n",
    "\n",
    "#Nasnet Model Location\n",
    "nas_net_model = 'C:/AI/pnas_net/model.ckpt'\n",
    "animal_net_logs = 'C:/AI/Animal_Net_v2/logs'\n",
    "animal_net_model = 'C:/AI/Animal_Net_v2/model/cell_net'\n",
    "\n",
    "test_image = generate_image_array('C:/AI/Animal_Net/test_image/Cat.jpg', 299, 299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network on top of Large Nas-Net\n",
    "\n",
    "Here we will build the Nas-Net and then stack our own network on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.\n"
     ]
    }
   ],
   "source": [
    "#Reset the graph \n",
    "reset_graph()\n",
    "\n",
    "#Set constants for Neural Network\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.01\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 250\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 25\n",
    "n_hidden6 = 10\n",
    "n_final_layer = 2\n",
    "\n",
    "\n",
    "#Placeholder for input data\n",
    "X = tf.placeholder(tf.float32, shape=[None, 299, 299, 3], name=\"input\")\n",
    "y = tf.placeholder(tf.int32, shape=[None], name=\"output\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name = 'training')\n",
    "\n",
    "#Define initalizer and batch normalization layers\n",
    "bn_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "#Import the Nas_Net and build it\n",
    "with slim.arg_scope(nas.pnasnet_large_arg_scope()):\n",
    "    net, end_points = nas.build_pnasnet_large(X, num_classes=1001, is_training=False)\n",
    "    \n",
    "    #A saver to load the pretrained Data\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #For getting predictions from Original Network\n",
    "    soft_max_pna = tf.get_default_graph().get_tensor_by_name(\"final_layer/predictions:0\")\n",
    "    \n",
    "    #Load in the noder where we are going to connect our own network\n",
    "    last_feature_node = tf.get_default_graph().get_tensor_by_name(\"final_layer/dropout/Identity:0\")\n",
    "\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"DNN_Classifier\"):\n",
    "    #Use a stop layer to freeze all the layers beneath in Nas-Net\n",
    "    with tf.name_scope(\"Hidden_Layer_1\"):\n",
    "        stop_layer = tf.stop_gradient(last_feature_node)\n",
    "        hidden1 = tf.layers.dense(stop_layer, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "        hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "        bn1 = bn_batch_norm_layer(hidden1_drop)\n",
    "        bn1_act = tf.nn.elu(bn1)\n",
    "    \n",
    "    with tf.name_scope(\"Hidden_Layer_2\"):\n",
    "        hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\", kernel_initializer=he_init)\n",
    "        hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "        bn2 = bn_batch_norm_layer(hidden2_drop)\n",
    "        bn2_act = tf.nn.elu(bn2)\n",
    "    \n",
    "    with tf.name_scope(\"Hidden_Layer_3\"):\n",
    "        hidden3 = tf.layers.dense(bn2_act, n_hidden3, name=\"hidden3\", kernel_initializer=he_init)\n",
    "        hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training)\n",
    "        bn3 = bn_batch_norm_layer(hidden3_drop)\n",
    "        bn3_act = tf.nn.elu(bn3)\n",
    "    \n",
    "    with tf.name_scope(\"Hidden_Layer_4\"):\n",
    "        hidden4 = tf.layers.dense(bn3_act, n_hidden4, name=\"hidden4\", kernel_initializer=he_init)\n",
    "        hidden4_drop = tf.layers.dropout(hidden4, dropout_rate, training=training)\n",
    "        bn4 = bn_batch_norm_layer(hidden4_drop)\n",
    "        bn4_act = tf.nn.elu(bn4)\n",
    "    \n",
    "    with tf.name_scope(\"Hidden_Layer_5\"):\n",
    "        hidden5 = tf.layers.dense(bn4_act, n_hidden5, name=\"hidden5\", kernel_initializer=he_init)\n",
    "        hidden5_drop = tf.layers.dropout(hidden5, dropout_rate, training=training)\n",
    "        bn5 = bn_batch_norm_layer(hidden5_drop)\n",
    "        bn5_act = tf.nn.elu(bn5)\n",
    "    \n",
    "    with tf.name_scope(\"Hidden_Layer_6\"):\n",
    "        hidden6 = tf.layers.dense(bn5_act, n_hidden6, name=\"hidden6\", kernel_initializer=he_init)\n",
    "        hidden6_drop = tf.layers.dropout(hidden6, dropout_rate, training=training)\n",
    "        bn6 = bn_batch_norm_layer(hidden6_drop)\n",
    "        bn6_act = tf.nn.elu(bn6)\n",
    "    \n",
    "    with tf.name_scope(\"Final_Layer\"):\n",
    "        logits_before_bn = tf.layers.dense(bn6_act, n_final_layer, name=\"outputs\")\n",
    "        logits = bn_batch_norm_layer(logits_before_bn, name=\"logits\")\n",
    "        softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "            loss_summary = tf.summary.scalar('loss_summary', loss)\n",
    "            \n",
    "    with tf.name_scope(\"train\"):\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        \n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        accuracy_summary = tf.summary.scalar('accuracy_summary', accuracy)\n",
    "        \n",
    "        \n",
    "            \n",
    "#Variables for global initialization\n",
    "saver2 = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/tmp/pnas_net/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #Initalizer all variables\n",
    "    init.run()\n",
    "    \n",
    "    #Restore the pretrained variables from Nas-Net\n",
    "    saver.restore(sess, nas_net_model)\n",
    "    \n",
    "    \n",
    "    #Save all of these variables to the new Cell_Net Model\n",
    "    saver2.save(sess, cell_net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Exporting Graph\n",
    "filewriter = tf.summary.FileWriter(cell_net_logs, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Testing\n",
    "\n",
    "Here we will make sure the pre-trained network loaded and is effective with Imagenet.\n",
    "If correct using first of Imagenet test it should print 286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/tmp/Cell_Net/model_test/cell_net\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver2.restore(sess, cell_net_model)\n",
    "    y_raw = soft_max_pna.eval(feed_dict={X: test_image})\n",
    "    \n",
    "frame = pd.DataFrame(y_raw)\n",
    "item = frame.iloc[0,:]\n",
    "item = pd.Series.idxmax(item)\n",
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "Here we are going to train the network. Accuracy/Loss is recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/tmp/Cell_Net/model/cell_net\n",
      "Loaded model. Training network initially. Logs into: C:/tmp/Cell_Net/logs/cell_out-run-20180531192055/\n",
      "Step: 0 Loss: 0.681009\n",
      "Step: 20 Loss: 0.448954\n",
      "Step: 40 Loss: 0.339628\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "num_epochs = 50\n",
    "steps_between_test_save = 20\n",
    "batch_size = 5\n",
    "\n",
    "# Save the Graph into a file with Filewriter and add summaries for this session\n",
    "model_path = log_dir_build(cell_net_logs, \"cell_out\")\n",
    "filewriter = tf.summary.FileWriter(model_path, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, cell_net_model)\n",
    "    \n",
    "    #Build iterators to pull train and test data\n",
    "    iterator_train = build_iterator(True, train_list, batch_size, num_epochs=num_epochs, num_parallel_calls=8)\n",
    "    iterator_test = build_iterator(False, val_list, batch_size, num_epochs=num_epochs, num_parallel_calls=4)\n",
    "    next_train = iterator_train.get_next()\n",
    "    next_test = iterator_test.get_next()\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "\n",
    "    #Iterate through training \n",
    "    while step < num_epochs:\n",
    "        #Get training data\n",
    "        X_val, y_val = next_train\n",
    "        X_val, y_val = get_values(sess, X_val, y_val)\n",
    "        \n",
    "        #run Training Op\n",
    "        sess.run([training_op, extra_update_ops, accuracy_summary], feed_dict={X: X_val, y: y_val, training: True})\n",
    "        \n",
    "        #Maybe Test Accuracy\n",
    "        if (step % steps_between_test_save) == 0:\n",
    "            X_val, y_val = next_test\n",
    "            X_val, y_val = get_values(sess, X_val, y_val)\n",
    "            acc_sum, loss_sum, loss_val = sess.run([accuracy_summary, loss_summary, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            print(\"Step: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            saver2.save(sess, cell_net_model)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver2.save(sess, cell_net_model)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/tmp/Cell_Net/model/cell_net\n",
      "Loaded model. Starting training from step: 50, Logging in directory: C:/tmp/Cell_Net/logs/cell_out-run-20180531192055/\n",
      "Step: 60 Loss: 0.415838\n",
      "Step: 80 Loss: 0.357053\n",
      "Step: 100 Loss: 0.517353\n",
      "Step: 120 Loss: 0.135984\n",
      "Step: 140 Loss: 0.284187\n",
      "Step: 160 Loss: 0.0725838\n",
      "Step: 180 Loss: 0.202192\n",
      "Step: 200 Loss: 0.0777485\n",
      "Step: 220 Loss: 0.0684065\n",
      "Step: 240 Loss: 0.173998\n",
      "Step: 260 Loss: 0.0411993\n",
      "Step: 280 Loss: 0.244819\n",
      "Step: 300 Loss: 0.189933\n",
      "Step: 320 Loss: 0.0505728\n",
      "Step: 340 Loss: 0.0339866\n",
      "Step: 360 Loss: 0.204192\n",
      "Step: 380 Loss: 0.21926\n",
      "Step: 400 Loss: 0.0374186\n",
      "Step: 420 Loss: 0.0241346\n",
      "Step: 440 Loss: 0.125693\n",
      "Step: 460 Loss: 0.0724637\n",
      "Step: 480 Loss: 0.0583581\n",
      "Step: 500 Loss: 0.0268569\n",
      "Step: 520 Loss: 0.0746905\n",
      "Step: 540 Loss: 0.313732\n",
      "Step: 560 Loss: 0.0576057\n",
      "Step: 580 Loss: 0.0195325\n",
      "Step: 600 Loss: 0.254252\n",
      "Step: 620 Loss: 0.17301\n",
      "Step: 640 Loss: 0.0225835\n",
      "Step: 660 Loss: 0.0231231\n",
      "Step: 680 Loss: 0.217269\n",
      "Step: 700 Loss: 0.232683\n",
      "Step: 720 Loss: 0.0171685\n",
      "Step: 740 Loss: 0.273797\n",
      "Step: 760 Loss: 0.0158683\n",
      "Step: 780 Loss: 0.0203797\n",
      "Step: 800 Loss: 0.09176\n",
      "Step: 820 Loss: 0.0143854\n",
      "Step: 840 Loss: 0.106776\n",
      "Step: 860 Loss: 0.113837\n",
      "Step: 880 Loss: 0.0076998\n",
      "Step: 900 Loss: 0.168969\n",
      "Step: 920 Loss: 0.0128918\n",
      "Step: 940 Loss: 0.204826\n",
      "Step: 960 Loss: 0.0262538\n",
      "Step: 980 Loss: 0.118618\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#For picking up with additional training\n",
    "#Total number of epochs to train\n",
    "num_epochs = 1000\n",
    "steps_between_test_save = 20\n",
    "batch_size = 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, cell_net_model)\n",
    "    \n",
    "    #Build iterators to pull train and test data\n",
    "    iterator_train = build_iterator(True, train_list, batch_size, num_epochs=num_epochs, num_parallel_calls=8)\n",
    "    iterator_test = build_iterator(False, val_list, batch_size, num_epochs=num_epochs, num_parallel_calls=4)\n",
    "    next_train = iterator_train.get_next()\n",
    "    next_test = iterator_test.get_next()\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = tf.train.global_step(sess, global_step)\n",
    "    print(\"Loaded model. Starting training from step: \" + str(step) + \", Logging in directory: \" + model_path)\n",
    "\n",
    "    #Iterate through training \n",
    "    while step < num_epochs:\n",
    "        #Get training data\n",
    "        X_val, y_val = next_train\n",
    "        X_val, y_val = get_values(sess, X_val, y_val)\n",
    "        \n",
    "        #run Training Op\n",
    "        sess.run([training_op, extra_update_ops, accuracy_summary], feed_dict={X: X_val, y: y_val, training: True})\n",
    "        \n",
    "        #Maybe Test Accuracy\n",
    "        if (step % steps_between_test_save) == 0:\n",
    "            X_val, y_val = next_test\n",
    "            X_val, y_val = get_values(sess, X_val, y_val)\n",
    "            acc_sum, loss_sum, loss_val = sess.run([accuracy_summary, loss_summary, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            print(\"Step: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "            saver2.save(sess, cell_net_model)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver2.save(sess, cell_net_model)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model\n",
    "\n",
    "Use this to run a sample through the network and get a softmax estimate for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = generate_image_array('C:/tmp/Cell_Net/test_image/Cat.jpg', 299, 299)\n",
    "dog = generate_image_array('C:/tmp/Cell_Net/test_image/Dog.jpg', 299, 299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore graph from meta and restore variables\n",
    "    new_saver = tf.train.import_meta_graph(cell_net_model + '.meta')\n",
    "    new_saver.restore(sess, cell_net_model)\n",
    "    soft = tf.get_default_graph().get_tensor_by_name(\"DNN_Classifier/Softmax:0\")\n",
    "    steps = tf.get_default_graph().get_tensor_by_name(\"DNN_Classifier/train/global_step:0\")\n",
    "    current_step = steps.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/tmp/Cell_Net/model/cell_net\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore graph from meta and restore variables\n",
    "    new_saver = tf.train.import_meta_graph(cell_net_model + '.meta')\n",
    "    new_saver.restore(sess, cell_net_model)\n",
    "    soft = tf.get_default_graph().get_tensor_by_name(\"DNN_Classifier/Softmax:0\")\n",
    "    input_tensor = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    val = soft.eval(feed_dict={input_tensor: cat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97931594,  0.02068399]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Binary_Categorical_Predictor(cell_net_model, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/tmp/Cell_Net/model/cell_net\n",
      "Probability of Cat: 0.0262442\n",
      "Probability of Dog: 0.973756\n"
     ]
    }
   ],
   "source": [
    "predictor.print_image_probability('C:/tmp/Cell_Net/test_image/Jimmy.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
