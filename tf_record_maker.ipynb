{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Neccesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All neccesary classes for project\n",
    "\n",
    "#general\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "#image manipulation\n",
    "from PIL import Image as PI\n",
    "from PIL import ImageFilter\n",
    "from resizeimage import resizeimage\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import Custom Functions\n",
    "from Utilities.model_builder import get_image\n",
    "from Utilities.model_builder import get_file_lists\n",
    "from Utilities.model_builder import parse_record\n",
    "from Utilities.model_builder import get_batch\n",
    "from Utilities.model_builder import build_iterator\n",
    "from Utilities.models import log_dir_build\n",
    "from Utilities.utilities import generate_image\n",
    "from Utilities.utilities import generate_image_array\n",
    "from Utilities.blur_tool import blur_images_in_directory\n",
    "from Utilities.resize_tool import resize_images_in_directory\n",
    "from Utilities.blur_tool import blur_and_print_image\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "from Utilities.utilities import display_scores\n",
    "from Utilities.bounded_file_label_extractor import get_files_and_labels\n",
    "from Utilities.bounded_box_record_maker import process_bounded_image_files\n",
    "from Utilities.build_image_data_notebook import process_dataset\n",
    "\n",
    "#for preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TF Records\n",
    "\n",
    "Links for labels, train, and validation are used to build tf_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build tf_records\n",
    "#Set Variables\n",
    "validation_directory = \"D:/Machine_Learning/Datasets/Cifar_80/test_data\"\n",
    "train_directory = \"D:/Machine_Learning/Datasets/Cifar_80/train_data\"\n",
    "output_directory = \"D:/Machine_Learning/Datasets/Cifar_80/tf_records\"\n",
    "labels_file = \"D:/Machine_Learning/Datasets/Cifar_80/labels.txt\"\n",
    "\n",
    "num_threads = 2\n",
    "num_shards = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining list of input files and labels from D:/Machine_Learning/Datasets/Cifar_80/test_data.\n",
      "Found 8000 JPEG files across 80 labels inside D:/Machine_Learning/Datasets/Cifar_80/test_data.\n",
      "Launching 2 threads for spacings: [[0, 4000], [4000, 8000]]\n",
      "2019-01-09 15:35:27.423876 [thread 0]: Processed 1000 of 4000 images in thread batch.\n",
      "2019-01-09 15:35:27.428877 [thread 1]: Processed 1000 of 4000 images in thread batch.\n",
      "2019-01-09 15:35:28.797877 [thread 0]: Processed 2000 of 4000 images in thread batch.2019-01-09 15:35:28.797877 [thread 1]: Processed 2000 of 4000 images in thread batch.\n",
      "\n",
      "2019-01-09 15:35:30.195876 [thread 0]: Processed 3000 of 4000 images in thread batch.\n",
      "2019-01-09 15:35:30.197877 [thread 1]: Processed 3000 of 4000 images in thread batch.\n",
      "2019-01-09 15:35:31.511878 [thread 0]: Processed 4000 of 4000 images in thread batch.\n",
      "2019-01-09 15:35:31.525876 [thread 0]: Wrote 4000 images to D:/Machine_Learning/Datasets/Cifar_80/tf_records\\validation-00000-of-00002\n",
      "2019-01-09 15:35:31.526877 [thread 0]: Wrote 4000 images to 4000 shards.\n",
      "2019-01-09 15:35:31.526877 [thread 1]: Processed 4000 of 4000 images in thread batch.\n",
      "2019-01-09 15:35:31.548877 [thread 1]: Wrote 4000 images to D:/Machine_Learning/Datasets/Cifar_80/tf_records\\validation-00001-of-00002\n",
      "2019-01-09 15:35:31.548877 [thread 1]: Wrote 4000 images to 4000 shards.\n",
      "2019-01-09 15:35:32.046877: Finished writing all 8000 images in data set.\n",
      "Determining list of input files and labels from D:/Machine_Learning/Datasets/Cifar_80/train_data.\n",
      "Found 40000 JPEG files across 80 labels inside D:/Machine_Learning/Datasets/Cifar_80/train_data.\n",
      "Launching 2 threads for spacings: [[0, 20000], [20000, 40000]]\n",
      "2019-01-09 15:35:34.395876 [thread 1]: Processed 1000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:34.401878 [thread 0]: Processed 1000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:35.775877 [thread 1]: Processed 2000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:35.813876 [thread 0]: Processed 2000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:37.188878 [thread 1]: Processed 3000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:37.226877 [thread 0]: Processed 3000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:38.567877 [thread 1]: Processed 4000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:38.601880 [thread 0]: Processed 4000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:39.913876 [thread 1]: Processed 5000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:39.953877 [thread 0]: Processed 5000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:41.272877 [thread 1]: Processed 6000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:41.316878 [thread 0]: Processed 6000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:42.643877 [thread 1]: Processed 7000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:42.677878 [thread 0]: Processed 7000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:44.044877 [thread 1]: Processed 8000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:44.066877 [thread 0]: Processed 8000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:45.400877 [thread 1]: Processed 9000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:45.429878 [thread 0]: Processed 9000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:46.773877 [thread 1]: Processed 10000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:46.791878 [thread 0]: Processed 10000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:48.116878 [thread 1]: Processed 11000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:48.145878 [thread 0]: Processed 11000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:49.494877 [thread 1]: Processed 12000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:49.514880 [thread 0]: Processed 12000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:50.869878 [thread 1]: Processed 13000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:50.894878 [thread 0]: Processed 13000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:52.232877 [thread 1]: Processed 14000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:52.272876 [thread 0]: Processed 14000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:53.602877 [thread 1]: Processed 15000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:53.654877 [thread 0]: Processed 15000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:55.017877 [thread 1]: Processed 16000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:55.064877 [thread 0]: Processed 16000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:56.398877 [thread 1]: Processed 17000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:56.438876 [thread 0]: Processed 17000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:57.757876 [thread 1]: Processed 18000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:57.799876 [thread 0]: Processed 18000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:59.141876 [thread 1]: Processed 19000 of 20000 images in thread batch.\n",
      "2019-01-09 15:35:59.174876 [thread 0]: Processed 19000 of 20000 images in thread batch.\n",
      "2019-01-09 15:36:00.520876 [thread 1]: Processed 20000 of 20000 images in thread batch.\n",
      "2019-01-09 15:36:00.545877 [thread 1]: Wrote 20000 images to D:/Machine_Learning/Datasets/Cifar_80/tf_records\\train-00001-of-00002\n",
      "2019-01-09 15:36:00.546877 [thread 1]: Wrote 20000 images to 20000 shards.\n",
      "2019-01-09 15:36:00.590876 [thread 0]: Processed 20000 of 20000 images in thread batch.\n",
      "2019-01-09 15:36:00.622876 [thread 0]: Wrote 20000 images to D:/Machine_Learning/Datasets/Cifar_80/tf_records\\train-00000-of-00002\n",
      "2019-01-09 15:36:00.622876 [thread 0]: Wrote 20000 images to 20000 shards.\n",
      "2019-01-09 15:36:00.971876: Finished writing all 40000 images in data set.\n"
     ]
    }
   ],
   "source": [
    "#make validation records\n",
    "process_dataset('validation', validation_directory, num_shards, labels_file, num_threads, output_directory)\n",
    "\n",
    "#make validation records\n",
    "process_dataset('train', train_directory, num_shards, labels_file, num_threads, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Area\n",
    "\n",
    "Use this area to run tests on above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
